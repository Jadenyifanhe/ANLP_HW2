however O
, O
to O
our O
knowledge O
, O
none O
of O
these O
methods O
has O
so O
far O
been O
successful O
in O
nlp O
due O
to O
the O
discrete O
nature O
of O
texts O
. O

2020 O
) O
show O
impressive O
results O
in O
image O
classiﬁcation O
with O
simple O
linear O
interpolation O
of O
data O
. O

earlier O
methods O
apply O
simple O
rules O
like O
rotation O
and O
scaling O
of O
images O
( O
simard O
et O
al O
. O
, O

deﬁning O
vicinity O
is O
however O
challenging O
as O
it O
requires O
to O
extract O
samples O
from O
a O
distribution O
without O
hurting O
the O
labels O
. O

2001 O
) O
can O
be O
an O
effective O
choice O
for O
achieving O
better O
out O
- O
of O
- O
training O
generalization O
. O

data O
augmentation O
supported O
by O
the O
vicinal O
risk O
minimization O
( O
vrm O
) O
principle O
( O
chapelle O
et O
al O
. O
, O

the O
widely O
used O
empirical O
risk O
minimization O
( O
erm O
) O
principle O
where O
models O
are O
trained O
to O
minimize O
the O
average O
training O
error O
has O
been O
shown O
to O
be O
insufﬁcient O
to O
achieve O
generalization O
on O
distributions O
that O
differ O
slightly O
from O
the O
training O
data O
( O
szegedy O
et O
al O
. O
, O

one O
of O
the O
fundamental O
challenges O
in O
deep O
learning O
is O
to O
train O
models O
that O
generalize O
well O
to O
examples O
outside O
the O
training O
distribution O
. O

we O
perform O
each O
of O
the O
experiments O
in O
a O
single O
gpu O
setup O
with O
ﬂoat32 O
precision O
. O

we O
ﬁx O
the O
maximum O
sequence O
length O
to O
280for O
xner B-TaskName
and O
128tokens O
for O
xnli B-TaskName
. O

in O
our O
experiments O
, O
effective O
batch B-HyperparameterName
- I-HyperparameterName
size I-HyperparameterName
is O
another O
crucial O
hyperparameter O
that O
can O
be O
obtained O
by O
gradient O
accumulation O
steps O
. O

in O
an O
average O
, O
for O
per O
million O
augmentation O
requires O
.5 O
- O
2 O
days O
based O
of O
various O
settings O
of O
training O
mechanism O
( O
ie O
. O
, O

average O
run O
- O
time O
for O
each O
of O
the O
languages O
may O
differ O
based O
on O
total O
number O
of O
augmented O
samples O
. O

in O
table O
8 O
, O
lr O
- O
warm O
- O
up O
- O
steps O
refer O
to O
the O
warmup O
- O
step O
from O
triangular O
learning O
rate O
scheduling O
. O

in O
the O
case O
of O
a O
given O
number O
of O
epochs O
, O
we O
convert O
it O
to O
a O
total O
number O
of O
steps O
. O

we O
train O
our O
model O
with O
respect O
to O
the O
number O
of O
steps O
instead O
of O
the O
number O
of O
epochs O
. O

however O
, O
for O
cross O
- O
lingual O
adaptation O
, O
we O
validate O
( O
for O
model O
selection O
) O
our O
model O
with O
the O
target O
language O
development O
set O
. O

in O
the O
warm O
- O
up O
step O
, O
we O
train O
and O
validate O
the O
task O
models O
with O
english O
data O
. O

d O
hyperparameters O
we O
present O
the O
hyperparameter O
settings O
for O
xner B-TaskName
and O
xnli O
tasks B-TaskName
for O
the O
xla B-MethodName
framework O
in O
table O
8 O
. O

for O
a O
fair O
comparison O
between O
the O
two O
methods O
, O
we O
use O
only O
the O
per O
- O
sample O
loss O
in O
our O
primary O
( O
single O
- O
model O
) O
distillation O
methods O
. O

however O
, O
as O
mentioned O
, O
the O
clustering O
- O
based O
method O
allows O
us O
to O
incorporate O
other O
indicative O
features O
like O
length O
, O
ﬂuency O
, O
etc O
. O

even O
though O
they O
are O
different O
, O
these O
two O
distillation O
methods O
consider O
the O
same O
source O
of O
information O
. O

relation O
with O
distillation O
by O
model O
conﬁdence O
astute O
readers O
might O
have O
already O
noticed O
that O
per O
- O
sample O
loss O
has O
a O
direct O
deterministic O
relation O
with O
the O
model O
conﬁdence O
. O

here O
, O
distillation O
hyperparameter O
is O
the O
posterior O
probability O
threshold O
based O
on O
which O
samples O
are O
selected O
. O

c O
- O
d O
) O
distribution O
of O
selected O
sentence O
lengths O
on O
target O
( O
spanish B-DatasetName
) O
language O
xner B-TaskName
classiﬁcation O
. O

we O
use O
a O
two O
- O
component O
gaussian O
mixture O
model O
( O
gmm O
) O
due O
to O
its O
ﬂexibility O
in O
modeling O
the O
sharpness O
of O
a O
distribution O
li O
et O
al O
. O
( O

however O
, O
contrary O
to O
those O
approaches O
which O
use O
actual O
( O
supervised O
) O
labels O
, O
we O
use O
the O
model O
predicted O
pseudo O
labels O
to O
compute O
the O
loss O
for O
the O
samples O
. O

we O
propose O
to O
model O
per O
- O
sample O
loss O
distribution O
( O
along O
with O
other O
task O
- O
speciﬁc O
features O
) O
with O
a O
mixture O
model O
, O
which O
we O
ﬁt O
using O
an O
expectation O
- O
maximization O
( O
em O
) O
algorithm O
. O

it O
has O
been O
shown O
in O
computer O
vision O
that O
deep O
models O
tend O
to O
learn O
good O
samples O
faster O
than O
noisy O
ones O
, O
leading O
to O
a O
lower O
loss O
for O
good O
samples O
and O
higher O
loss O
for O
noisy O
ones O
han O
et O
al O
. O
( O

here O
our O
goal O
is O
to O
cluster O
the O
samples O
based O
on O
their O
goodness O
. O

in O
the O
following O
, O
we O
introduce O
a O
clustering O
- O
based O
method O
that O
can O
consider O
these O
additional O
features O
to O
separate O
good O
samples O
from O
bad O
ones O
. O

one O
might O
also O
want O
to O
consider O
other O
features O
like O
ﬂuency O
, O
which O
can O
be O
estimated O
by O
a O
pre O
- O
trained O
conditional O
lm O
like O
gpt O
radford O
et O
al O
. O
( O

for O
example O
, O
for O
sequence O
labeling O
, O
sequence O
length O
can O
be O
an O
important O
feature O
as O
the O
models O
tend O
to O
make O
more O
mistakes O
( O
hence O
noisy O
) O
for O
longer O
sequences O
bari O
et O
al O
. O
( O

apart O
from O
classiﬁer O
conﬁdence O
, O
there O
could O
be O
other O
important O
features O
that O
can O
distinguish O
a O
good O
sample O
from O
a O
noisy O
one O
. O

c O
details O
on O
distillation O
by O
clustering O
one O
limitation O
of O
the O
conﬁdence O
- O
based O
( O
singlemodel O
) O
distillation O
is O
that O
it O
does O
not O
consider O
task O
- O
speciﬁc O
information O
. O

training O
with O
conﬁdence O
penalty O
not O
only O
improves O
pseudo O
labeling O
accuracy B-TaskName
but O
also O
helps O
the O
distillation O
methods O
to O
perform O
better O
noise O
ﬁltering O
. O

in O
summary O
, O
comparing O
the O
figures O
3(c O
- O
d O
) O
- O
4(cd O
) O
, O
we O
can O
conclude O
that O
training O
without O
conﬁdence O
penalty O
can O
make O
the O
model O
more O
prone O
to O
over-ﬁtting O
, O
resulting O
in O
more O
noisy O
pseudo O
labels O
. O

this O
shows O
that O
using O
the O
conﬁdence O
penalty O
in O
training O
, O
model O
becomes O
more O
robust O
. O

from O
these O
plots O
, O
we O
observe O
that O
the O
conﬁdence O
penalty O
also O
helps O
to O
perform O
a O
better O
distillation O
as O
more O
sentences O
are O
selected O
( O
by O
the O
distillation O
procedure O
) O
from O
the O
lower O
length O
distribution O
, O
while O
still O
covering O
the O
entire O
lengths O
. O

however O
, O
if O
we O
only O
select O
the O
lower O
length O
samples O
we O
will O
easily O
overﬁt O
. O

in O
general O
, O
the O
performance O
of O
the O
lower O
length O
samples O
is O
more O
accurate O
. O

2020 O
) O
shows O
that O
cross O
- O
lingual O
ner O
inference O
is O
heavily O
dependent O
on O
the O
length O
distribution O
of O
the O
samples O
. O

finally O
, O
figures O
4(c O
) O
and O
4(d O
) O
show O
the O
length O
distribution O
of O
all O
vs. O
the O
selected O
sentences O
( O
by O
distillation O
by O
model O
conﬁdence O
) O
without O
and O
with O
conﬁdence O
penalty O
. O

our O
distillation O
method O
should O
be O
able O
to O
distill O
out O
these O
noisy O
pseudo O
samples O
. O

also O
, O
we O
can O
see O
that O
as O
the O
sentence O
length O
increases O
, O
there O
are O
more O
wrong O
predictions O
. O

as O
we O
can O
see O
, O
the O
losses O
are O
indeed O
more O
scattered O
when O
we O
train O
the O
model O
with O
conﬁdence O
penalty O
, O
which O
indicates O
higher O
per O
- O
sample O
entropy O
, O
as O
expected O
. O

on O
their O
length O
in O
the O
x O
- O
axis O
; O
y O
- O
axis O
represents O
the O
loss O
. O

c O
- O
d O
) O
histogram O
of O
loss O
distribution O
on O
target O
( O
spanish B-DatasetName
) O
language O
xner B-TaskName
classiﬁcation O
. O

from O
the O
visualization O
, O
it O
can O
be O
seen O
that O
the O
model O
trained O
with O
conﬁdence O
penalty O
shows O
better O
inter O
- O
class O
separation O
which O
exhibits O
robustness O
of O
the O
multilingual O
model O
. O
( O

in O
addition O
to O
that O
, O
the O
ﬁgures O
also O
suggest O
that O
the O
conﬁdence O
penalty O
helps O
to O
separate O
the O
clean O
samples O
from O
the O
noisy O
ones O
either O
by O
clustering O
or O
by O
model O
conﬁdence O
. O

it O
shows O
that O
without O
conﬁdence O
penalty O
, O
there O
are O
many O
noisy O
samples O
with O
a O
small O
loss O
which O
is O
not O
desired O
. O

the O
pseudo O
labels O
) O
distribution O
in O
histogram O
without O
and O
with O
conﬁdence O
penalty O
, O
respectively O
. O

from O
the O
ﬁgure O
, O
it O
is O
evident O
that O
the O
use O
of O
conﬁdence O
penalty O
in O
the O
warm O
- O
up O
step O
makes O
the O
model O
more O
robust O
to O
unseen O
out O
- O
of O
- O
distribution O
target O
language O
data O
yielding O
better O
predictions O
, O
which O
in O
turn O
also O
provides O
a O
better O
prior O
for O
self O
- O
training O
with O
pseudo O
labels O
. O

we O
show O
the O
class O
distribution O
from O
the O
ﬁnal O
logits O
( O
on O
the O
target O
language O
) O
using O
t O
- O
sne O
plots O
. O

1 O
in O
the O
main O
paper O
) O
in O
the O
target O
language O
( O
spanish O
) O
classiﬁcation O
on O
the O
xner B-TaskName
dev O
. O

b O
visualization O
of O
conﬁdence O
penalty O
b.1 O
effect O
of O
conﬁdence O
penalty O
in O
classiﬁcation O
in O
figure O
3 O
( O
a O
- O
b O
) O
, O
we O
present O
the O
effect O
of O
the O
conﬁdence O
penalty O
( O
eq O
. O

in O
that O
sense O
, O
the O
extra O
cost O
comes O
from O
distillation O
and O
co O
- O
labeling O
, O
which O
is O
not O
signiﬁcant O
and O
is O
compensated O
by O
the O
signiﬁcant O
improvements O
that O
uxla B-MethodName
offers O
. O

in O
contrast O
, O
uxla B-MethodName
uses O
3 O
different O
models O
and O
jointly O
trains O
them O
where O
the O
models O
assist O
each O
other O
through O
distillation O
and O
co O
- O
labeling O
. O

efﬁciency O
of O
the O
method O
and O
expensive O
extra O
costs O
for O
large O
- O
scale O
pretrained O
models O
it O
is O
a O
common O
practice O
in O
model O
selection O
to O
train O
3 O
- O
5 O
disjoint O
lm O
- O
based O
task O
models O
( O
e.g. O
, O
xlm B-MethodName
- I-MethodName
r I-MethodName
on O
ner B-TaskName
) O
with O
different O
random O
seeds O
and O
report O
the O
ensemble O
score O
or O
score O
of O
the O
best O
( O
validation O
set O
) O
model O
. O

the O
combination O
of O
these O
helps O
to O
distill O
out O
the O
noisy O
samples O
better O
. O

need O
for O
the O
combination O
of O
co O
- O
teaching O
, O
codistillation O
and O
co O
- O
guessing O
? O

we O
also O
include O
the O
labeled O
source O
data O
which O
ensures O
that O
our O
model O
does O
not O
overﬁt O
on O
target O
distribution O
as O
well O
as O
persists O
the O
generalization O
capability O
of O
the O
source O
distribution O
. O

finally O
, O
inspired O
by O
the O
joint O
training O
of O
the O
cross O
- O
lingual O
language O
model O
, O
in O
the O
third O
epoch O
we O
use O
all O
four O
datasets O
. O

because O
of O
this O
, O
for O
a O
smooth O
transition O
, O
we O
apply O
the O
vicinal O
samples O
in O
the O
second O
epoch O
. O

while O
pseudo O
- O
labeling O
may O
induce O
noise O
, O
the O
model O
’s O
predictions O
for O
in O
- O
domain O
cross O
- O
lingual O
samples O
are O
usually O
better O
. O

our O
initial O
attempt O
with O
three O
different O
heads O
( O
sharing O
a O
backbone O
network O
) O
did O
n’t O
work O
well O
. O

there O
could O
be O
some O
other O
ways O
to O
achieve O
the O
same O
thing O
. O

co O
- O
labeling O
( O
section O
3.3 O
) O
utilizes O
this O
property O
. O

for O
both O
xnli B-TaskName
and O
paws B-TaskName
- I-TaskName
x I-TaskName
tasks O
, O
with O
only O
5 O
% O
labeled O
data O
in O
the O
source O
, O
uxla O
gets O
comparable O
results O
to O
the O
baseline O
that O
uses O
100 O
% O
labeled O
data.1987references O
eric O
arazo O
, O
diego O
ortego O
, O
paul O
albert O
, O
noel O
e O
o’connor O
, O
and O
kevin O
mcguinness O
. O

for O
the O
zero O
- O
resource O
xner B-TaskName
task O
, O
uxla O
sets O
a O
new O
sota O
for O
all O
the O
tested O
languages O
. O

with O
extensive O
experiments O
on O
three O
different O
cross O
- O
lingual O
tasks O
spanning O
many O
language O
pairs O
, O
we O
have O
demonstrated O
the O
effectiveness O
of O
uxla B-MethodName
. O

it O
performs O
simultaneous O
self O
- O
training O
with O
data O
augmentation O
and O
unsupervised O
sample O
selection O
. O

6 O
conclusion O
we O
propose O
a O
novel O
data O
augmentation O
framework O
, O
uxla B-MethodName
, O
for O
zero B-TaskName
- I-TaskName
resource I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
task O
adaptation O
. O

2021 O
) O
, O
we O
propose O
a O
contextualized O
lm O
based O
data O
augmentation O
for O
neural O
machine O
translation O
and O
show O
its O
advantages O
over O
traditional O
back O
- O
translation O
gaining O
improved O
performance O
in O
low O
- O
resource O
scenarios O
. O

this O
disjoint O
characteristic O
gives O
our O
framework O
the O
ﬂexibility O
to O
replace O
lmeven O
with O
a O
better O
monolingual O
lm O
for O
a O
speciﬁc O
target O
language O
, O
which O
in O
turn O
makes O
uxla B-MethodName
extendable O
to O
utilize O
stronger O
lms O
that O
may O
come O
in O
the O
future O
. O

unlike O
them O
our O
lm O
augmentation O
is O
purely O
unsupervised O
and O
we O
do O
not O
perform O
any O
ﬁne O
- O
tuning O
of O
the O
pretrained O
vicinity O
model O
. O

also O
, O
there O
are O
fundamental O
differences O
in O
the O
way O
we O
use O
the O
pretrained O
lm O
. O

these O
methods O
rely O
on O
labels O
to O
perform O
label O
- O
constrained O
augmentation O
, O
thus O
not O
directly O
comparable O
with O
ours O
. O

a O
number O
of O
recent O
methods O
have O
been O
proposed O
using O
contextualized O
lms O
( O
kobayashi O
, O
2018 O
; O
wu O
et O
al O
. O
, O

older O
data O
augmentation O
approaches O
relied O
on O
distributional O
clusters O
( O
täckström O
et O
al O
. O
, O

2020 O
) O
, O
however O
, O
show O
that O
the O
shared O
subword O
has O
a O
minimal O
contribution O
; O
instead O
, O
the O
structural O
similarity O
between O
languages O
is O
more O
crucial O
for O
effective O
transfer O
. O

2019 O
) O
also O
found O
structural O
similarity O
( O
e.g. O
, O
word O
order O
) O
to O
be O
another O
important O
factor O
for O
successful O
crosslingual O
transfer O
. O

2019 O
) O
evaluate O
zero O
- O
shot O
cross O
- O
lingual O
transferability O
of O
mbert B-MethodName
on O
several O
tasks O
and O
attribute O
its O
generalization O
capability O
to O
shared O
subword O
units O
. O

wu O
and O
dredze O
( O
2019 O
) O
, O
keung O
et O
al O
. O
( O

2020 O
) O
train O
the O
largest O
multilingual O
language O
model B-MethodName
xlm I-MethodName
- I-MethodName
r I-MethodName
with O
roberta O
( O
liu O
et O
al O
. O
, O

xlm(lample B-MethodName
and O
conneau O
, O
2019 O
) O
extends O
mbert O
with O
a O
conditional O
lm O
and O
a O
translation O
lm O
( O
using O
parallel O
data O
) O
objectives O
. O

2019 O
) O
extends O
( O
english B-DatasetName
) O
bert O
by O
jointly O
training O
on O
102 O
languages O
. O

5 O
related O
work O
recent O
years O
have O
witnessed O
signiﬁcant O
progress O
in O
learning O
multilingual O
pretrained O
models O
. O

more O
baselines O
, O
analysis O
and O
visualizations O
are O
added O
in O
appendix O
. O

this O
indicates O
that O
augmentation O
of O
uxla B-MethodName
does O
not O
overﬁt O
on O
a O
target O
language O
. O

for O
example O
, O
arabic B-DatasetName
gets O
improvements O
for O
all O
uxla B-MethodName
- O
adapted O
models O
( O
compare O
50.88 O
with O
others O
in O
row O
5 O
) O
. O

for O
some O
languages O
, O
uxla B-MethodName
adaptation O
on O
a O
different O
language O
also O
improves O
the O
performance O
. O

after O
ﬁne O
- O
tuning O
in O
a O
speciﬁc O
target O
language O
, O
the O
f1 B-MetricName
scores O
in O
english O
remain O
almost O
similar O
( O
see O
ﬁrst O
row O
) O
. O

4.4 O
robustness O
& O
efﬁciency O
table O
6 O
shows O
the O
robustness O
of O
the O
ﬁne O
- O
tuned O
uxla B-MethodName
model O
on O
xner B-TaskName
task O
. O

these O
comparisons O
ensure O
that O
the O
capability O
of O
uxla B-MethodName
through O
co O
- O
teaching O
and O
co O
- O
distillation O
is O
beyond O
the O
ensemble O
effect O
. O

moreover O
, O
ensembling O
the O
trained O
models O
from O
uxla O
further O
improves O
the O
performance O
. O

from O
the O
results O
of O
ensemble O
models O
, O
we O
see O
that O
the O
ensemble O
boosts O
the O
baseline O
xlm B-MethodName
- I-MethodName
r. I-MethodName
however O
, O
our O
regular O
uxla O
still O
outperforms O
the O
ensemble O
baselines O
by O
a O
sizeable O
margin O
. O

the O
improvements O
in O
xnli-100 B-TaskName
% O
are O
marginal O
and O
inconsistent O
, O
which O
we O
suspect O
due O
to O
the O
balanced O
class O
distribution O
. O

speciﬁcally O
, O
we O
get O
0.56 B-MetricValue
% I-MetricValue
, O
0.74 B-MetricValue
% I-MetricValue
, O
1.89 B-MetricValue
% I-MetricValue
, O
and O
1.18 B-MetricValue
% I-MetricValue
improvements O
in O
xner B-TaskName
, O
xnli-5 B-TaskName
% O
, O
paws B-TaskName
- I-TaskName
x-5 I-TaskName
% O
, O
and O
paws B-TaskName
- I-TaskName
x-100 I-TaskName
% O
respectively O
( O
table O
1,3,4 O
) O
. O

4.3 O
effect O
of O
conﬁdence O
penalty O
& O
ensemble O
for O
all O
the O
three O
tasks O
, O
we O
get O
reasonable O
improvements O
over O
the O
baselines O
by O
training O
with O
conﬁdence O
penalty O
( O
§ O
2.1 O
) O
. O

the O
zero O
shot+con O
- O
penalty O
column O
represents O
the O
zeroshot O
results O
for O
the O
model O
after O
warmup O
. O

each O
column O
( O
e.g. O
, O
es B-DatasetName
) O
under O
uxla B-MethodName
represents O
results O
in O
all O
target O
languages O
for O
a O
uxla B-MethodName
trained O
with O
the O
augmented O
data O
in O
a O
speciﬁc O
language O
( O
e.g. O
, O
es B-DatasetName
) O
. O

we O
observe O
that O
in O
every O
epoch O
, O
there O
is O
a O
signiﬁcant O
boost O
in O
f1 B-MetricName
scores O
for O
each O
of O
the O
languages O
. O

4.2 O
augmentation O
in O
stages O
figure O
2 O
presents O
the O
effect O
of O
different O
types O
of O
augmented O
data O
used O
by O
different O
epochs O
in O
our O
multi O
- O
epoch O
co O
- O
teaching O
framework O
. O

we O
observe O
better O
performance O
with O
model O
agreement O
in O
all O
the O
cases O
on O
top O
of O
the O
single O
- O
model O
distillation O
which O
validates O
its O
utility O
. O

in O
table O
5 O
, O
we O
also O
compare O
the O
results O
with O
the O
model O
agreement O
( O
shown O
as O
\ O
) O
to O
the O
results O
without O
using O
any O
agreement O
( O
) O
. O

see O
appendix O
b O
for O
more O
analysis O
on O
. O
two O
- O
stage O
distillation O
we O
now O
validate O
whether O
the O
second O
- O
stage O
distillation O
( O
distillation O
by O
model O
agreement O
) O
is O
needed O
. O

we O
notice O
that O
the O
best O
results O
for O
each O
of O
the O
languages O
are O
obtained O
for O
values O
other O
than O
100 O
% O
, O
which O
indicates O
that O
distillation O
is O
indeed O
an O
effective O
step O
in B-MethodName
u I-MethodName
xla I-MethodName
. O

here O
100 O
% O
refers O
to O
the O
case O
when O
no O
single O
- O
model O
distillation O
is O
done O
based O
on O
model O
conﬁdence O
. O

however O
, O
we O
should O
not O
rule O
out O
the O
clustering O
method O
as O
it O
gives O
a O
more O
general1985model O
en B-DatasetName
de B-DatasetName
es B-DatasetName
fr B-DatasetName
ja I-DatasetName
ko I-DatasetName
zh I-DatasetName
supervised O
results O
( O
translate O
- O
train O
- O
all O
) O
xlm B-MethodName
- I-MethodName
r I-MethodName
( O
our O
impl O
. O
) O

in O
our O
main O
experiments O
( O
tables O
1 O
- O
4 O
) O
and O
subsequent O
analysis O
, O
we O
use O
model O
conﬁdence O
for O
distillation O
. O

from O
table O
5 O
, O
we O
see O
that O
both O
perform O
similarly O
with O
model O
conﬁdence O
being O
slightly O
better O
. O

4.1 O
analysis O
of O
distillation O
methods O
model O
conﬁdence O
vs. O
clustering O
we O
ﬁrst O
analyze O
the O
performance O
of O
our O
single O
- O
model O
distillationmethods O
( O
§ O
2.3 O
) O
to O
see O
which O
of O
the O
two O
alternatives O
works O
better O
. O

for O
this O
, O
we O
use O
the O
xner B-TaskName
task O
and O
analyze O
the O
model O
based O
on O
the O
results O
in O
table O
1 O
. O

4 O
analysis O
in O
this O
section O
, O
we O
analyze O
uxla B-MethodName
by O
dissecting O
it O
and O
measuring O
the O
contribution O
of O
its O
each O
of O
the O
components O
. O

interestingly O
, O
in O
the O
100 O
% O
setup O
, O
our O
uxla B-MethodName
( O
ensemble O
) O
achieves O
almost O
similar O
accuracies O
compared O
to O
supervised O
ﬁnetuning O
of O
xlm B-MethodName
- I-MethodName
r I-MethodName
on O
all O
target O
language O
training O
dataset O
. O

moreover O
, O
our O
5 O
% O
setting O
outperforms O
100 O
% O
xlm B-MethodName
- I-MethodName
r I-MethodName
baselines O
for O
es B-DatasetName
, O
ja B-DatasetName
, O
andzh B-DatasetName
. O

in O
general O
, O
we O
get O
an O
average O
improvements O
of O
5.94 B-MetricValue
% I-MetricValue
and O
3.25 B-MetricValue
% I-MetricValue
in O
paws B-TaskName
- B-TaskName
x-5 I-TaskName
% I-TaskName
and O
pawsx-100 B-TaskName
% I-TaskName
settings O
respectively O
. O

speciﬁcally O
, O
in O
5 O
% O
setting O
, O
uxla B-MethodName
gets O
absolute O
gains O
of O
5.33 B-MetricValue
% I-MetricValue
, O
5.94 B-MetricValue
% I-MetricValue
, O
5.04 B-MetricValue
% I-MetricValue
, O
6.85 B-MetricValue
% I-MetricValue
, O
7.00 B-MetricValue
% I-MetricValue
, O
and O
5.45 B-MetricValue
% I-MetricValue
for O
de B-DatasetName
, O
es B-DatasetName
, O
fr B-DatasetName
, O
ja B-DatasetName
, O
ko B-DatasetName
, O
andzh B-DatasetName
, O
respectively O
, O
while O
in O
100 O
% O
setting O
, O
it O
gets B-MetricValue
2.21 I-MetricValue
% I-MetricValue
, O
2.36 B-MetricValue
% I-MetricValue
, O
2.00 B-MetricValue
% I-MetricValue
, O
3.99 B-MetricValue
% I-MetricValue
, O
4.53 B-MetricValue
% I-MetricValue
, O
and B-MetricValue
4.41 I-MetricValue
% I-MetricValue
improvements O
respectively O
. O

pa B-TaskName
ws I-TaskName
- I-TaskName
x I-TaskName
similar O
to O
xnli B-TaskName
, O
we O
observe O
sizable O
improvements O
for O
uxla B-MethodName
over O
the O
baselines O
on B-TaskName
paws I-TaskName
- I-TaskName
x I-TaskName
for O
both O
5 O
% O
and O
100 O
% O
settings O
( O
table O
4 O
) O
. O

speciﬁcally O
, O
uxla B-MethodName
gets O
absolute O
improvements O
of O
1.95 B-MetricValue
% I-MetricValue
, B-MetricValue
1.68 I-MetricValue
% I-MetricValue
, O
4.30 B-MetricValue
% I-MetricValue
, O
3.50 B-MetricValue
% I-MetricValue
, B-MetricValue
3.24 I-MetricValue
% I-MetricValue
, O
and B-MetricValue
1.65 I-MetricValue
% I-MetricValue
fores B-DatasetName
, O
de B-DatasetName
, O
ar B-DatasetName
, O
sw B-DatasetName
, O
hi B-DatasetName
, O
andur B-DatasetName
, O
respectively O
. O

xnli-100 B-TaskName
% O
now O
, O
considering O
uxla B-MethodName
’s O
performance O
on O
the O
full O
( O
100 O
% O
) O
labeled O
source O
data O
in O
table O
3 O
, O
we O
see O
that O
it O
achieves O
sota O
results O
for O
all O
of O
the O
languages O
with O
an O
absolute O
improvement O
of O
2.55 B-MetricValue
% I-MetricValue
on O
average O
from O
the O
xtreme B-MethodName
baseline O
. O

again O
, O
the O
gains O
are O
relatively O
higher O
for O
low O
- O
resource O
and/or O
dissimilar O
languages O
despite O
the O
base O
model O
being O
weak O
in O
such O
cases O
. O

speciﬁcally O
, O
uxla B-MethodName
gets O
absolute O
improvements O
of B-MetricValue
3.05 I-MetricValue
% I-MetricValue
, O
3.34 B-MetricValue
% I-MetricValue
, O
5.38 B-MetricValue
% I-MetricValue
, O
5.01 B-MetricValue
% I-MetricValue
, O
4.29 B-MetricValue
% I-MetricValue
, O
and O
4.12 B-MetricValue
% I-MetricValue
for O
es B-DatasetName
, O
de B-DatasetName
, O
ar B-DatasetName
, O
sw B-DatasetName
, O
hi B-DatasetName
, O
andur B-DatasetName
, O
respectively O
. O

it O
surpasses O
the O
standard O
5 B-MetricValue
% I-MetricValue
baseline O
by O
4.2 B-MetricValue
% I-MetricValue
on O
average O
. O

even O
for O
arandsw O
, O
we O
get O
0.22 B-MetricValue
% I-MetricValue
and B-MetricValue
1.11 I-MetricValue
% I-MetricValue
improvements O
, O
respectively O
. O

we O
observe O
that O
with O
only O
5 O
% O
labeled O
data O
in O
the O
source O
, O
uxla B-MethodName
gets O
comparable O
results O
to O
the O
xtreme O
baseline O
that O
uses O
100 O
% O
labeled O
data O
( O
lagging O
behind O
by O
only O
0.7 O
% O
on O
avg O
. O
) O
; O

we O
consider O
xtreme B-MethodName
as O
our O
standard O
baseline O
for O
xnli-100 B-TaskName
% O
. O

however O
, O
our O
results O
resemble O
the O
reported O
xlm B-MethodName
- I-MethodName
r I-MethodName
results O
of O
xtreme B-MethodName
( O
hu O
et O
al O
. O
, O

in O
our O
single O
gpu O
implementation O
of O
xnli B-TaskName
, O
we O
could O
not O
reproduce O
the O
reported O
results O
of O
conneau O
et O
al O
. O
( O

28.54 B-MetricValue
% I-MetricValue
, O
16.05 B-MetricValue
% I-MetricValue
, O
and B-MetricValue
9.25 I-MetricValue
% I-MetricValue
absolute O
improvements O
for O
ur B-DatasetName
, O
my B-DatasetName
and O
ar B-DatasetName
, O
respectively O
. O

in O
table O
2 O
, O
we O
report O
the O
results O
on O
the O
three O
lowresource O
langauges O
from O
wikiann B-DatasetName
. O

interestingly O
, O
it O
surpasses O
supervised O
lstm B-MethodName
- I-MethodName
crf I-MethodName
for O
nlanddewithout O
using O
any O
target O
language O
labeled O
data O
. O

uxla O
gives O
absolute O
improvements O
of O
3.76 B-MetricValue
% I-MetricValue
, B-MetricValue
4.34 I-MetricValue
% I-MetricValue
, O
6.94 B-MetricValue
% I-MetricValue
, O
8.31 B-MetricValue
% I-MetricValue
, O
and O
4.18 B-MetricValue
% I-MetricValue
for O
es B-DatasetName
, O
nl B-DatasetName
, O
de B-DatasetName
, O
ar B-DatasetName
, O
andﬁ B-DatasetName
, O
respectively O
. O

we O
observe O
that O
after O
performing O
warm O
- O
up O
with O
conf O
- O
penalty O
( O
§ O
2.1 O
) O
, O
xlm O
- O
r O
performs O
better O
than O
mbert O
on O
average O
by3.8 B-MetricValue
% I-MetricValue
for O
all O
the O
languages O
. O

2020 O
) O
, O
where O
we O
also O
evaluate O
an O
ensemble O
by O
averaging O
the O
probabilities O
from O
the O
three O
models O
. O

we O
will O
refer O
to O
these O
two O
settings O
as O
5%and100 O
% O
. O

the O
evaluation O
is O
done O
on O
the O
entire O
test O
set O
in O
both O
setups O
. O

second O
, O
to O
compare O
with O
previous O
methods O
, O
we O
also O
evaluate O
on O
the O
standard O
100 O
% O
setup O
. O

we O
used O
a O
different O
seed O
each O
time O
to O
retrieve O
this O
5 O
% O
data O
. O

therefore O
, O
for O
xnli B-TaskName
and O
paws B-TaskName
- I-TaskName
x I-TaskName
, O
we O
experiment O
with O
two O
different O
setups O
. O

2020 O
) O
also O
argue O
that O
the O
translation O
process O
can O
induce O
subtle O
artifacts O
that O
may O
have O
a O
notable O
impact O
on O
models O
. O

cross O
- O
lingual O
models O
trained O
in O
this O
setup O
may O
pick O
up O
distributional O
bias O
( O
in O
the O
label O
space O
) O
from O
the O
source O
. O

thus O
, O
the O
data O
is O
parallel O
and O
lacks O
enough O
diversity O
( O
source O
and O
target O
come O
from O
the O
same O
domain O
) O
. O

however O
, O
both O
xnli B-TaskName
and O
paws B-TaskName
- I-TaskName
x I-TaskName
training O
data O
come O
with O
machine O
- O
translated O
texts O
in O
target O
languages O
. O

we O
use O
the O
standard O
task O
setting O
for O
xner B-TaskName
, O
where O
we O
take O
100 O
% O
samples O
from O
the O
datasets O
as O
they O
come O
from O
various O
domains O
and O
sizes O
without O
any O
speciﬁc O
bias O
. O

we O
wish O
to O
investigate O
our O
method O
in O
tasks O
that O
exhibit O
such O
properties O
. O

in O
this O
scenario O
, O
there O
might O
be O
two O
different O
distributional O
gaps O
: O
( O
i O
) O
the O
generalization O
gap O
for O
the O
source O
distribution O
, O
and O
( O
ii O
) O
the O
gap O
between O
the O
source O
and O
target O
language O
distribution O
. O

evaluation O
setup O
our O
goal O
is O
to O
adapt O
a O
task O
model O
from O
a O
source O
language O
distribution O
to O
an O
unknown O
target O
language O
distribution O
assuming O
no O
labeled O
data O
in O
the O
target O
. O

we O
evaluate O
on O
all O
the O
six O
( O
typologically O
distinct O
) O
languages O
: O
fr B-DatasetName
, O
es B-DatasetName
, O
de B-DatasetName
, O
chinese B-DatasetName
( O
zh B-DatasetName
) O
, O
japanese B-DatasetName
( O
ja B-DatasetName
) O
, O
and O
korean B-DatasetName
( O
ko B-DatasetName
) O
. O

2019 O
) O
requires O
the O
models O
to O
determine O
whether O
two O
sentences O
are O
paraphrases O
. O

pa B-TaskName
ws I-TaskName
- I-TaskName
x I-TaskName
the O
paraphrase O
adversaries O
from O
word O
scrambling O
cross O
- O
lingual O
task O
( O
yang O
et O
al O
. O
, O

we O
experiment O
with O
spanish B-DatasetName
, O
german B-DatasetName
, O
arabic B-DatasetName
, O
swahili B-DatasetName
( O
sw B-DatasetName
) O
, O
hindi B-DatasetName
( O
hi B-DatasetName
) O
and O
urdu B-DatasetName
. O

for O
a O
given O
pair O
of O
sentences O
, O
the O
task O
is O
to O
predict O
the O
entailment O
relationship O
between O
the O
two O
sentences O
, O
i.e. O
, O
whether O
the O
second O
sentence O
( O
hypothesis O
) O
is O
an O
entailment O
, O
contradiction O
, O
or1983model O
en O
es O
nl O
de O
ar O
ﬁ O
supervised O
results B-MethodName
lstm I-MethodName
- I-MethodName
crf I-MethodName
( O
bari O
et O
al O
. O
, O

xnli B-TaskName
we O
use O
the O
standard O
dataset O
( O
conneau O
et O
al O
. O
, O

2017 O
) O
of O
different O
( O
unlabeled O
) O
training O
data O
sizes O
: O
urdu B-DatasetName
( O
ur-20k B-DatasetName
training O
samples O
) O
, O
bengali B-DatasetName
( O
bn10 B-DatasetName
k O
samples O
) O
, O
and O
burmese B-DatasetName
( O
my-100 O
samples O
) O
. O

to O
show O
how O
the O
models O
perform O
on O
extremely O
lowresource O
languages O
, O
we O
experiment O
with O
three O
structurally O
different O
languages O
from O
wikiann B-DatasetName
( O
pan O
et O
al O
. O
, O

note O
that O
arabic B-DatasetName
is O
structurally O
different O
from O
english B-DatasetName
, O
and O
finnish B-DatasetName
is O
from O
a O
different O
language O
family O
. O

we O
also O
evaluate O
on O
finnish B-DatasetName
( O
ﬁ B-DatasetName
) O
and O
arabic B-DatasetName
( O
ar B-DatasetName
) O
datasets O
collected O
from O
bari O
et O
al O
. O
( O

3.1 O
tasks O
& O
settings O
xner B-TaskName
: O
we O
use O
the O
standard O
conll B-DatasetName
datasets O
( O
sang O
, O
2002 O
; O
sang O
and O
meulder O
, O
2003 O
) O
for O
english B-DatasetName
( O
en B-DatasetName
) O
, O
german B-DatasetName
( O
de B-DatasetName
) O
, O
spanish B-DatasetName
( O
es B-DatasetName
) O
and O
dutch B-DatasetName
( O
nl B-DatasetName
) O
. O

for O
all O
experiments O
, O
we O
report O
the O
mean O
score O
of O
the O
three O
models O
that O
use O
different O
seeds O
. O

we O
assume O
labeled O
training O
data O
only O
in O
english O
, O
and O
transfer O
the O
trained O
model O
to O
a O
target O
language O
. O

3 O
experiments O
we O
consider O
three O
tasks O
in O
the O
zero B-TaskName
- I-TaskName
resource I-TaskName
crosslingual I-TaskName
transfer I-TaskName
setting O
. O

by O
tweaking O
, O
we O
can O
control O
how O
many O
samples O
a O
dataset O
can O
provide O
in O
the O
mix O
. O

to O
ensure O
that O
our O
model O
does O
not O
overﬁt O
on O
one O
particular O
dataset O
, O
we O
employ O
a O
balanced O
sampling O
strategy O
. O

also O
, O
the O
co O
- O
distillation O
produces O
sample O
sets O
of O
variable O
sizes O
. O

the O
datasets O
used O
at O
different O
stages O
can O
be O
of O
different O
sizes O
. O

here O
, O
distillation O
hyperparameteris O
the O
posterior O
probability O
threshold O
based O
on O
which O
samples O
are O
selected O
. O

the O
loss O
is O
computed O
based O
on O
the O
pseudo O
labels O
predicted O
by O
the O
model O
. O

we O
use O
a O
1d O
twocomponent O
gaussian O
mixture O
model O
( O
gmm O
) O
to O
model O
per O
- O
sample O
loss O
distribution O
and O
cluster O
the O
samples O
based O
on O
their O
goodness O
. O

ii)sample O
distillation O
by O
clustering O
: O
we O
propose O
this O
method O
based O
on O
the O
ﬁnding O
that O
large O
neural O
models O
tend O
to O
learn O
good O
samples O
faster O
than O
noisy O
ones O
, O
leading O
to O
a O
lower O
loss O
for O
good O
samples O
and O
higher O
loss O
for O
noisy O
ones O
( O
han O
et O
al O
. O
, O

the O
distillation O
is O
then O
done O
by O
selecting O
the O
top O
%samples O
with O
the O
highest O
conﬁdence O
scores O
. O
( O

in O
this O
case O
, O
the O
model O
’s O
conﬁdence O
is O
computed O
by O
p= O
maxc2f1:::cgpc O
(x O
) O
. O

for O
sentence O
- O
level O
tasks O
( O
e.g. O
, O
xnli B-TaskName
) O
, O
the O
model O
produces O
a O
single O
class O
distribution O
for O
each O
training O
example O
. O

this O
method O
is O
similar O
in O
spirit O
to O
the O
selection O
method O
proposed O
by O
ruder O
and O
plank O
( O
2018a O
) O
. O

stage O
1 O
: O
distillation O
by O
single O
- O
model O
the O
ﬁrst O
stage O
of O
distillation O
involves O
predictions O
from O
a O
single O
model O
for O
which O
we O
propose O
two O
alternatives O
: O
( O
i)distillation O
by O
model O
conﬁdence O
: O
in O
this O
approach O
, O
we O
select O
samples O
based O
on O
the O
model O
’s O
prediction O
conﬁdence O
. O

we O
propose O
a O
2 O
- O
stage O
sample O
distillation O
process O
to O
ﬁlter O
out O
noisy O
augmented O
data O
. O

however O
, O
the O
relabeling O
process O
can O
induce O
noise O
, O
especially O
for O
dissimilar O
/ O
low O
- O
resource O
languages O
, O
since O
the O
base O
task O
model O
may O
not O
be O
adapted O
fully O
in O
the O
early O
training O
stages O
. O

therefore O
, O
we O
need O
to O
relabel O
the O
augmented1982sentences O
no O
matter O
whether O
the O
original O
sentence O
has O
labels O
( O
source O
) O
or O
not O
( O
target O
) O
. O

here O
, O
eu B-DatasetName
is O
an O
organization O
whereas O
the O
newly O
predicted O
word O
trump O
is O
aperson O
( O
different O
name O
type O
) O
. O

for O
example O
, O
consider O
the O
following O
example O
generated O
by O
our O
vicinity O
model O
. O

the O
meaning O
of O
a O
sentence O
can O
change O
entirely O
even O
with O
minor O
variations O
in O
the O
original O
sentence O
. O

2019 O
) O
that O
generates O
new O
samples O
and O
their O
labels O
as O
simple O
linear O
interpolation O
, O
have O
not O
been O
successful O
in O
nlp O
. O

2.3 O
co O
- O
labeling O
through O
co O
- O
distillation O
due O
to O
discrete O
nature O
of O
texts O
, O
vrm O
based O
augmentation O
methods O
that O
are O
successful O
for O
images O
such O
as O
mixmatch O
( O
berthelot O
et O
al O
. O
, O

pairwise O
tasks O
like O
xnli B-TaskName
and O
paws B-TaskName
- I-TaskName
x I-TaskName
have O
pairwise O
dependencies O
: O
dependencies O
between O
a O
premise O
and O
a O
hypothesis O
in O
xnli B-TaskName
or O
dependencies O
between O
a O
sentence O
and O
its O
possible O
paraphrase O
in O
paws B-TaskName
- I-TaskName
x. I-TaskName
to O
model O
such O
dependencies O
, O
we O
use O
successive O
cross O
, O
which O
uses O
cross O
- O
product O
of O
two O
successive O
max O
applied O
independently O
to O
each O
component O
. O

for O
tasks O
involving O
a O
single O
sequence O
( O
e.g. O
, O
xner B-TaskName
) O
, O
we O
directly O
use O
successive O
max O
. O

augmentation O
of O
sentences O
through O
successive O
max O
orcross O
is O
carried O
out O
within O
the O
gen O
- O
lm(generate O
via O
lm O
) O
module O
in O
algorithm O
1 O
. O

we O
then O
take O
the O
cross O
of O
these O
two O
sets O
to O
generate12augmented O
samples O
. O

ii O
) O
successive O
cross O
in O
this O
approach O
, O
we O
divide O
each O
original O
( O
multi O
- O
sentence O
) O
sample O
xinto O
two O
parts O
and O
use O
successive O
max O
to O
create O
two O
sets O
of O
augmented O
samples O
of O
size O
1and2 O
, O
respectively O
. O

we O
generate O
(diversiﬁcation O
factor O
) O
virtual O
samples O
for O
each O
original O
example O
x O
, O
by O
randomly O
masking O
p%tokens B-HyperparameterName
each O
time O
. O
( O

for O
a O
speciﬁc O
mask O
, O
we O
sample O
scandidate O
words O
from O
the O
output O
distribution O
, O
and O
generate O
novel O
sentences O
by O
following O
one O
of O
the O
two O
alternative O
approaches O
. O
( O

in O
order O
to O
generate O
samples O
around O
each O
selected O
example O
, O
we O
ﬁrst O
randomly O
choose O
p%of B-HyperparameterName
the O
input O
tokens O
. O

2020 O
) O
as O
our O
vicinity O
model O
mlm O
, O
which O
is O
trained O
on O
massive O
multilingual O
corpora O
( O
2.5 O
tb O
of O
common O
- O
crawl O
data O
in O
100 O
languages O
) O
. O

we O
use O
xlm O
- O
r O
masked O
lm O
( O
conneau O
et O
al O
. O
, O

it O
has O
been O
shown O
that O
contextual O
lms O
pretrained O
on O
large O
- O
scale O
datasets O
capture O
useful O
linguistic O
features O
and O
can O
be O
used O
to O
generate O
ﬂuent O
grammatical O
texts O
( O
hewitt O
and O
manning O
, O
2019 O
) O
. O

appendix O
b O
shows O
visualizations O
on O
why O
conﬁdence O
penalty O
is O
helpful O
for O
distillation O
. O

we O
also O
report O
signiﬁcant O
gains O
with O
conﬁdence O
penalty O
in O
§ O
3 O
. O

such O
regularizer O
of O
output O
distribution O
has O
been O
shown O
to O
be O
effective O
for O
training O
large O
models O
( O
pereyra O
et O
al O
. O
, O

we O
address O
this O
by O
adding O
a O
negative O
entropy O
term O
 h O
to O
the O
ce O
loss O
as O
follows O
. O

however O
, O
training O
with O
the O
standard O
cross O
- O
entropy O
( O
ce O
) O
loss O
may O
result O
in O
overﬁtted O
models O
that O
produce O
overly O
conﬁdent O
predictions O
( O
low O
entropy O
) O
, O
especially O
when O
the O
class O
distribution O
is O
not O
balanced O
. O

overly O
conﬁdent O
predictions O
may O
also O
im O
- O
pose O
difﬁculties O
on O
our O
distillation O
methods O
( O
§ O
2.3 O
) O
in O
isolating O
good O
samples O
from O
noisy O
ones O
. O

in O
such O
situations O
, O
an O
overly O
conﬁdent O
( O
overﬁtted O
) O
model O
may O
produce O
more O
noisy O
pseudo O
labels O
, O
and O
the O
noise O
will O
then O
accumulate O
as O
the O
training O
progresses O
. O

training O
with O
conﬁdence O
penalty O
our O
goal O
is O
to O
train O
the O
task O
models O
so O
that O
they O
can O
be O
used O
reliably O
for O
self O
- O
training O
on O
a O
target O
language O
that O
is O
potentially O
dissimilar O
and O
low O
- O
resourced O
. O

for O
token O
- O
level O
prediction O
tasks O
( O
e.g. O
, O
ner B-TaskName
) O
, O
thetoken O
- O
level O
representations O
are O
fed O
into O
the O
classiﬁcation O
layer O
, O
whereas O
for O
sentence O
- O
level O
tasks O
( O
e.g. O
, O
xnli O
) O
, O
the O
[ O
cls O
] O
representation O
is O
used O
as O
input O
to O
the O
classiﬁcation O
layer O
. O

each O
model O
has O
the O
same O
architecture O
( O
xlmr O
large O
) O
but O
is O
initialized O
with O
different O
random O
seeds O
. O

2.1 O
warm O
- O
up O
: O
training O
task O
models O
we O
ﬁrst O
train O
three O
instances O
of O
the O
xlm O
- O
r O
model O
( O
(1);(2);(3 O
) O
) O
with O
an O
additional O
task O
- O
speciﬁc O
linear O
layer O
on O
the O
source O
language O
( O
english O
) O
labeled O
data O
. O

in O
the O
following O
, O
we O
describe O
the O
steps O
in O
algorithm O
1 O
. O

each O
of O
the O
taskmodels O
in O
uxla B-MethodName
is O
an O
instance O
of O
xlm O
- O
r O
ﬁnetuned O
on O
the O
source O
language O
task O
( O
e.g. O
, O
english O
ner O
) O
, O
whereas O
the O
pretrained O
masked O
lm O
parameterized O
bymlm(i.e O
. O

algorithm O
1 O
gives O
a O
pseudocode O
of O
the O
overall O
training O
method O
. O

the O
co O
- O
distillation O
and O
co O
- O
guessing O
mechanism O
ensure O
robustness O
of O
uxla B-MethodName
to O
out O
- O
of O
- O
domain O
distributions O
that O
can O
occur O
in O
a O
multilingual O
setup O
, O
e.g. O
, O
due O
to O
a O
structurally O
dissimilar O
and/or O
lowresource O
target O
language O
. O

model O
’s O
training O
data O
by O
taking O
the O
agreement O
from O
the O
other O
two O
models O
, O
a O
process O
we O
refer O
to O
as O
coguessing O
. O

the O
third O
model O
(l)is O
then O
progressively O
trained O
on O
these O
datasets O
: O
fds;d0 O
tgin O
epoch O
1 O
, O
~dtin O
epoch O
2 O
, O
and O
all O
in O
epoch O
3 O
. O

a O
pretrained O
lm O
( O
gen O
- O
lm O
) O
is O
used O
to O
generate O
new O
vicinal O
samples O
for O
both O
source O
and O
target O
languages O
, O
which O
are O
also O
pseudo O
- O
labeled O
and O
co O
- O
distilled O
using O
the O
two O
task O
models O
( O
(j),(k O
) O
) O
to O
generate O
~dsand O
~ O
dt O
. O

after O
training O
the O
base O
task O
models O
(1),(2 O
) O
, O
and(3)on O
source O
labeled O
data O
ds(warmup O
) O
, O
we O
use O
two O
of O
them O
( O
(j),(k O
) O
) O
topseudo O
- O
label O
andco O
- O
distill O
the O
unlabeled O
target O
language O
data O
( O
d0 O
t O
) O
. O

the O
selected O
samples O
with O
pseudo O
labels O
are O
then O
added O
to O
the O
target O
task1980 O
figure O
1 O
: O
training O
ﬂow O
of O
uxla B-MethodName
. O

in O
each O
epoch O
, O
the O
co O
- O
teaching O
process O
ﬁrst O
performs O
co O
- O
distillation O
, O
where O
two O
peer O
task O
models O
are O
used O
to O
select O
“ O
reliable O
” O
training O
examples O
to O
train O
the O
third O
model O
. O

to O
avoid O
conﬁrmation O
bias O
with O
self O
- O
training O
where O
the O
model O
accumulates O
its O
own O
errors O
, O
it O
simultaneously O
trains O
three O
task O
models O
to O
generate O
virtual O
training O
data O
through O
data O
augmentation O
and O
ﬁltering O
of O
potential O
label O
noises O
via O
multi O
- O
epoch O
co O
- O
teaching O
( O
zhou O
and O
li O
, O
2005 O
) O
. O

it O
performs O
self O
- O
training O
on O
the O
augmented O
data O
to O
acquire O
the O
corresponding O
pseudo O
labels O
. O

in O
the O
initial O
stage O
( O
epoch O
1 O
) O
, O
it O
uses O
the O
augmented O
training O
samples O
from O
the O
target O
language O
( O
d0 O
t O
) O
along O
with O
the O
original O
source O
( O
ds O
) O
. O

uxla B-MethodName
augments O
data O
from O
various O
origins O
at O
different O
stages O
of O
training O
. O

motivated O
by O
this O
, O
we O
present O
uxla B-TaskName
, O
our O
unsupervised O
data O
augmentation O
framework O
for O
zero O
- O
resource O
cross O
- O
lingual O
task O
adaptation O
. O

2 O
u B-MethodName
xla I-MethodName
framework O
while O
recent O
cross B-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
learning I-TaskName
efforts O
have O
relied O
almost O
exclusively O
on O
multi O
- O
lingual O
pretraining O
and O
zero O
- O
shot O
transfer O
of O
a O
ﬁne O
- O
tuned O
source O
model O
, O
we O
believe O
there O
is O
a O
great O
potential O
for O
more O
elaborate O
methods O
that O
can O
leverage O
the O
unlabeled O
data O
better O
. O

we O
also O
have O
similar O
ﬁndings O
in O
paws B-TaskName
- I-TaskName
x. I-TaskName
we O
provide O
a O
comprehensive O
analysis O
of O
the O
factors O
that O
contribute O
to O
uxla B-MethodName
’s O
performance O
. O

for O
xnli B-TaskName
, O
with O
only O
5 O
% O
labeled O
data O
in O
the O
source O
, O
it O
gets O
comparable O
results O
to O
the O
baseline O
that O
uses O
all O
the O
labeled O
data O
, O
and O
surpasses O
the O
standard O
baseline O
by O
2.55 B-MetricValue
% I-MetricValue
on O
average O
when O
it O
uses O
all O
the O
labeled O
data O
in O
the O
source O
. O

the O
relative O
gains O
for O
uxla B-MethodName
are O
particularly O
higher O
for O
structurally O
dissimilar O
and/or O
low O
- O
resource O
languages O
: B-MetricValue
28.54 I-MetricValue
% I-MetricValue
, B-MetricValue
16.05 I-MetricValue
% I-MetricValue
, O
and O
9.25 B-MetricValue
% I-MetricValue
absolute O
improvements O
for O
urdu B-DatasetName
, O
burmese B-DatasetName
, O
and O
arabic B-DatasetName
, O
respectively O
. O

uxla B-MethodName
yields O
impressive O
results O
on O
xner B-TaskName
, O
setting O
sota O
in O
all O
tested O
languages O
outperforming O
the O
baselines O
by O
a O
good O
margin O
. O

we O
validate O
the O
effectiveness O
and O
robustness O
of O
uxla B-MethodName
by O
performing O
extensive O
experiments O
on O
three O
diverse B-TaskName
zero I-TaskName
- I-TaskName
resource I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
tasks O
– O
xner B-TaskName
, O
xnli B-TaskName
, O
and O
paws B-TaskName
- I-TaskName
x I-TaskName
, O
which O
posit O
different O
sets O
of O
challenges O
, O
and O
across O
many O
( O
14 O
in O
total O
) O
language O
pairs O
comprising O
languages O
thatare O
similar O
/ O
dissimilar O
/ O
low O
- O
resourced O
. O

this O
co O
- O
training O
employs O
a O
twostage O
co O
- O
distillation O
process O
to O
ensure O
robust O
transfer O
to O
dissimilar O
and/or O
low O
- O
resource O
languages O
. O

2020 O
) O
, O
and O
get O
reliable O
task O
labels O
by O
simultaneous O
multilingual O
co O
- O
training O
. O

we O
propose O
novel O
ways O
to O
generate O
virtual O
sentences O
using O
a O
multilingual O
masked O
lm O
( O
conneau O
et O
al O
. O
, O

with O
the O
augmented O
data O
, O
it O
performs O
simultaneous O
self O
- O
learning O
with O
an O
effective O
distillation O
strategy O
to O
learn O
a O
strongly O
adapted O
cross O
- O
lingual O
model O
from O
noisy O
( O
pseudo O
) O
labels O
for O
the O
target O
language O
task O
. O

uxla O
augments O
data O
from O
the O
unlabeled O
training O
examples O
in O
the O
target O
language O
as O
well O
as O
from O
the O
virtual O
input O
samples O
generated O
from O
the O
vicinity O
distribution O
of O
the O
source O
and O
target O
language O
sentences O
. O

in O
this O
work O
, O
we O
propose O
uxla B-MethodName
, O
a O
robust O
unsupervised B-TaskName
cross I-TaskName
-lingual I-TaskName
augmentation I-TaskName
framework O
for O
improving O
cross B-TaskName
- I-TaskName
lingual I-TaskName
generalization I-TaskName
of O
multilingual O
lms O
. O

since O
they O
rely O
on O
labels O
, O
their O
application O
is O
limited O
by O
the O
availability O
of O
enough O
task O
labels O
. O

these O
methods O
use O
a O
constrained O
augmentation O
that O
alters O
a O
pretrained O
lm O
to O
a O
label O
- O
conditional O
lm O
for O
a O
speciﬁc O
task O
. O

2018 O
) O
and O
aug O
- O
bert O
( O
shi O
et O
al O
. O
, O

other O
related O
work O
includes O
contextual O
augmentation O
( O
kobayashi O
, O
2018 O
) O
, O
conditional B-MethodName
bert I-MethodName
( O
wu O
et O
al O
. O
, O

furthermore O
, O
back O
- O
translation O
is O
only O
applicable O
in O
a O
supervised O
setup O
and O
to O
tasks O
where O
it O
is O
possible O
to O
ﬁnd O
the O
alignments O
between O
the O
original O
labeled O
entities O
and O
the O
back O
- O
translated O
entities O
, O
such O
as O
in O
question O
answering O
( O
yu O
et O
al O
. O
, O

however O
, O
it O
requires O
parallel O
data O
to O
train O
effective O
machine O
translation O
systems O
, O
acquiring O
which O
can O
be O
more O
expensive O
for O
low O
- O
resource O
languages O
than O
annotating O
the O
target O
language O
data O
. O

2016 O
) O
which O
paraphrases O
an O
input O
sentence O
through O
round O
- O
trip O
translation O
. O

in O
nlp O
, O
to O
the O
best O
of O
our O
knowledge O
, O
the O
most O
successful O
augmentation O
method O
has O
so O
far O
been O
back O
- O
translation O
( O
sennrich O
et O
al O
. O
, O

the O
main O
reason O
is O
that O
unlike O
images O
, O
linguistic O
units O
are O
discrete O
and O
a O
smooth O
change O
in O
their O
embeddings O
may O
not O
result O
in O
a O
plausible O
linguistic O
unit O
that O
has O
similar O
meanings O
. O

however O
, O
when O
it O
comes O
to O
text O
, O
such O
unsupervised O
augmentation O
methods O
have O
rarely O
been O
successful O
. O

for O
images O
, O
the O
vicinity O
of O
a O
training O
image O
can O
be O
deﬁned O
by O
a O
set O
of O
operations O
like O
rotation O
and O
scaling O
, O
or O
by1979linear O
mixtures O
of O
features O
and O
labels O
( O
zhang O
et O
al O
. O
, O

these O
methods O
enlarge O
the O
support O
of O
the O
training O
distribution O
by O
generating O
new O
data O
points O
from O
a O
vicinity O
distribution O
around O
each O
training O
example O
. O

2001 O
) O
, O
such O
data O
augmentation O
methods O
have O
shown O
impressive O
results O
in O
vision O
( O
zhang O
et O
al O
. O
, O

formalized O
by O
the O
vicinal O
risk O
minimization O
( O
vrm O
) O
principle O
( O
chapelle O
et O
al O
. O
, O

1998 O
) O
, O
and O
train O
the O
model O
on O
examples O
that O
are O
similar O
but O
different O
from O
the O
labeled O
data O
in O
the O
source O
language O
. O

one O
attractive O
way O
to O
improve O
cross B-TaskName
- I-TaskName
lingual I-TaskName
generalization I-TaskName
is O
to O
perform O
data O
augmentation O
( O
simard O
et O
al O
. O
, O

in O
our O
experiments O
( O
§ O
3.2 O
) O
, O
in O
cross B-TaskName
- I-TaskName
lingual I-TaskName
ner I-TaskName
( O
xner B-TaskName
) O
, O
we O
report O
f1 B-MetricName
reductions O
of O
28.3 B-MetricValue
% I-MetricValue
in O
urdu B-DatasetName
and O
30.4 B-MetricValue
% I-MetricValue
in O
burmese B-DatasetName
for B-MethodName
xlm I-MethodName
- I-MethodName
r I-MethodName
, O
which O
is O
trained O
on O
a O
much O
larger O
multilingual O
dataset O
than O
mbert B-MethodName
. O

the O
difﬁculty O
level O
of O
transfer O
is O
further O
exacerbated O
if O
the O
( O
dissimilar O
) O
target O
language O
is O
low O
- O
resourced O
, O
as O
the O
joint O
pretraining O
step O
may O
not O
have O
seen O
many O
instances O
from O
this O
language O
in O
the O
ﬁrst O
place O
. O

2020 O
) O
report O
about O
23:6%accuracy B-MetricValue
drop O
in O
hindi B-DatasetName
( O
structurally O
dissimilar O
) O
compared O
to O
9%drop B-MetricValue
in O
spanish B-DatasetName
( O
structurally O
similar O
) O
in O
cross B-TaskName
- I-TaskName
lingual I-TaskName
natural I-TaskName
language I-TaskName
inference I-TaskName
( O
xnli B-TaskName
) O
. O

for O
example O
, O
for O
transferring O
mbert B-MethodName
from O
english O
, O
k O
et O
al O
. O
( O

they O
all O
agree O
that O
the O
crosslingual O
generalization O
ability O
of O
the O
model O
is O
limited O
by O
the O
( O
lack O
of O
) O
structural O
similarity O
between O
the O
source O
and O
target O
languages O
. O

2020 O
) O
have O
also O
highlighted O
one O
crucial O
limiting O
factor O
for O
successful O
crosslingual B-TaskName
transfer I-TaskName
. O

despite O
their O
effectiveness O
, O
recent O
studies O
( O
pires O
et O
al O
. O
, O

the O
joint O
pretraining O
with O
multiple O
languages O
allows O
these O
models O
to O
generalize O
across O
languages O
. O

2020 O
) O
coupled O
equal O
contributionwith O
supervised O
ﬁne O
- O
tuning O
in O
the O
source O
language O
have O
been O
quite O
successful O
in O
transferring O
linguistic O
and O
task O
knowledge O
from O
one O
language O
to O
another O
without O
using O
any O
task O
label O
in O
the O
target O
language O
. O

2019 O
) O
and B-MethodName
xlm I-MethodName
- I-MethodName
r I-MethodName
( O
conneau O
et O
al O
. O
, O

jointly O
trained O
deep O
multi O
- O
lingual O
lms O
like O
mbert B-MethodName
( O
devlin O
et O
al O
. O
, O

recently O
, O
the O
pretrain-ﬁnetune O
paradigm O
has O
also O
been O
extended O
to O
multi O
- O
lingual O
setups O
to O
train O
effective O
multi O
- O
lingual O
models O
that O
can O
be O
used O
for O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
. O

however O
, O
getting O
labeled O
data O
for O
every O
target O
task O
in O
every O
target O
language O
is O
difﬁcult O
, O
especially O
for O
low O
- O
resource O
languages O
. O

these O
methods O
typically O
follow O
two O
basic O
steps O
, O
where O
a O
supervised O
task O
- O
speciﬁc O
ﬁnetuning O
follows O
a O
large O
- O
scale O
lm O
pretraining O
( O
radford O
et O
al O
. O
, O

1 O
introduction O
self O
- O
supervised O
learning O
in O
the O
form O
of O
pretrained O
language O
models O
( O
lm O
) O
has O
been O
the O
driving O
force O
in O
developing O
state O
- O
of O
- O
the O
- O
art O
nlp O
systems O
in O
recent O
years O
. O

with O
an O
in O
- O
depth O
framework O
dissection O
, O
we O
demonstrate O
the O
cumulative O
contributions O
of O
different O
components O
to O
its O
success O
. O

u B-MethodName
xla I-MethodName
achieves O
sota O
results O
in O
all O
the O
tasks O
, O
outperforming O
the O
baselines O
by O
a O
good O
margin O
. O

to O
show O
its O
effectiveness O
, O
we O
conduct O
extensive O
experiments O
on O
three O
diverse B-TaskName
zero I-TaskName
- I-TaskName
resource I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
transfer I-TaskName
tasks O
. O

at O
its O
core O
, B-MethodName
u I-MethodName
xla I-MethodName
performs O
simultaneous O
selftraining O
with O
data O
augmentation O
and O
unsupervised O
sample O
selection O
. O

in O
particular O
, B-MethodName
u I-MethodName
xla I-MethodName
aims O
to O
solve O
crosslingual B-TaskName
adaptation I-TaskName
problems O
from O
a O
source O
language O
task O
distribution O
to O
an O
unknown O
target O
language O
task O
distribution O
, O
assuming O
no O
training O
label O
in O
the O
target O
language O
. O

we O
propose O
u B-MethodName
xla I-MethodName
a O
novel O
unsupervised O
data O
augmentation O
framework O
for O
zero B-TaskName
- I-TaskName
resource I-TaskName
transfer I-TaskName
learning I-TaskName
scenarios O
. O

however O
, O
annotated O
data O
for O
every O
target O
task O
in O
every O
target O
language O
is O
rare O
, O
especially O
for O
low O
- O
resource O
languages O
. O

for O
each O
dataset O
, O
the O
state O
- O
of O
- O
the O
- O
art O
, O
the O
best O
baseline O
model O
on O
accuracy B-MetricName
, O
and O
the O
dot B-MethodName
models O
that O
reach O
the O
best O
accuracy B-MetricName
efﬁciency O
trade O
- O
off O
are O
highlighted O
. O

2020 O
) O
for O
t B-DatasetName
abfact I-DatasetName
and O
( O
yin O
et O
al O
. O
, O

2019 O
) O
for O
wikisql B-DatasetName
, O
( O
eisenschlos O
et O
al O
. O
, O

for O
t B-DatasetName
abfact I-DatasetName
the O
median O
is O
in O
the O
margin B-MetricName
error I-MetricName
but O
the O
best O
model O
using O
the O
joint O
learning O
outperforms O
the O
other O
learning O
strategies O
. O

j B-MethodName
- I-MethodName
dot I-MethodName
achieves O
higher O
accuracy B-MetricName
for O
similar O
efﬁciency O
, O
for O
both O
wikisql B-DatasetName
and O
wikitq B-DatasetName
. O

the O
second O
strategy O
, B-MethodName
jp I-MethodName
- I-MethodName
dot I-MethodName
is O
a O
hybrid O
method O
where O
the O
joint O
learning O
is O
used O
along O
with O
an O
additional O
pruning O
loss O
. O

p O
- O
dot B-MethodName
disables O
the O
buck O
- O
propagation O
to O
the O
pruning O
transformer O
through O
the O
attention O
scores O
, O
instead O
it O
uses O
an O
additional O
pruning O
loss O
similar O
to O
the O
task O
- O
speciﬁc O
loss O
. O

we O
compare O
a O
joint O
learning O
j O
- O
dot B-MethodName
model O
– O
that O
enables O
the O
back O
propagation O
of O
the O
pruning O
transformer O
by O
modifying O
the O
attention O
scores O
– O
to O
two O
differentdot O
models O
where O
we O
modify O
the O
learning O
strategy O
. O

c dot O
indicates O
the O
column O
based O
selection O
: O
for O
each O
token O
from O
one O
column O
, O
the O
pruning O
score O
attributes O
a O
column O
score O
instead O
of O
a O
token O
score O
. O

d O
all O
models O
results O
we O
report O
all O
dot B-MethodName
results O
in O
table O
11 O
. O

for O
tabfact B-DatasetName
the O
median O
is O
in O
the O
margin B-MetricName
error B-MetricName
but O
the O
best O
model O
using O
the O
joint O
learning O
outperforms O
the O
other O
learning O
strategies O
. O

table O
10 O
shows O
that O
for O
both O
wikisql B-DatasetName
andwikitq B-DatasetName
joint O
learning O
achieves O
higher O
accuracy B-MetricName
for O
similar O
efﬁciency O
. O

2020 O
) O
where O
the O
attention O
scores O
are O
not O
affected O
by O
the O
pruning O
scores O
. O

both O
are O
similar O
to O
jscalar O
deﬁned O
by O
( O
herzig O
et O
al O
. O
, O

we O
not O
jpruning scalar O
the O
pruning O
loss O
and O
jtask specific scalar O
the O
task O
- O
speciﬁc O
loss O
. O

this O
raises O
a O
question O
of O
whether O
adding O
a O
pruning O
loss O
similar O
to O
the O
task O
- O
speciﬁc O
loss O
can O
improve O
the O
end O
- O
toend O
accuracy B-MetricName
. O

according O
to O
the O
analysis O
done O
in O
section O
5 O
, O
the O
pruning O
model O
– O
jointly O
learned O
– O
is O
selecting O
the O
tokens O
to O
solve O
the O
main O
task O
. O

one O
cause O
could O
be O
the O
hyper O
- O
parameters O
tuning O
as O
we O
do O
not O
tune O
the O
hyper O
parameters O
for O
dot B-MethodName
.c.2 O
choice O
of O
joint O
learning O
: O
is O
it O
better O
to O
impose O
the O
meaning O
of O
relevant O
tokens O
? O

this O
indicates O
that O
the O
pruning O
model O
extracts O
twice O
more O
relevant O
tokens O
than O
the O
heuristiccc B-MethodName
. O

for O
the O
bucket O
[ O
512;1024 B-HyperparameterValue
] O
, O
dot B-MethodName
model O
gives O
close O
results O
to O
512 B-HyperparameterValue
length O
baseline O
. O

for O
a O
length B-HyperparameterName
> O
1024 B-HyperparameterValue
, O
thedot B-MethodName
model O
outperforms O
the O
256and B-HyperparameterValue
512length B-HyperparameterValue
baselines O
for O
all O
tasks O
. O

we O
use O
dot(m256  !l O
) O
model O
for O
the O
three O
datasets O
, O
a O
token O
based O
pruning O
for O
both O
wikisql B-DatasetName
andtabfact I-DatasetName
and O
a O
column O
based O
pruning O
for B-DatasetName
wikisql I-DatasetName
. O

table O
9 O
reports O
the O
accuracy B-MetricName
results O
computed O
over O
the O
test O
set O
for O
all O
buckets O
. O

c.1 O
pruning O
transformer O
enables O
reaching O
high O
accuracy B-MetricValue
for O
long O
input O
sequences O
to O
study O
the O
model O
accuracy B-MetricName
on O
different B-HyperparameterName
input I-HyperparameterName
sequence I-HyperparameterName
lengths I-HyperparameterName
, O
we O
bucketize O
the O
datasets O
. O

the O
column O
based O
dot O
models O
have O
the O
same O
number O
of O
parameters O
than O
the O
token O
based O
dot B-MethodName
models O
. O

2019 O
) O
83:9 O
( O
a O
) O
state O
- O
of O
- O
the O
- O
art O
wikisql B-DatasetName
model O
test O
accuracy B-MetricName
( O
zhong O
et O
al O
. O
, O

the O
formula O
to O
count O
the O
number O
of O
parameters O
is O
given O
by O
table O
7 O
. O

the O
sequence B-HyperparameterName
length I-HyperparameterName
changes O
the O
total O
number O
of O
used O
parameters O
. O

these O
models O
correspond O
to O
the O
bert O
open O
sourced O
model B-HyperparameterName
sizes I-HyperparameterName
described O
in O
turc O
et O
al O
. O
( O

b.3 O
models O
complexity O
in O
all O
our O
experiments O
we O
use O
different O
transformer O
sizes O
called O
large O
, O
medium O
, O
small O
and O
mini O
. O

b.2 O
state O
- O
of O
- O
the O
- O
art O
we O
report O
state O
- O
of O
- O
the O
- O
art O
for O
the O
three O
datasets O
in O
table O
5 O
. O

additionally O
, O
we O
use O
an O
adam O
optimizer O
with O
weight O
decay O
for O
all O
the O
baselines O
and O
dot O
models O
– O
the O
same O
conﬁguration O
as O
bert O
. O

we O
report O
the O
models O
hyper O
parameters O
used O
fortapas B-MethodName
baselines O
and O
dot B-MethodName
in O
table O
4 O
. O

baselines O
and O
dot B-MethodName
are O
initialized O
from O
models O
pre O
- O
trained O
with O
a O
mask O
- O
lm O
task O
and O
on O
sqa(iyyer O
et O
al O
. O
, O

2020 O
) O
and O
for O
tabfact B-DatasetName
the O
one O
used O
by O
( O
eisenschlos O
et O
al O
. O
, O

forwikisql B-DatasetName
andwikitq B-DatasetName
we O
use O
the O
same O
hyper O
- O
parameters O
as O
the O
one O
used O
by O
( O
herzig O
et O
al O
. O
, O

these O
hyper O
- O
parameters O
are O
the O
same O
for O
all O
the O
baselines O
and O
dot B-MethodName
models O
. O

reports O
the O
learning B-HyperparameterName
rate I-HyperparameterName
( O
lr B-HyperparameterName
) O
, O
the B-HyperparameterName
warmup I-HyperparameterName
ratio I-HyperparameterName
( O
 O
) O
, O
the O
hidden O
dropout O
, O
the O
attention O
dropout O
and O
the B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName
steps I-HyperparameterName
( B-HyperparameterName
num I-HyperparameterName
steps I-HyperparameterName
) O
used O
for O
each O
dataset O
. O

we O
estimate O
the O
error B-MetricName
margin I-MetricName
as O
half O
the O
inter O
quartile O
range O
, O
that O
is O
half O
the O
difference O
between O
the O
25th B-MetricValue
and75thpercentiles B-MetricValue
. O

in O
our O
experiment O
we O
report O
only O
the O
results O
for O
the O
model O
with O
input B-HyperparameterName
length I-HyperparameterName
= O
k. B-HyperparameterValue
this O
makes O
the O
attention O
scores O
similar O
to O
the O
ones O
computed O
over O
t O
= O
2i. O
b O
experiments O
in O
all O
the O
experiment O
we O
report O
the O
median O
accuracy B-MetricName
and O
the O
error B-MetricName
margin I-MetricName
computed O
over O
3 O
runs O
. O

we O
experimented O
with O
a O
task O
- O
speciﬁc O
model O
with O
a O
big O
input B-HyperparameterName
length I-HyperparameterName
> O
k B-HyperparameterValue
and O
compare O
it O
to O
a O
task O
- O
speciﬁc O
model O
with B-HyperparameterName
input I-HyperparameterName
length I-HyperparameterName
= O
k. B-HyperparameterValue
the O
two O
models O
gives O
similar O
accuracy B-TaskName
. O

as O
future O
work O
we O
will O
explore O
hierarchical O
pruning O
and O
adaptdot B-MethodName
to O
other O
semi O
- O
structured O
nlp O
tasks O
. O

this O
accelerates O
the O
training B-MetricName
and I-MetricName
inference I-MetricName
time I-MetricName
at O
a O
low O
drop O
in O
accuracy B-MetricName
. O

7 O
conclusion O
we O
introduced O
double B-MethodName
transformer I-MethodName
( O
dot B-MethodName
) O
where O
an O
additional O
small O
model O
prunes O
the O
input O
of O
a O
larger O
second O
model O
. O

to O
the O
best O
of O
our O
knowledge O
these O
methods O
have O
not O
been O
investigated O
in O
the O
context O
of O
semi O
- O
structured O
data O
such O
as O
tables O
or O
evaluated O
with O
a O
focus O
on O
efﬁciency O
. O

we O
rely O
on O
a O
soft O
attention O
mask O
instead O
as O
a O
way O
to O
partially O
reduce O
the O
information O
coming O
from O
some O
tokens O
during O
training O
. O

partially O
masked O
tokens O
are O
then O
replaced O
at O
the O
input O
embedding O
layer O
by O
some O
linear O
interpolation O
. O

2017 O
) O
, O
based O
on O
reparametrization O
( O
diederik O
and O
max O
, O
2014 O
) O
to O
approximate O
the O
discrete O
choice O
of O
a O
rationale O
from O
an O
input O
text O
, O
before O
using O
it O
as O
input O
for O
a O
classiﬁer O
. O

2019 O
) O
propose O
instead O
using O
stochastic O
computation O
nodes O
and O
continuous O
relaxations O
( O
maddison O
et O
al O
. O
, O

2016 O
) O
learn O
the O
rationale O
as O
a O
latent O
discrete O
variable O
inside O
a O
computation O
graph O
with O
the O
reinforce O
method O
( O
williams O
, O
1992 O
) O
. O

2016 O
) O
, O
which O
are O
a O
subset O
of O
words O
in O
the O
input O
text O
that O
serve O
as O
a O
justiﬁcation O
for O
the O
prediction O
. O

interpretable O
nlp O
another O
related O
line O
of O
work O
attempts O
to O
interpret O
neural O
networks O
by O
searching O
forrationales O
( O
lei O
et O
al O
. O
, O

2020 O
) O
explore O
heuristic B-MethodName
methods I-MethodName
based O
on O
lexicaloverlap O
and O
apply O
it O
to O
tasks O
involving O
tabular O
data O
, O
as O
we O
do O
, O
but O
our O
algorithm O
is O
learned O
end O
- O
to O
- O
end O
and O
more O
general O
in O
nature O
. O

our O
method O
most O
closely O
resembles O
the O
last O
category O
, O
but O
we O
focus O
our O
efforts O
on O
shrinking O
the O
sequence O
length O
of O
the O
input O
instead O
of O
model O
weights O
. O

2020 O
) O
who O
used O
structured O
dropout O
to O
reduce O
transformer O
depth O
at O
inference O
time O
. O

the O
fourth O
category O
is O
to O
use O
pruning O
strategies O
such O
as O
mccarley O
( O
2019 O
) O
, O
who O
studied O
structured O
pruning O
to O
reduce O
the O
number O
of O
parameters O
in O
each O
transformer O
layer O
, O
and O
fan O
et O
al O
. O
( O

the O
third O
category O
is O
to O
modify O
the O
transformer O
architecture O
to O
improve O
the O
dependence O
on O
the O
sequence O
length O
( O
choromanski O
et O
al O
. O
, O

the O
second O
category O
is O
to O
use O
quantization O
- O
aware O
training O
during O
the O
ﬁnetuning O
phase O
of O
bert O
models O
, O
such O
as O
( O
zafrir O
et O
al O
. O
, O

2019 O
) O
, O
or O
for O
both O
( O
jiao O
et O
al O
. O
, O

2019 O
) O
, O
or O
for O
building O
task O
- O
speciﬁc O
models O
( O
sun O
et O
al O
. O
, O

the O
ﬁrst O
is O
to O
use O
knowledge O
distillation O
, O
either O
during O
the O
pretraining O
phase O
( O
sanh O
et O
al O
. O
, O

6 O
related O
work O
efﬁcient O
transformers O
improving O
the O
computational O
efﬁciency O
of O
transformer O
models O
, O
especially O
for O
serving O
, O
is O
an O
active O
research O
topic O
. O

unlike O
the O
token O
selection O
the O
column O
pruning O
combined O
with O
hem B-MethodName
gives O
a O
lower O
accuracy B-MetricName
. O

for O
wikitq B-DatasetName
, O
the O
top O
- O
kpruning O
is O
a O
column O
based O
selection O
. O

combining O
the O
token O
based O
strategy O
with O
hem O
, O
outperforms O
on O
accuracy B-MetricName
the O
token O
pruning O
dot B-MethodName
combined O
withcc B-MethodName
. O

for O
both O
wikisql B-DatasetName
andtabfact B-DatasetName
we O
use O
a O
token O
based O
selection O
to O
select O
the O
top- O
ktokens O
. O

as O
both O
heuristics O
are O
applied O
in O
the O
pre O
- O
processing O
step O
, O
using O
hem B-MethodName
orcc B-MethodName
along O
with O
a O
similar O
dot B-MethodName
model O
, O
does O
n’t O
change O
the O
average B-MetricName
number I-MetricName
of I-MetricName
processed I-MetricName
examples I-MetricName
per I-MetricName
secondnpe I-MetricName
= O
s B-MetricName
computed O
over O
the O
training O
step O
. O

we O
use O
token O
selection O
for O
both O
wikisql B-DatasetName
and O
t B-DatasetName
abfact I-DatasetName
and O
column O
selection O
for O
wikitq B-DatasetName
. O

this O
ﬁgure O
compares O
the O
efﬁciency O
accuracy B-MetricName
trade O
- O
off O
of O
using O
different O
pruning O
transformers O
– O
medium O
, O
small O
and O
mini O
– O
and O
study O
the O
impact O
of O
hem B-MethodName
and O
ccondot B-MethodName
. O

the O
models O
are O
faster O
being O
closer O
to O
the O
right O
side O
of O
the O
ﬁgures O
and O
have O
higher O
accuracy B-MetricName
being O
closer O
to O
the O
top O
. O

the O
dot B-MethodName
models O
are O
displayed O
according O
to O
their O
accuracy B-MetricName
in O
function O
of O
the O
average O
number O
of O
processed O
examples O
par O
second O
. O

effects O
ofhem O
andccondot O
table O
1 O
and O
figure O
4 O
compare O
the O
effect O
of O
using O
hem B-MethodName
and3277 O
figure O
4 O
: O
dot B-MethodName
models O
efﬁciency O
accuracy B-MetricName
trade O
- O
off O
. O

for O
this O
dataset O
using O
medium O
pruning O
transformer O
, B-MethodName
dot(m)reaches I-MethodName
a O
better O
accuracy B-MetricName
efﬁciency O
tradeoff:2points O
higher O
in O
accuracy O
compared O
to O
using O
a O
small O
transformer O
. O

we O
reduce O
the O
task O
complexity O
by O
using O
column O
selection O
instead O
of O
token O
selection O
. O

thus O
selecting O
the O
top O
256tokens O
is O
a O
harder O
task O
compared O
to O
previous O
detests O
. O

this O
dataset O
is O
more O
complex O
, O
it O
requires O
more O
reasoning O
including O
operation O
to O
run O
over O
multiple O
cells O
in O
one O
column O
. O

even O
by O
increasing O
the O
pruning O
complexity O
, O
dot O
can O
not O
recover O
the O
full O
drop O
. O

in O
other O
words O
there O
is O
no O
gain O
of O
using O
a O
more O
complex O
model O
to O
select O
the O
top- O
ktokens O
especially O
when O
we O
restrict O
kto256 O
. O

for O
both O
wikisql B-DatasetName
andtabfact B-DatasetName
the O
small O
model O
reaches O
a O
better O
accuracy B-MetricName
efﬁciency O
trade O
- O
off O
: O
using O
a O
small O
instead O
of O
medium O
– O
4hidden O
layers O
instead O
of O
8 O
– O
drops O
the O
accuracy B-MetricName
by O
less O
than O
0:4 B-MetricValue
% I-MetricValue
– O
in O
the O
margin O
error B-MetricName
– O
while O
accelerating O
the O
model O
times O
1:3 B-MetricValue
. O

tokens O
and O
attribute O
token O
scores O
that O
can O
be O
used O
by O
the O
task O
- O
speciﬁc O
transformer O
. O

the O
difference O
is O
larger O
when O
the O
pruning O
transformer O
attributes O
a O
higher O
score O
to O
the O
answer O
tokens O
. O

the O
pruning O
transformer O
must O
be O
deep O
enough O
to O
learn O
the O
top- O
k O
figure O
3 O
: O
distribution O
of O
the O
answer O
token O
scores O
minus O
the O
average O
scores O
of O
the O
top- O
ktokens O
. O

for O
all O
datasets O
the O
mini O
model O
drops O
drastically O
the O
accuracy B-MetricName
. O

figure O
4 O
compares O
the O
results O
of O
medium O
, O
small O
and O
mini O
models O
– O
complexity O
in O
appendix O
b.3 O
. O

pruning O
transformer O
depth O
we O
study O
the O
pruning O
transformer O
complexity O
impact O
on O
the O
efﬁciency O
accuracy B-MetricName
trade O
- O
off O
. O

the O
difference O
is O
lower O
for O
wikitq B-DatasetName
as O
it O
is O
a O
harder O
task O
: O
the O
set O
of O
answer O
tokens O
is O
larger O
, O
especially O
for O
aggregation O
, O
making O
their O
scores O
closer O
to O
the O
average O
. O

the O
pruning O
transformer O
tends O
to O
attribute O
high O
scores O
to O
the O
answer O
tokens O
, O
suggesting O
that O
it O
learns O
to O
answer O
the O
downstream O
question O
– O
a O
positive O
difference O
– O
especially O
for O
wikisql B-DatasetName
. O

we O
compute O
the O
difference O
between O
the O
answer O
token O
scores O
and O
the O
average O
scores O
of O
the O
top- O
k O
tokens O
, O
and O
report O
the O
distribution O
in O
figure O
3 O
. O

the O
pruning O
transformer O
prunes O
efﬁciently O
– O
two O
times O
better O
: O
dot(256  !l)reaches B-MethodName
accuracy B-MetricName
close O
and O
higher O
than B-MethodName
usingcc512   I-MethodName
! O

for O
each O
dataset O
, O
the O
state O
- O
of O
- O
the O
- O
art O
, O
the O
best O
baseline O
model O
on O
accuracy B-MetricName
, O
and O
the O
dot B-MethodName
models O
that O
reach O
the O
best O
accuracy B-MetricName
efﬁciency O
trade O
- O
off O
are O
highlighted O
. O

we O
run O
dot O
with O
token O
pruning O
for O
wikisql B-DatasetName
and O
t B-DatasetName
abfact I-DatasetName
and O
column O
pruning O
for O
wikitq B-DatasetName
. O

thus O
, O
the O
task O
- O
speciﬁc O
transformer O
has O
access O
to O
less O
tokens O
, O
therefore O
to O
possibly O
less O
context O
that O
can O
lead O
to O
an O
accuracy B-MetricName
drop O
. O

tapas O
forwikisql B-DatasetName
andtabfact B-DatasetName
– O
in O
the O
margin O
error O
– O
and O
a O
slightly O
lower O
accuracy B-MetricName
for O
wikitq B-DatasetName
: O
the O
pruning O
transformer O
selects O
only O
256top O
- O
ktokens O
compared O
to512   O
! O

the O
results O
shows O
that O
dot B-MethodName
gives O
a O
similar O
accuracy B-MetricName
to512   O
! O

for O
the O
bucket O
[ O
512;1024 O
] O
, O
we O
expect O
all O
models O
to O
reach O
a O
higher O
accuracy B-MetricName
, O
as O
we O
expect O
lower O
loss O
of O
context O
than O
for O
the O
bucket O
> O
1024 O
when O
applyingcc B-MethodName
. O

this O
indicates O
that O
the B-MethodName
pruning I-MethodName
model I-MethodName
extracts O
two O
times O
more O
relevant O
tokens O
than O
the O
heuristic B-MethodName
cc B-MethodName
. O

for O
the O
bucket>1024 O
thedot B-MethodName
model O
outperforms O
the O
256and512length O
baselines O
for O
all O
tasks O
. O

5 O
analysis O
accuracy B-MetricName
for O
long O
input O
sequences O
to O
study O
the O
long O
inputs O
, O
we O
bucketize O
the O
datasets O
per O
example O
input O
length O
. O

dot B-MethodName
outperforms O
the O
smaller O
models O
showing O
the O
importance O
of O
using O
both O
transformers O
. O

small O
task O
models O
the O
previous O
results O
, O
raise O
the O
question O
of O
whether O
a O
smaller O
task O
model O
can O
reach O
a O
similar O
accuracy B-MetricName
. O

by O
restrictingdot(m)to B-MethodName
select O
only O
256tokens O
we O
decrease O
the O
accuracy B-MetricName
by O
a O
bigger O
drop O
3:9%to B-MetricValue
be O
3:5 B-MetricValue
times O
faster O
compared O
to O
hem1024    B-MethodName
! O

unlike O
the O
previous O
datasets O
, O
wikitq B-DatasetName
is O
a O
harder O
task O
to O
solve O
and O
requires O
passing O
more O
data O
. O

dot(s)still B-MethodName
achieves O
a O
good O
trade O
- O
off O
: O
with O
a O
decrease O
of O
0:4%of B-MetricValue
accuracy B-MetricName
it O
is O
1:5times B-MetricValue
faster O
. O

for O
tabfact B-DatasetName
dataset O
, O
dot B-MethodName
is O
compared O
to O
a O
faster O
baseline O
than O
the O
one O
used O
for O
wikisql B-DatasetName
as O
it O
takes O
only O
512input O
tokens O
instead O
of O
1024 O
. O

dot(m)anddot(s)reach B-MethodName
better O
efﬁciency O
accuracy B-MetricName
trade O
- O
off O
for O
wikisql B-DatasetName
: O
with O
a O
small O
drop O
of O
accuracy B-MetricName
by O
0:4%(respectively B-MetricValue
0:7 B-MetricValue
% I-MetricValue
) O
, O
they O
are O
3:5 B-MetricValue
( O
respectively O
4:6 B-MetricValue
) O
times O
faster O
than O
the O
best O
baseline O
. O

using O
hem B-MethodName
as O
pre O
- O
processing O
step O
improves O
dot B-MethodName
models O
com O
- O
pared O
toccfor B-MethodName
both O
wikisql B-DatasetName
andtabfact B-DatasetName
. O

efﬁciency O
accuracy B-MetricName
trade O
- O
off O
table O
1 O
reports O
the O
accuracy B-MetricName
test O
results O
along O
with O
the O
average B-MetricName
number I-MetricName
of I-MetricName
processed I-MetricName
examples I-MetricName
per I-MetricName
second I-MetricName
npe B-MetricName
= O
s B-MetricName
computed O
at O
training O
time O
. O

4 O
results O
the O
baseline O
tapas B-MethodName
model O
outperforms O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
on O
all O
datasets O
( O
table O
1 O
): O
+2:1 O
forwikisql B-DatasetName
( O
cc1024    B-MethodName
! O

in O
all O
our O
experiments O
we O
report O
results O
for O
dot B-MethodName
using O
token O
selection O
for O
wikisql B-DatasetName
andtabfact B-DatasetName
and O
a O
column O
selection O
for O
wikitq B-DatasetName
. O

the O
dataset O
requires O
both O
linguistic O
reasoning O
and O
symbolic O
reasoning O
with O
operations O
such O
as O
comparison O
, O
ﬁltering O
or O
counting O
. O

2019 O
) O
contains O
118 O
k O
statements O
about O
16 O
k O
wikipedia O
tables O
, O
labeled O
as O
either O
entailed O
or O
refuted O
. O

the O
metric O
we O
use O
is O
the O
denotation O
accuracy O
as O
computed B-TaskName
by O
the O
ofﬁcial O
evaluation O
script O
. O

the O
questions O
are O
complex O
and O
often O
require O
comparisons O
, O
superlatives O
or O
aggregation O
. O

wikitq B-DatasetName
( O
pasupat O
and O
liang O
, O
2015 O
) O
consists O
of O
22;033question O
- O
answer O
pairs O
on O
2;108wikipedia O
tables O
. O

here O
we O
train O
and O
test O
in O
the O
weakly O
- O
supervised O
setting O
where O
the O
answer O
to O
the O
question O
is O
the O
result O
of O
the O
sql O
applied O
to O
the O
table O
. O

2017 O
) O
is O
a O
corpus O
of O
80;654 O
questions O
with O
sql O
queries O
, O
related O
to O
24;241wikipedia O
tables O
. O

thedot B-MethodName
transformers O
’ O
complexity O
– O
detailed O
in O
appendix O
b.3 O
– O
is O
similar O
to O
a O
normal O
transformer O
where O
only O
some O
constants O
are O
changed O
. O

baselines O
and O
dot B-MethodName
( O
hyper O
- O
parameters O
in O
appendix O
b.1 O
) O
are O
initialized O
from O
models O
pre O
- O
trained O
with O
a O
mask O
- O
lm O
task O
, O
the O
intermediate O
pretraining O
data O
( O
eisenschlos O
et O
al O
. O
, O

for O
example O
, O
cc1024   !dot(s256  !l)denotes B-MethodName
accpreprocessing O
to O
select O
1024 O
tokens O
passed O
to O
the O
dot O
model O
: O
one O
small B-MethodName
pruning I-MethodName
model O
that O
selects O
256 O
tokens O
and O
feeds O
them O
into O
a O
large O
task O
model O
. O

thetype O
correspond O
to O
the O
model O
size O
: O
small O
( O
s O
) O
, O
medium O
( O
m O
) O
or O
large O
( O
l O
) O
as O
deﬁned O
in O
turc O
et O
al O
. O
( O

heuristic B-MethodName
exact I-MethodName
match I-MethodName
( O
hem B-MethodName
) O
( O
eisenschlos O
et O
al O
. O
, O

this O
is O
done O
by O
ﬁrst O
selecting O
the O
ﬁrst O
token O
from O
each O
cell O
, O
then O
the O
second O
and O
so O
on O
until O
the O
desired O
limit O
is O
reached O
. O

the O
objective O
of O
the O
algorithm O
is O
to O
ﬁt O
an O
equal O
number O
of O
tokens O
for O
each O
cell O
. O

cell B-MethodName
concatenation I-MethodName
( O
cc)thetapas B-MethodName
model O
uses O
a O
default O
heuristic O
to O
limit O
the O
input O
tokens O
. O

3 O
experimental O
setup O
we O
compare O
our O
approach O
against O
models O
using O
heuristic B-MethodName
pruning I-MethodName
. O

unlike O
previous O
soft O
- O
masking O
methods O
( O
bastings O
et O
al O
. O
, O

this O
enables O
back O
propagation O
for O
both O
models O
based O
on O
a O
single O
loss O
. O

the O
pruning O
scores O
affect O
the O
task O
model O
’s O
attention O
in O
all O
layers O
. O

2017 O
) O
– O
the O
dashed O
bloc O
– O
by O
adding O
the O
pruning O
scores O
– O
the O
solid O
bloc O
. O

we O
change O
the O
attention O
architecture O
( O
vaswani O
et O
al O
. O
, O

each O
row O
of O
the O
attention O
matrix O
is O
obtained O
by O
a O
softmax O
on O
the O
attention O
scores O
z O
< O
t;t0 O
> O
given O
by O
matmul O
matmul O
scale O
st O
et O
wq O
et O
wk O
et O
wv O
pruning O
scores O
from O
the O
first O
transformer O
scaled O
dot O
product O
attention O
from O
the O
second O
transformer O
sumsoftmax O
figure O
2 O
: O
scaled O
dot O
product O
attention O
of O
the O
task O
model O
. O

2017 O
) O
, O
given O
the O
input O
embeddingetat O
positiont O
, O
for O
each O
layer O
and O
attention O
head O
, O
the O
self O
- O
attention O
output O
is O
given O
by O
a O
linear O
combination O
of O
the O
value O
vector O
projections O
using O
the O
attention O
matrix O
. O

to O
enable O
the O
joint O
learning O
, O
we O
change O
the O
attention O
scores O
of O
the O
task O
model O
. O

the O
pruning O
scores O
are O
then O
passed O
to O
the O
task O
transformer O
as O
shown O
in O
figure O
2 O
. O

letqbe O
the O
query O
( O
or O
statement O
) O
and O
tthe O
table O
. O

we O
explore O
learning O
the O
pruning B-MethodName
model I-MethodName
using O
an O
additional O
loss O
in O
appendix O
c.2 O
. O

2 O
the O
dot B-MethodName
model O
as O
show O
in O
figure O
1 O
, O
the O
double B-MethodName
transformer I-MethodName
dot B-MethodName
is O
composed O
of O
two O
transformers O
: O
the O
pruning O
transformer O
selects O
the O
most O
relevant O
ktokens O
followed O
by O
a O
task O
- O
speciﬁc O
model O
that O
operates O
on O
the O
selected O
tokens O
to O
solve O
the O
task O
. O

we O
study O
the O
meaning O
of O
relevant O
tokens O
and O
show O
that O
the O
selection O
is O
deeply O
linked O
to O
solving O
the O
main O
task O
by O
studying O
the O
answer O
token O
scores O
. O

we O
show O
that O
the O
pruning O
transformer O
selects O
relevant O
tokens O
, O
resulting O
in O
higher O
accuracy B-MetricName
for O
longer O
input O
sequences O
. O

the O
pruning B-MethodName
model I-MethodName
is O
small O
, O
allowing O
the O
use O
of O
long O
input O
sequences O
. O

the O
pruning O
model O
selects O
the O
kmost O
relevant O
tokens O
and O
passes O
them O
to O
the O
task O
model O
. O

in O
section O
2 O
, O
we O
explain O
how O
we O
jointly O
learn O
both O
models O
by O
incorporating O
the O
pruning O
scores O
into O
the O
attention O
mechanism O
. O

the O
second O
transformer O
is O
a O
task O
- O
speciﬁc O
model O
adapted O
for O
each O
task O
to O
solve O
: O
we O
use O
another O
tapas O
qa B-TaskName
model O
for O
qa B-TaskName
and O
a O
classiﬁcation O
model O
( O
eisenschlos O
et O
al O
. O
, O

tapas O
answers O
questions O
by O
selecting O
tokens O
from O
a O
given O
table O
. O

the O
pruning O
transformer O
is O
based O
on O
the O
tapas O
qa O
model O
( O
herzig O
et O
al O
. O
, O

the O
combined O
model O
achieves O
a O
better O
efﬁciency B-MetricName
- O
accuracy B-MetricName
trade O
- O
off O
. O

decomposing O
the O
problem O
into O
two O
simpler O
tasks O
imposes O
additional O
structure O
that O
makes O
training O
more O
efﬁcient O
: O
the O
ﬁrst O
model O
is O
shallow O
, O
allowing O
the O
use O
of O
long O
input O
sequences O
at O
moderate O
cost O
, O
and O
the O
second O
model O
is O
deeper O
and O
uses O
the O
shortened O
input O
that O
solves O
the O
task O
. O

we O
propose O
to O
use O
dot B-MethodName
, O
a O
double B-MethodName
transformer I-MethodName
model O
( O
figure O
1 O
): O
a O
ﬁrst O
transformer O
– O
which O
we O
callpruning O
transformer O
– O
selectsktokens O
given O
a O
query O
and O
a O
table O
and O
a O
task O
- O
speciﬁc O
transformer O
solves O
the O
task O
based O
on O
these O
tokens O
. O

this O
raises O
the O
question O
of O
whether O
a O
better O
pruning O
strategy O
can O
be O
learned O
. O

2019 O
) O
that O
using O
heuristic B-MethodName
pruning I-MethodName
accelerates O
the O
training O
time O
while O
achieving O
a O
similar O
accuracy B-MetricName
. O

2020 O
) O
show O
on O
the O
tabfactdata B-DatasetName
set O
( O
wenhu O
et O
al O
. O
, O

to O
the O
best O
of O
our O
knowledge O
, O
the O
only O
technique O
that O
was O
applied O
to O
nlp O
tasks O
with O
semistructured O
tables O
is O
heuristic B-MethodName
pruning I-MethodName
. O

improving O
the O
computational O
efﬁciency O
of O
transformer O
models O
has O
recently O
become O
an O
active O
research O
topic O
. O

using O
longer O
sequence O
lengths O
translates O
into O
increased O
training O
and O
inference O
time O
. O

2017 O
) O
, O
where O
nis O
the O
input O
sequence O
length O
, O
and O
dis O
the O
embedding O
dimension O
. O

the O
total O
computational O
complexity O
per O
layer O
for O
self O
- O
attention O
is O
o(n2d)(vaswani O
et O
al O
. O
, O

while O
transformer O
models O
lead O
to O
signiﬁcant O
improvements O
in O
accuracy B-MetricName
, O
they O
suffer O
from O
high O
work O
done O
at O
google O
research.computation O
and O
memory O
cost O
, O
especially O
for O
large O
inputs O
. O

2020 O
) O
– O
a O
binary O
classiﬁcation O
task O
to O
support O
or O
refute O
a O
sentence O
based O
on O
the O
table O
’s O
content O
. O

in O
particular O
, O
transformer O
models O
have O
been O
used O
to O
solve O
tasks O
that O
include O
semi O
- O
structured O
table O
knowledge O
, O
such O
as O
table O
question B-TaskName
answering I-TaskName
( O
herzig O
et O
al O
. O
, O

1 O
introduction O
recently O
, O
transfer O
learning O
with O
large O
- O
scale O
pretrained O
language O
models O
has O
been O
successfully O
used O
to O
solve O
many O
nlp O
tasks O
( O
devlin O
et O
al O
. O
, O

finally O
, O
we O
analyse O
the O
pruning O
and O
give O
some O
insight O
into O
its O
impact O
on O
the O
task O
model O
. O

we O
also O
show O
that O
the O
pruning O
transformer O
effectively O
selects O
relevant O
tokens O
enabling O
the O
end O
- O
to O
- O
end O
model O
to O
maintain O
similar O
accuracy B-MetricName
as O
slower O
baseline O
models O
. O

we O
show O
that O
for O
a O
small O
drop O
of O
accuracy B-MetricName
, O
dot B-MethodName
improves O
training O
and O
inference O
time O
by O
at O
least O
50 B-MetricValue
% I-MetricValue
. O

we O
run O
experiments O
on O
three O
benchmarks O
, O
including O
entailment B-TaskName
and O
question B-TaskName
- I-TaskName
answering I-TaskName
. O

the O
two O
transformers O
are O
jointly O
trained O
by O
optimizing O
the O
task O
- O
speciﬁc O
loss O
. O

additionally O
, O
we O
modify O
the O
task O
- O
speciﬁc O
attention O
to O
incorporate O
the O
pruning O
scores O
. O

to O
improve O
efﬁciency O
while O
maintaining O
a O
high O
accuracy O
, O
we O
propose O
a O
new O
architecture O
, O
dot B-MethodName
, O
a O
double B-MethodName
transformer I-MethodName
model O
, O
that O
decomposes O
the O
problem O
into O
two O
sub O
- O
tasks O
: O
a O
shallow O
pruning O
transformer O
that O
selects O
the O
top- O
ktokens O
, O
followed O
by O
a O
deep O
task O
- O
speciﬁc O
transformer O
that O
takes O
as O
input O
those O
k O
tokens O
. O

these O
model O
architectures O
are O
typically O
deep O
, O
resulting O
in O
slow O
training O
and O
inference O
, O
especially O
for O
long O
inputs O
. O

acknowledgments O
we O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
. O

we O
hope O
that O
our O
results O
will O
catalyze O
new O
developments O
in O
transfer O
learning O
for O
nlp O
. O

our O
method O
signiﬁcantly O
outperformed O
existing O
transfer O
learning O
techniques O
and O
the O
stateof O
- O
the O
- O
art O
on O
six O
representative O
text B-TaskName
classiﬁcation I-TaskName
tasks O
. O

we O
have O
also O
proposed O
several O
novel B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
techniques O
that O
in O
conjunction O
prevent O
catastrophic O
forgetting O
and O
enable O
robust O
learning O
across O
a O
diverse O
range O
of O
tasks O
. O

7 O
conclusion O
we O
have O
proposed O
ulmfit B-MethodName
, O
an O
effective O
and O
extremely O
sample O
- O
efﬁcient O
transfer O
learning O
method O
that O
can O
be O
applied O
to O
any O
nlp O
task O
. O

finally O
, O
while O
we O
have O
provided O
a O
series O
of O
analyses O
and O
ablations O
, O
more O
studies O
are O
required O
to O
better O
understand O
what O
knowledge O
a O
pretrained O
language O
model O
captures O
, O
how O
this O
changes O
during O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
, O
and O
what O
information O
different O
tasks O
require O
. O

while O
an O
extension O
to O
sequence O
labeling O
is O
straightforward O
, O
other O
tasks O
with O
more O
complex O
interactions O
such O
as O
entailment O
or O
question B-TaskName
answering I-TaskName
may O
require O
novel O
ways O
to O
pretrain B-MethodName
and O
ﬁne B-MethodName
- I-MethodName
tune I-MethodName
. O

another O
direction O
is O
to O
apply O
the O
method O
to O
novel O
tasks O
and O
models O
. O

2016 O
) O
to O
create O
a O
model O
that O
is O
more O
general O
or O
better O
suited O
for O
certain O
downstream O
tasks O
, O
ideally O
in O
a O
weakly O
- O
supervised O
manner O
to O
retain O
its O
universal O
properties O
. O

language O
modeling O
can O
also O
be O
augmented O
with O
additional O
tasks O
in O
a O
multi O
- O
task O
learning O
fashion O
( O
caruana O
, O
1993 O
) O
or O
enriched O
with O
additional O
supervision O
, O
e.g. O
syntax O
- O
sensitive O
dependencies O
( O
linzen O
et O
al O
. O
, O

2018)—focusing O
on O
predicting O
a O
subset O
of O
words O
such O
as O
the O
most O
frequent O
ones O
might O
retain O
most O
of O
the O
performance O
while O
speeding O
up O
training O
. O

2016 O
) O
, O
while O
recent O
work O
shows O
that O
an O
alignment O
between O
source O
and O
target O
task O
label O
sets O
is O
important O
( O
mahajan O
et O
al O
. O
, O

one O
possible O
direction O
is O
to O
improve O
language O
model O
pretraining B-MethodName
and B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
and O
make O
them O
more O
scalable O
: O
for O
imagenet O
, O
predicting O
far O
fewer O
classes O
only O
incurs O
a O
small O
performance O
drop O
( O
huh O
et O
al O
. O
, O

2018 O
): O
a O
) O
nlp O
for O
non O
- O
english O
languages O
, O
where O
training O
data O
for O
supervised O
pretraining O
tasks O
is O
scarce O
; O
b O
) O
new O
nlp O
tasks O
where O
no O
state O
- O
of O
- O
the O
- O
art O
architecture O
exists O
; O
and O
c O
) O
tasks O
with O
limited O
amounts O
of O
labeled O
data O
( O
and O
some O
amounts O
of O
unlabeled O
data).given O
that O
transfer O
learning O
and O
particularly O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
for O
nlp O
is O
under O
- O
explored O
, O
many O
future O
directions O
are O
possible O
. O

6 O
discussion O
and O
future O
directions O
while O
we O
have O
shown O
that O
ulmfit O
can O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
widely O
used O
text B-TaskName
classiﬁcation I-TaskName
tasks O
, O
we O
believe O
that O
language O
model B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
will O
be O
particularly O
useful O
in O
the O
following O
settings O
compared O
to O
existing O
transfer O
learning O
approaches O
( O
conneau O
et O
al O
. O
, O

on O
imdb B-DatasetName
we O
lower O
the O
test O
error B-MetricName
from O
5:30of B-MetricValue
a O
single O
model O
to4:58for B-MetricValue
the O
bidirectional O
model O
. O

impact O
of O
bidirectionality O
at O
the O
cost O
of O
training O
a O
second O
model O
, O
ensembling O
the O
predictions O
of O
a O
forward O
and O
backwards O
lm O
- O
classiﬁer O
brings O
a O
performance O
boost O
of O
around B-MetricValue
0:5–0:7 I-MetricValue
. O

in O
contrast O
, O
ulmfit B-MethodName
is O
more O
stable O
and O
suffers O
from O
no O
such O
catastrophic O
forgetting O
; O
performance O
remains O
similar O
or O
improves O
until O
late O
epochs O
, O
which O
shows O
the O
positive O
effect O
of O
the O
learning O
rate O
schedule O
. O

the O
error B-MetricName
then O
increases O
as O
the O
model O
starts O
to O
overﬁt O
and O
knowledge O
captured O
through O
pretraining O
is O
lost O
. O

on O
all O
datasets O
, B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
the B-MethodName
full I-MethodName
model I-MethodName
leads O
to O
the O
lowest O
error O
comparatively O
early O
in O
training O
, O
e.g. O
already O
after O
the O
ﬁrst O
epoch O
on O
imdb.336 B-DatasetName
figure O
4 O
: O
validation B-MetricName
error I-MetricName
rate I-MetricName
curves O
for O
ﬁnetuning B-MethodName
the O
classiﬁer O
with O
ulmfit B-MethodName
and O
‘ O
full B-MethodName
’ O
on O
imdb B-DatasetName
, B-DatasetName
trec-6 I-DatasetName
, O
and O
ag B-DatasetName
( O
top O
to O
bottom O
) O
. O

to O
better O
understand O
the B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
behavior O
of O
our O
model O
, O
we O
compare O
the O
validation O
error B-MetricName
of O
the O
classiﬁer O
ﬁne O
- O
tuned O
with O
ulmfit B-MethodName
and O
‘ O
full B-MethodName
’ O
during O
training O
in O
figure O
4 O
. O

classiﬁer O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
behavior O
while O
our O
results O
demonstrate O
that O
how O
we B-MethodName
ﬁne I-MethodName
- I-MethodName
tune I-MethodName
the O
classiﬁer O
makes O
a O
signiﬁcant O
difference O
, B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
for O
inductive O
transfer O
is O
currently O
under O
- O
explored O
in O
nlp O
as O
it O
mostly O
has O
been O
thought O
to O
be O
unhelpful O
( O
mou O
et O
al O
. O
, O

importantly O
, O
ulmfit B-MethodName
is O
the O
only O
method O
that O
shows O
excellent O
performance O
across O
the O
board O
— O
and O
is O
therefore O
the O
only O
universal O
method O
. O

finally O
, O
full B-MethodName
ulmfit B-MethodName
classiﬁer B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
( O
bottom O
row O
) O
achieves O
the O
best O
performance O
on O
imdb B-DatasetName
and O
trec-6 B-DatasetName
and O
competitive O
performance O
on O
ag B-DatasetName
. O

cosine B-MethodName
annealing I-MethodName
is O
competitive O
with O
slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
on O
large O
data O
, O
but O
under O
- O
performs O
on O
smaller O
datasets O
. O

discr B-MethodName
’ O
consistently O
boosts O
the O
performance O
of O
‘ O
full B-MethodName
’ O
and O
‘ O
freez B-MethodName
’ O
, O
except O
for O
the O
large O
ag B-DatasetName
. O

freez B-MethodName
’ O
provides O
similar O
performance O
as O
‘ O
full B-MethodName
’ O
. O
‘ O

chainthaw O
’ O
achieves O
competitive O
performance O
on O
the O
smaller O
datasets O
, O
but O
is O
outperformed O
signiﬁcantly O
on O
the O
large O
ag B-DatasetName
. O
‘ O

last O
’ O
, O
the O
standard B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
method O
in O
cv O
, O
severely O
underﬁts O
and O
is O
never O
able O
to O
lower O
the O
training O
error B-MetricName
to O
0 O
. O
‘ O

fine B-MethodName
- I-MethodName
tuning I-MethodName
the O
classiﬁer O
signiﬁcantly O
improves O
over O
training O
from B-MethodName
scratch I-MethodName
, O
particularly O
on O
the O
small O
trec-6 B-DatasetName
. O
‘ O

we O
use O
a O
learning B-HyperparameterName
rate I-HyperparameterName
l= B-HyperparameterName
0:01for B-HyperparameterValue
‘ O
discr B-MethodName
’ O
, O
learning B-HyperparameterName
rates I-HyperparameterName
8to B-HyperparameterValue
avoid O
overﬁtting O
, O
we O
only O
train O
the O
vanilla B-MethodName
lm I-MethodName
classiﬁer O
for O
5epochs B-HyperparameterValue
and O
keep O
dropout B-HyperparameterName
of O
0:4 B-HyperparameterValue
in O
the O
classiﬁer O
. O

we O
compare O
the O
latter O
to O
an O
alternative O
, O
aggressive O
cosine B-MethodName
annealing I-MethodName
schedule I-MethodName
( O
‘ O
cos B-MethodName
’ O
) O
( O
loshchilov O
and O
hutter O
, O
2017 O
) O
. O

we O
furthermore O
assess O
the O
importance O
of O
discriminative B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
( O
‘ O
discr B-MethodName
’ O
) O
and O
slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
( O
‘ O
stlr B-MethodName
’ O
) O
. O

2017 O
) O
, O
and B-MethodName
gradual I-MethodName
unfreezing I-MethodName
( O
‘ O
freez B-MethodName
’ O
) O
. O

2014 O
) O
, O
‘ O
chain O
- O
thaw O
’ O
( O
felbo O
et O
al O
. O
, O

impact O
of O
classiﬁer B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
we O
compare O
training O
from O
scratch O
, B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
the B-MethodName
full I-MethodName
model I-MethodName
( O
‘ O
full B-MethodName
’ O
) O
, O
only O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
the I-MethodName
last I-MethodName
layer I-MethodName
( O
‘ O
last B-MethodName
’ O
) O
( O
donahue O
et O
al O
. O
, O

discr B-MethodName
’ O
and O
‘ O
stlr B-MethodName
’ O
improve O
performance O
across O
all O
three O
datasets O
and O
are O
necessary O
on O
the O
smaller B-DatasetName
trec-6 I-DatasetName
, O
where O
regular B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
is O
not O
beneﬁcial O
. O

fine B-MethodName
- I-MethodName
tuning I-MethodName
the O
lm O
is O
most O
beneﬁcial O
for O
larger O
datasets O
. O
‘ O

2010 O
) O
( O
‘ O
full B-MethodName
’ O
) O
, O
the O
most O
commonly O
used B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
method O
, O
with O
and O
without O
discriminative B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
( O
‘ O
discr B-MethodName
’ O
) O
and O
slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
( O
‘ O
stlr B-MethodName
’ O
) O
in O
table O
6 O
. O

impact O
of O
lm O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
we O
compare O
no O
ﬁnetuning B-MethodName
against O
ﬁne O
- O
tuning O
the O
full O
model O
( O
erhan O
et O
al O
. O
, O

on O
the O
smaller B-DatasetName
trec-6 I-DatasetName
, O
a O
vanilla O
lm O
without O
dropout O
runs O
the O
risk O
of O
overﬁtting O
, O
which O
decreases O
performance O
. O

using O
our O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
techniques O
, O
even O
a O
regular O
lm O
reaches O
surprisingly O
good O
performance O
on O
the O
larger O
datasets O
. O

impact O
of O
lm O
quality O
in O
order O
to O
gauge O
the O
importance O
of O
choosing O
an O
appropriate O
lm O
, O
we O
compare O
a O
vanilla O
lm O
with O
the O
same O
hyperparameters O
without O
any O
dropout8with O
the O
awd B-MethodName
- I-MethodName
lstm I-MethodName
lm I-MethodName
with O
tuned O
dropout O
parameters O
in O
table O
5 O
. O

however O
, O
even O
for O
large O
datasets O
, O
pretraining B-MethodName
improves O
performance.335lm O
imdb B-DatasetName
trec-6 B-DatasetName
ag B-DatasetName
vanilla B-MethodName
lm I-MethodName
5.98 O
7.41 O
5.76 O
awd B-MethodName
- I-MethodName
lstm I-MethodName
lm B-MethodName
5.00 O
5.69 O
5.38 O
table O
5 O
: O
validation B-MetricName
error I-MetricName
rates I-MetricName
for O
ulmfit B-MethodName
with O
a O
vanilla B-MethodName
lm I-MethodName
and I-MethodName
the I-MethodName
awd I-MethodName
- I-MethodName
lstm I-MethodName
lm I-MethodName
. O

pretraining B-MethodName
is O
most O
useful O
for O
small O
and O
medium O
- O
sized O
datasets O
, O
which O
are O
most O
common O
in O
commercial O
applications O
. O

impact O
of O
pretraining B-MethodName
we O
compare O
using O
no O
pretraining B-MethodName
with O
pretraining B-MethodName
on O
wikitext-103 B-DatasetName
( O
merity O
et O
al O
. O
, O

on O
trec-6 B-DatasetName
, O
ulmfit B-MethodName
signiﬁcantly O
improves O
upon O
training O
from B-MethodName
scratch I-MethodName
; O
as O
examples O
are O
shorter O
and O
fewer O
, O
supervised O
and O
semi O
- O
supervised O
ulmfit B-MethodName
achieve O
similar O
results O
. O

if O
we O
allow O
ulmfit B-MethodName
to O
also O
utilize O
unlabeled O
examples O
( O
50k O
for O
imdb B-DatasetName
, O
100k O
for O
ag B-DatasetName
) O
, O
at O
100labeled O
examples O
, O
we O
match O
the O
performance O
of O
training O
from O
scratch O
with O
50and100more O
data O
on O
ag B-DatasetName
and O
imdb B-DatasetName
respectively O
. O

on O
imdb B-DatasetName
and O
ag B-DatasetName
, O
supervised O
ulmfit B-MethodName
with O
only O
100 O
labeled O
examples O
matches O
the O
performance O
of O
training O
from O
scratch O
with O
10and20 O
more O
data O
respectively O
, O
clearly O
demonstrating O
the O
beneﬁt O
of O
general O
- O
domain O
lm O
pretraining B-MethodName
. O

we O
split O
off O
balanced O
fractions O
of O
the O
training O
data O
, O
keep O
the O
validation O
set O
ﬁxed O
, O
and O
use O
the O
same O
hyperparameters O
as O
before O
. O

we O
compare O
ulmfit B-MethodName
to O
training O
from O
scratch O
— O
which O
is O
necessary O
for O
hypercolumn O
- O
based O
approaches O
. O

we O
evaluate O
ulmfit B-MethodName
on O
different O
numbers O
of O
labeled O
examples O
in O
two O
settings O
: O
only O
labeled O
examples O
are O
used O
for O
lm O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
( O
‘ O
supervised O
’ O
) O
; O
and O
all O
task O
data O
is O
available O
and O
can O
be O
used O
to O
ﬁne B-MethodName
- I-MethodName
tune I-MethodName
the O
lm O
( O
‘ O
semi O
- O
supervised O
’ O
) O
. O

we O
ﬁne B-MethodName
- I-MethodName
tune I-MethodName
the O
classiﬁer O
for O
50epochs B-HyperparameterValue
and O
train O
all O
methods O
but O
ulmfit B-MethodName
with O
early O
stopping O
. O

for O
all O
experiments O
, O
we O
split O
off O
10 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
training O
set O
and O
report O
error B-MetricName
rates I-MetricName
on O
this O
validation O
set O
with O
unidirectional O
lms O
. O

we O
run O
experiments O
on O
three O
corpora O
, O
imdb B-DatasetName
, O
trec6 B-DatasetName
, O
and O
ag B-DatasetName
that O
are O
representative O
of O
different O
tasks O
, O
genres O
, O
and O
sizes O
. O

5 O
analysis O
in O
order O
to O
assess O
the O
impact O
of O
each O
contribution O
, O
we O
perform O
a O
series O
of O
analyses O
and O
ablations O
. O

on O
dbpedia B-DatasetName
, O
yelp B-DatasetName
- I-DatasetName
bi I-DatasetName
, O
and B-DatasetName
yelp I-DatasetName
- I-DatasetName
full I-DatasetName
, O
we O
reduce O
the O
error B-MetricName
by O
4.8 B-MetricValue
% I-MetricValue
, O
18.2 B-MetricValue
% I-MetricValue
, O
2.0 B-MetricValue
% I-MetricValue
respectively O
. O

on O
ag O
, O
we O
observe O
a O
similarly O
dramatic O
error B-MetricName
reduction O
by O
23.7 B-MetricValue
% I-MetricValue
compared O
to O
the O
state O
- O
of O
- O
the O
- O
art O
. O

our O
method O
again O
outperforms O
the O
state O
- O
ofthe O
- O
art O
signiﬁcantly O
. O

we O
show O
the O
test O
error B-MetricName
rates I-MetricName
on O
the O
larger O
ag B-DatasetName
, O
dbpedia B-DatasetName
, O
yelp B-DatasetName
- I-DatasetName
bi I-DatasetName
, O
and O
yelp B-DatasetName
- I-DatasetName
full I-DatasetName
datasets O
in O
table O
3 O
. O

2017 O
) O
, O
we O
consistently O
outperform O
their O
approach O
on O
both O
datasets O
. O

note O
that O
despite O
pretraining B-MethodName
on O
more O
than O
two O
orders O
of O
magnitude O
less O
data O
than O
the O
7 O
million O
sentence O
pairs O
used O
by O
mccann O
et O
al O
. O
( O

nevertheless O
, O
the O
competitive O
performance O
on O
trec-6 B-DatasetName
demonstrates O
that O
our O
model O
performs O
well O
across O
different O
dataset O
sizes O
and O
can O
deal O
with O
examples O
that O
range O
from O
single O
sentences O
— O
in O
the O
case O
of O
trec-6 B-DatasetName
— O
to O
several O
paragraphs O
for O
imdb B-DatasetName
. O

on O
trec-6 B-DatasetName
, O
our O
improvement O
— O
similar O
as O
the O
improvements O
of O
state O
- O
of O
- O
the O
- O
art O
approaches O
— O
is O
not O
statistically O
signiﬁcant O
, O
due O
to O
the O
small O
size O
of O
the O
500 O
- O
examples O
test O
set O
. O

paragraphs O
long O
— O
similar O
to O
emails O
( O
e.g O
for O
legal O
discovery O
) O
and O
online O
comments O
( O
e.g O
for O
community O
management O
) O
; O
and O
sentiment B-TaskName
analysis I-TaskName
is O
similar O
to O
many O
commercial O
applications O
, O
e.g. O
product O
response O
tracking O
and O
support O
email O
routing O
. O

imdb B-DatasetName
in O
particular O
is O
reﬂective O
of O
realworld O
datasets O
: O
its O
documents O
are O
generally O
a O
few334 O
figure O
3 O
: O
validation O
error B-MetricName
rates I-MetricName
for O
supervised O
and O
semi O
- O
supervised O
ulmfit B-MethodName
vs. O
training O
from B-MethodName
scratch I-MethodName
with O
different O
numbers O
of O
training O
examples O
on O
imdb B-DatasetName
, O
trec-6 B-DatasetName
, O
and O
ag B-DatasetName
( O
from O
left O
to O
right O
) O
. O

we O
note O
that O
the O
language O
model O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
approach O
of O
dai O
and O
le O
( O
2015 O
) O
only O
achieves O
an O
error B-MetricName
of B-MetricValue
7.64 I-MetricValue
vs. O
4.6 B-MetricValue
for O
our O
method O
on O
imdb B-DatasetName
, O
demonstrating O
the O
beneﬁt O
of O
transferring O
knowledge O
from O
a O
large O
imagenet O
- O
like O
corpus O
using O
our O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
techniques O
. O

2017 O
) O
and O
sophisticated O
embedding O
schemes O
( O
johnson O
and O
zhang O
, O
2016 O
) O
, O
while O
our O
method O
employs O
a O
regular O
lstm O
with O
dropout O
. O

2018 O
) O
, O
multiple O
forms O
of O
attention O
( O
mccann O
et O
al O
. O
, O

this O
is O
promising O
as O
the O
existing O
stateof O
- O
the O
- O
art O
requires O
complex O
architectures O
( O
peters O
et O
al O
. O
, O

on O
imdb B-DatasetName
, O
we O
reduce O
the O
error B-MetricName
dramatically O
by O
43.9 B-MetricValue
% I-MetricValue
and O
22 B-MetricValue
% I-MetricValue
with O
regard O
to O
cove B-MethodName
and O
the O
state O
- O
of O
- O
the O
- O
art O
respectively O
. O

our O
method O
outperforms O
both O
cove B-MethodName
, O
a O
state O
- O
of O
- O
the O
- O
art O
transfer O
learning O
method O
based O
on O
hypercolumns O
, O
as O
well O
as O
the O
state O
- O
of O
- O
the O
- O
art O
on O
both O
datasets O
. O

we O
show O
the O
test O
error B-MetricName
rates I-MetricName
on O
the O
imdb B-DatasetName
and O
trec-6 B-DatasetName
datasets O
used O
by O
mccann O
et O
al O
. O
( O

4.2 O
results O
for O
consistency O
, O
we O
report O
all O
results O
as O
error B-MetricName
rates I-MetricName
( O
lower O
is O
better O
) O
. O

for O
the O
ag B-DatasetName
, O
yelp B-DatasetName
, O
and O
dbpedia B-DatasetName
datasets O
, O
we O
compare O
against O
the O
state O
- O
of O
- O
the O
- O
art O
text B-TaskName
categorization I-TaskName
method O
by O
johnson O
and O
zhang O
( O
2017 O
) O
. O

2017 O
) O
, O
a O
stateof O
- O
the O
- O
art O
transfer O
learning O
method O
for O
nlp O
. O

for O
the O
imdb B-DatasetName
and O
trec-6 B-DatasetName
datasets O
, O
we O
compare O
against O
cove B-MethodName
( O
mccann O
et O
al O
. O
, O

baselines O
and O
comparison O
models O
for O
each O
task O
, O
we O
compare O
against O
the O
current O
state O
- O
of O
- O
theart O
. O

we O
found O
50epochs B-HyperparameterValue
to O
be O
a O
good O
default O
for B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
the O
classiﬁer.used O
in O
( O
merity O
et O
al O
. O
, O

we O
otherwise O
use O
the O
same O
practices O
7on O
small O
datasets O
such O
as O
trec-6 B-DatasetName
, O
we O
ﬁne B-MethodName
- I-MethodName
tune I-MethodName
the O
lm O
only O
for O
15epochs B-HyperparameterValue
without O
overﬁtting O
, O
while O
we O
can O
ﬁne B-MethodName
- I-MethodName
tune I-MethodName
longer O
on O
larger O
datasets O
. O

we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
, O
a O
base O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0:004 B-HyperparameterValue
and0:01for B-HyperparameterValue
ﬁnetuning O
the O
lm O
and O
the O
classiﬁer O
respectively O
, O
and O
tune O
the O
number O
of O
epochs O
on O
the O
validation O
set O
of O
each O
task7 O
. O

we O
use O
adam B-HyperparameterName
with O
1= B-HyperparameterValue
0:7instead I-HyperparameterValue
of O
the O
default O
1= B-HyperparameterValue
0:9and I-HyperparameterValue
2= B-HyperparameterValue
0:99 I-HyperparameterValue
, O
similar O
to O
( O
dozat O
and O
manning O
, O
2017 O
) O
. O

we O
apply O
dropout B-HyperparameterName
of O
0:4to B-HyperparameterValue
layers O
, B-HyperparameterValue
0:3to I-HyperparameterValue
rnn O
layers O
, O
0:4to B-HyperparameterValue
input O
embedding O
layers O
, O
0:05to B-HyperparameterValue
embedding O
layers O
, O
and O
weight B-HyperparameterName
dropout I-HyperparameterName
of O
0:5to B-HyperparameterValue
the O
rnn O
hidden O
- O
to O
- O
hidden O
matrix O
. O

2017a O
) O
with O
an O
embedding B-HyperparameterName
size I-HyperparameterName
of O
400,3layers B-HyperparameterValue
, O
1150 B-HyperparameterValue
hidden B-HyperparameterName
activations I-HyperparameterName
per I-HyperparameterName
layer I-HyperparameterName
, O
and O
a O
bptt B-HyperparameterName
batch I-HyperparameterName
size I-HyperparameterName
of O
70 B-HyperparameterValue
. O

we O
use O
the O
awd B-MethodName
- I-MethodName
lstm I-MethodName
language O
model O
( O
merity O
et O
al O
. O
, O

to O
this O
end O
, O
if O
not O
mentioned O
otherwise O
, O
we O
use O
the O
same O
set O
of O
hyperparameters O
across O
tasks O
, O
which O
we O
tune O
on O
the O
imdb O
validation O
set O
. O

hyperparameters O
we O
are O
interested O
in O
a O
model O
that O
performs O
robustly O
across O
a O
diverse O
set O
of O
tasks O
. O

in O
addition O
, O
to O
allow O
the O
language O
model O
to O
capture O
aspects O
that O
might O
be O
relevant O
for O
classiﬁcation O
, O
we O
add O
special O
tokens O
for O
upper O
- O
case O
words O
, O
elongation O
, O
and O
repetition O
. O

pre O
- O
processing O
we O
use O
the O
same O
pre O
- O
processing O
as O
in O
earlier O
work O
( O
johnson O
and O
zhang O
, O
2017 O
; O
mccann O
et O
al O
. O
, O

topic B-TaskName
classiﬁcation I-TaskName
for O
topic B-TaskName
classiﬁcation I-TaskName
, O
we O
evaluate O
on O
the O
large O
- O
scale O
ag B-DatasetName
news O
and O
dbpedia B-DatasetName
ontology O
datasets O
created O
by O
zhang O
et O
al O
. O
( O

2016 O
) O
5.9 B-MethodName
lstm I-MethodName
- I-MethodName
cnn I-MethodName
( O
zhou O
et O
al O
. O
, O

2017 O
) O
4.2 O
oh B-MethodName
- I-MethodName
lstm B-MethodName
( O
johnson O
and O
zhang O
, O
2016 O
) O
5.9 O
tbcnn B-MethodName
( O
mou O
et O
al O
. O
, O

question B-TaskName
classiﬁcation I-TaskName
we O
use O
the O
six O
- O
class O
version O
of O
the O
small O
trec B-DatasetName
dataset O
( O
v O
oorhees O
and O
tice O
, O
1999 O
) O
dataset O
of O
open O
- O
domain O
, O
fact O
- O
based O
questions O
divided O
into O
broad O
semantic O
categories.333model O
test O
model O
testimdbcove O
( O
mccann O
et O
al O
. O
, O

2011 O
) O
and O
on O
the O
binary O
and O
ﬁve O
- O
class O
version O
of O
the O
yelp B-DatasetName
review O
dataset O
compiled O
by O
zhang O
et O
al O
. O
( O

sentiment B-TaskName
analysis I-TaskName
for O
sentiment B-TaskName
analysis I-TaskName
, O
we O
evaluate O
our O
approach O
on O
the O
binary O
movie O
review O
imdb B-DatasetName
dataset O
( O
maas O
et O
al O
. O
, O

we O
show O
the O
statistics O
for O
each O
dataset O
and O
task O
in O
table O
1 O
. O

2017 O
) O
as O
instances O
of O
three O
common O
text B-TaskName
classiﬁcation I-TaskName
tasks O
: O
sentiment B-TaskName
analysis I-TaskName
, O
question B-TaskName
classiﬁcation I-TaskName
, O
and O
topic B-TaskName
classiﬁcation I-TaskName
. O

4.1 O
experimental O
setup O
datasets O
and O
tasks O
we O
evaluate O
our O
method O
on O
six O
widely O
- O
studied O
datasets O
, O
with O
varying O
numbers O
of O
documents O
and O
varying O
document O
length O
, O
used O
by O
state O
- O
of O
- O
the O
- O
art O
text B-TaskName
classiﬁcation I-TaskName
and B-MethodName
transfer I-MethodName
learning I-MethodName
approaches O
( O
johnson O
and O
zhang O
, O
2017 O
; O
mccann O
et O
al O
. O
, O

4 O
experiments O
while O
our O
approach O
is O
equally O
applicable O
to O
sequence O
labeling O
tasks O
, O
we O
focus O
on O
text B-TaskName
classiﬁcation I-TaskName
tasks O
in O
this O
work O
due O
to O
their O
important O
realworld O
applications O
. O

we B-MethodName
ﬁne I-MethodName
- I-MethodName
tune I-MethodName
a O
classiﬁer O
for O
each O
lm O
independently O
using O
bpt3c O
and O
average O
the O
classiﬁer O
predictions O
. O

for O
all O
our O
experiments O
, O
we O
pretrain B-MethodName
both O
a O
forward O
and O
a O
backward O
lm O
. O

2017 O
, O
2018 O
) O
, O
we O
are O
not O
limited O
to O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
a O
unidirectional O
language O
model O
. O

bidirectional O
language O
model O
similar O
to O
existing O
work O
( O
peters O
et O
al O
. O
, O

in O
practice O
, O
we O
use O
variable O
length O
backpropagation O
sequences O
( O
merity O
et O
al O
. O
, O

bptt O
for O
text O
classiﬁcation O
( O
bpt3c O
) O
language O
models O
are O
trained O
with O
backpropagation O
through O
time O
( O
bptt O
) O
to O
enable O
gradient O
propagation O
for O
large O
input O
sequences O
. O

while B-MethodName
discriminative I-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
, O
slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
, O
and O
gradual B-MethodName
unfreezing I-MethodName
all O
are O
beneﬁcial O
on O
their O
own O
, O
we O
show O
in O
section O
5 O
that O
they O
complement O
each O
other O
and O
enable O
our O
method O
to O
perform O
well O
across O
diverse O
datasets O
. O

2017 O
) O
, O
except O
that O
we O
add O
a O
layer O
at O
a O
time O
to O
the O
set O
of O
‘ O
thawed O
’ O
layers O
, O
rather O
than O
only O
training O
a O
single O
layer O
at O
a O
time O
. O

this O
is O
similar O
to O
‘ O
chain O
- O
thaw O
’ O
( O
felbo O
et O
al O
. O
, O

we O
then O
unfreeze O
the O
next O
lower O
frozen O
layer O
and O
repeat O
, O
until O
we O
ﬁnetune B-MethodName
all O
layers O
until O
convergence O
at O
the O
last O
iteration O
. O

2014 O
): O
we O
ﬁrst O
unfreeze O
the O
last O
layer O
and O
ﬁne B-MethodName
- I-MethodName
tune I-MethodName
all O
unfrozen O
layers O
for O
one O
epoch O
. O

gradual B-MethodName
unfreezing I-MethodName
rather O
than O
ﬁne O
- O
tuning O
all O
layers O
at O
once O
, O
which O
risks O
catastrophic O
forgetting O
, O
we O
propose O
to O
gradually O
unfreeze O
the O
model O
starting O
from O
the O
last O
layer O
as O
this O
contains O
the O
least O
general O
knowledge O
( O
yosinski O
et O
al O
. O
, O

besides O
discriminative B-MethodName
ﬁnetuning I-MethodName
and O
triangular B-MethodName
learning I-MethodName
rates I-MethodName
, O
we O
propose O
gradual B-MethodName
unfreezing I-MethodName
for I-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
the O
classiﬁer O
. O

overly O
aggressive O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
will O
cause O
catastrophic O
forgetting O
, O
eliminating O
the O
beneﬁt O
of O
the O
information O
captured O
through O
language O
modeling O
; O
too O
cautious B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
will O
lead O
to O
slow O
convergence O
( O
and O
resultant O
overﬁtting O
) O
. O

fine B-MethodName
- I-MethodName
tuning I-MethodName
the O
target O
classiﬁer O
is O
the O
most O
critical O
part O
of O
the O
transfer B-MethodName
learning I-MethodName
method O
. O

as O
input O
documents O
can O
consist O
of O
hundreds O
of O
words O
, O
information O
may O
get O
lost O
if O
we O
only O
consider O
the O
last O
hidden O
state O
of O
the O
model O
. O

6while O
loshchilov O
and O
hutter O
( O
2017 O
) O
use O
multiple O
annealing O
cycles O
, O
we O
generally O
found O
one O
cycle O
to O
work O
best.332occur O
anywhere O
in O
the O
document O
. O

concat O
pooling O
the O
signal O
in O
text O
classiﬁcation O
tasks O
is O
often O
contained O
in O
a O
few O
words O
, O
which O
may O
5we O
also O
credit O
personal O
communication O
with O
the O
author O
. O

the O
ﬁrst O
linear O
layer O
takes O
as O
the O
input O
the O
pooled O
last O
hidden O
layer O
states O
. O

note O
that O
the O
parameters O
in O
these O
task O
- O
speciﬁc O
classiﬁer O
layers O
are O
the O
only O
ones O
that O
are O
learned O
from O
scratch O
. O

following O
standard O
practice O
for O
cv O
classiﬁers O
, O
each O
block O
uses O
batch O
normalization O
( O
ioffe O
and O
szegedy O
, O
2015 O
) O
and O
dropout O
, O
with O
relu O
activations O
for O
the O
intermediate O
layer O
and O
a O
softmax O
activation O
that O
outputs O
a O
probability O
distribution O
over O
target O
classes O
at O
the O
last O
layer O
. O

3.3 O
target O
task O
classiﬁer O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
finally O
, O
for O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
the O
classiﬁer O
, O
we O
augment O
the O
pretrained O
language O
model O
with O
two O
additional O
linear O
blocks O
. O

stlr B-MethodName
modiﬁes O
triangular O
learning O
rates O
( O
smith O
, O
2017 O
) O
with O
a O
short O
increase O
and O
a O
long O
decay O
period O
, O
which O
we O
found O
key O
for O
good O
performance.5 O
in O
section O
5 O
, O
we O
compare O
against O
aggressive O
cosine O
annealing O
, O
a O
similar O
schedule O
that O
has O
recently O
been O
used O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
cv O
( O
loshchilov O
and O
hutter O
, O
2017).6 O
figure O
2 O
: O
the O
slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rate I-MethodName
schedule O
used O
for B-MethodName
ulmfit I-MethodName
as O
a O
function O
of O
the O
number O
of O
training O
iterations O
. O

4in O
other O
words O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
times O
the O
number O
of O
updates O
per O
epoch.the O
lr O
, O
cutis O
the O
iteration O
when O
we O
switch O
from O
increasing O
to O
decreasing O
the O
lr O
, O
pis O
the O
fraction O
of O
the O
number O
of O
iterations O
we O
have O
increased O
or O
will O
decrease O
the O
lr O
respectively O
, O
ratio O
speciﬁes O
how O
much O
smaller O
the O
lowest O
lr O
is O
from O
the O
maximum O
lrmax O
, O
andtis O
the O
learning O
rate O
at O
iteration O
t. O
we O
generally O
use O
cutfrac B-HyperparameterName
= O
0:1,ratio B-HyperparameterValue
= O
32 B-HyperparameterValue
and O
max= B-HyperparameterName
0:01 B-HyperparameterValue
. O

instead O
, O
we O
propose O
slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
( O
stlr B-MethodName
) O
, O
which O
ﬁrst O
linearly O
increases O
the O
learning O
rate O
and O
then O
linearly O
decays O
it O
according O
to O
the O
following O
update O
schedule O
, O
which O
can O
be O
seen O
in O
figure O
2 O
: O
cut O
= O
btcutfracc B-HyperparameterName
p= O
( O
t O
= O
cut O
; O
ift O
< O
cut O
1 t cut O
cut(ratio O
 1);otherwise O
t=max1 O
+ O
p(ratio 1 B-HyperparameterName
) B-HyperparameterName
ratio(3 I-HyperparameterName
) O
wheretis O
the O
number O
of O
training O
iterations4 B-HyperparameterName
, I-HyperparameterName
cutfrac I-HyperparameterName
is O
the O
fraction O
of O
iterations O
we O
increase O
3an O
unrelated O
method O
of O
the O
same O
name O
exists O
for O
deep O
boltzmann O
machines O
( O
salakhutdinov O
and O
hinton O
, O
2009 O
) O
. O

using O
the O
same O
learning O
rate O
( O
lr O
) O
or O
an O
annealed O
learning O
rate O
throughout O
training O
is O
not O
the O
best O
way O
to O
achieve O
this O
behaviour O
. O

slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
for O
adapting O
its O
parameters O
to O
task O
- O
speciﬁc O
features O
, O
we O
would O
like O
the O
model O
to O
quickly O
converge O
to O
a O
suitable O
region O
of O
the O
parameter O
space O
in O
the O
beginning O
of O
training O
and O
then O
reﬁne O
its O
parameters O
. O

the O
sgd O
update O
with O
discriminative B-MethodName
ﬁnetuning I-MethodName
is O
then O
the O
following O
: O
l O
t=l O
t 1 lrlj( O
) O
( O
2 O
) O
we O
empirically O
found O
it O
to O
work O
well O
to O
ﬁrst O
choose O
the O
learning O
rate O
lof O
the O
last O
layer O
by O
ﬁne O
- O
tuning O
only O
the O
last O
layer O
and O
using O
l 1= O
l=2:6as O
the O
learning O
rate O
for O
lower O
layers O
. O

similarly O
, O
we O
obtainf1;:::;lgwherelis O
the O
learning O
rate O
of O
thel O
- O
th O
layer O
. O

for O
discriminative B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
, O
we O
split O
the O
parametersintof1;:::;lgwherelcontains O
the O
parameters O
of O
the O
model O
at O
the O
l O
- O
th O
layer O
and O
lis O
the O
number O
of O
layers O
of O
the O
model O
. O

for O
context O
, O
the O
regular O
stochastic O
gradient O
descent O
( O
sgd O
) O
update O
of O
a O
model O
’s O
parameters O
at O
time O
steptlooks O
like O
the O
following O
( O
ruder O
, O
2016 O
): O
t=t 1 rj( O
) O
( O
1 O
) O
whereis O
the O
learning O
rate O
and O
rj()is O
the O
gradient O
with O
regard O
to O
the O
model O
’s O
objective O
function O
. O

instead O
of O
using O
the O
same O
learning O
rate O
for O
all O
layers O
of O
the O
model O
, O
discriminative B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
allows O
us O
to O
tune O
each O
layer O
with O
different O
learning O
rates O
. O

2014 O
) O
, O
they O
should O
be O
ﬁne B-MethodName
- I-MethodName
tuned I-MethodName
to O
different O
extents O
. O

discriminative B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
as O
different O
layers O
capture O
different O
types O
of O
information O
( O
yosinski O
et O
al O
. O
, O

we O
propose O
discriminative B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
andslanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
for B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
the O
lm O
, O
which O
we O
introduce O
in O
the O
following O
. O

given O
a O
pretrained O
general O
- O
domain O
lm O
, O
this O
stage O
converges O
faster O
as O
it O
only O
needs O
to O
adapt O
to O
the O
idiosyncrasies O
of O
the O
target O
data O
, O
and O
it O
allows O
us O
to O
train O
a O
robust O
lm O
even O
for O
small O
datasets O
. O

we O
thus O
ﬁne B-MethodName
- I-MethodName
tune I-MethodName
the O
lm O
on O
data O
of O
the O
target O
task O
. O

3.2 O
target O
task O
lm O
ﬁne B-MethodName
- I-MethodName
tuning I-MethodName
no O
matter O
how O
diverse O
the O
general O
- O
domain O
data O
used O
for O
pretraining O
is O
, O
the O
data O
of O
the O
target O
task O
will O
likely O
come O
from O
a O
different O
distribution O
. O

while O
this O
stage O
is O
the O
most O
expensive O
, O
it O
only O
needs O
to O
be O
performed O
once O
and O
improves O
performance O
and O
convergence O
of O
downstream O
models O
. O

we O
leave O
the O
exploration O
of O
more O
diverse O
pretraining O
corpora O
to O
future O
work O
, O
but O
expect O
that O
they O
would O
boost O
performance O
. O

pretraining B-MethodName
is O
most O
beneﬁcial O
for O
tasks O
with O
small O
datasets O
and O
enables O
generalization O
even O
with O
100 O
labeled O
examples O
. O

2017b O
) O
consisting O
of O
28,595 O
preprocessed O
wikipedia O
articles O
and O
103 O
million O
words O
. O

we O
pretrain O
the O
language O
model O
on O
wikitext-103 B-DatasetName
( O
merity O
et O
al O
. O
, O

we O
discuss O
these O
in O
the O
following O
sections.3.1 O
general O
- O
domain O
lm O
pretraining B-MethodName
an O
imagenet O
- O
like O
corpus O
for O
language O
should O
be O
large O
and O
capture O
general O
properties O
of O
language O
. O

ulmfit B-MethodName
consists O
of O
the O
following O
steps O
, O
which O
we O
show O
in O
figure O
1 O
: O
a O
) O
general O
- O
domain O
lm O
pretraining B-MethodName
( O
x3.1 O
) O
; O
b O
) O
target O
task O
lm B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
( O
x3.2 O
) O
; O
and O
c O
) O
target O
task O
classiﬁer B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
( O
x3.3 O
) O
. O

analogous O
to O
cv O
, O
we O
expect O
that O
downstream O
performance O
can O
be O
improved O
by O
using O
higherperformance O
language O
models O
in O
the O
future O
. O

2017a O
) O
, O
a O
regular O
lstm B-MethodName
( O
with O
no O
attention O
, O
short O
- O
cut O
connections O
, O
or O
other O
sophisticated O
additions O
) O
with O
various O
tuned O
dropout O
hyperparameters O
. O

in O
our O
experiments O
, O
we O
use O
the O
state O
- O
of O
- O
theart O
language O
model O
awd B-MethodName
- I-MethodName
lstm I-MethodName
( O
merity O
et O
al O
. O
, O

the O
method O
is O
universal O
in O
the O
sense O
that O
it O
meets O
these O
practical O
criteria O
: O
1 O
) O
it O
works O
across O
tasks O
varying O
in O
document O
size O
, O
number O
, O
and O
label O
type O
; O
2 O
) O
it O
uses O
a O
single O
architecture O
and O
training O
process O
; O
3 O
) O
it O
requires O
no O
custom O
feature O
engineering O
or O
preprocessing O
; O
and O
4 O
) O
it O
does O
not O
require O
additional O
in O
- O
domain O
documents O
or O
labels O
. O

we O
propose O
universal B-MethodName
language I-MethodName
model I-MethodName
finetuning I-MethodName
( O
ulmfit B-MethodName
) O
, O
which O
pretrains O
a O
language O
model O
( O
lm O
) O
on O
a O
large O
general O
- O
domain O
corpus O
and O
ﬁne B-MethodName
- I-MethodName
tunes I-MethodName
it O
on O
the O
target O
task O
using O
novel O
techniques O
. O

formally O
, O
language O
modeling O
induces O
a O
hypothesis O
spacehthat O
should O
be O
useful O
for O
many O
other O
nlp O
tasks O
( O
vapnik O
and O
kotz O
, O
1982 O
; O
baxter O
, O
2000 O
) O
. O

moreover O
, O
language O
modeling O
already O
is O
a O
key O
component O
of O
existing O
tasks O
such O
as O
mt O
and O
dialogue O
modeling O
. O

task O
, O
which O
we O
show O
signiﬁcantly O
improves O
performance O
( O
see O
section O
5 O
) O
. O

c O
) O
the O
classiﬁer O
is O
ﬁne B-MethodName
- I-MethodName
tuned I-MethodName
on O
the O
target O
task O
using O
gradual B-MethodName
unfreezing I-MethodName
, O
‘ O
discr B-MethodName
’ O
, O
and O
stlr B-MethodName
to O
preserve O
low O
- O
level O
representations O
and O
adapt O
high O
- O
level O
ones O
( O
shaded O
: O
unfreezing O
stages O
; O
black O
: O
frozen O
) O
. O

b O
) O
the O
full O
lm O
is O
ﬁne B-MethodName
- I-MethodName
tuned I-MethodName
on O
target O
task O
data O
using O
discriminative B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
( O
‘ O
discr B-MethodName
’ O
) O
and O
slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
( O
stlr B-MethodName
) O
to O
learn O
task O
- O
speciﬁc O
features O
. O

2017 O
) O
, O
it O
provides O
data O
in O
near O
- O
unlimited O
quantities O
for O
most O
domains O
and O
languages O
. O

2018 O
) O
, O
and O
sentiment B-TaskName
( O
radford O
et O
al O
. O
, O

in O
contrast O
to O
tasks O
like O
mt O
( O
mccann O
et O
al O
. O
, O

2016 O
) O
, O
hierarchical O
relations O
( O
gulordava O
et O
al O
. O
, O

language O
modeling O
can O
be O
seen O
as O
the O
ideal O
source O
task O
and O
a O
counterpart O
of O
imagenet O
for O
nlp O
: O
it O
captures O
many O
facets O
of O
language O
relevant O
for O
downstream O
tasks O
, O
such O
as O
long O
- O
term O
dependencies O
( O
linzen O
et O
al O
. O
, O

3 O
universal O
language O
model O
fine B-MethodName
- I-MethodName
tuning I-MethodName
we O
are O
interested O
in O
the O
most O
general O
inductive O
transfer O
learning O
setting O
for O
nlp O
( O
pan O
and O
yang O
, O
2010 O
): O
given O
a O
static O
source O
task O
tsandanytarget O
taskttwithts6 O
= O
tt O
, O
we O
would O
like O
to O
improve O
performance O
on O
tt O
. O

in O
contrast O
, O
ulmfit B-MethodName
leverages O
general O
- O
domain O
pretraining O
and O
novel O
ﬁnetuning O
techniques O
to O
prevent O
overﬁtting O
even O
with O
only100labeled O
examples O
and O
achieves O
state O
- O
ofthe O
- O
art O
results O
also O
on O
small O
datasets O
. O

dai O
and O
le O
( O
2015 O
) O
also O
ﬁne B-MethodName
- I-MethodName
tune I-MethodName
a O
language O
model O
, O
but O
overﬁt O
with O
10k O
labeled O
examples O
and O
require O
millions O
of O
in O
- O
domain O
documents O
for O
good O
performance O
. O

2015 O
) O
but O
has O
been O
shown O
to O
fail O
between O
unrelated O
ones O
( O
mou O
et O
al O
. O
, O

2017 O
) O
, O
for O
distantly O
supervised O
sentiment B-TaskName
analysis I-TaskName
( O
severyn O
and O
moschitti O
, O
2015 O
) O
, O
or O
mt O
domains O
( O
sennrich O
et O
al O
. O
, O

fine B-MethodName
- I-MethodName
tuning I-MethodName
fine B-MethodName
- I-MethodName
tuning I-MethodName
has O
been O
used O
successfully O
to O
transfer O
between O
similar O
tasks O
, O
e.g. O
in O
qa B-TaskName
( O
min O
et O
al O
. O
, O

mtl O
requires O
the O
tasks O
to O
be O
trained O
from O
scratch O
every O
time O
, O
which O
makes O
it O
inefﬁcient O
and O
often O
requires O
careful O
weighting O
of O
the O
taskspeciﬁc O
objective O
functions O
( O
chen O
et O
al O
. O
, O

2018 O
) O
who O
add O
a O
language O
modeling O
objective O
to O
the O
model O
that O
is O
trained O
jointly O
with O
the O
main O
task O
model O
. O

this O
is O
the O
approach O
taken O
by O
rei O
( O
2017 O
) O
and O
liu O
et O
al O
. O
( O

multi O
- O
task O
learning O
a O
related O
direction O
is O
multi O
- O
task O
learning O
( O
mtl O
) O
( O
caruana O
, O
1993 O
) O
. O

in O
cv O
, O
hypercolumns O
have O
been O
nearly O
entirely O
superseded O
by O
end O
- O
to O
- O
end O
ﬁne O
- O
tuning O
( O
long O
et O
al O
. O
, O

2018 O
) O
require O
engineered O
custom O
architectures O
, O
while O
we O
show O
state O
- O
of O
- O
the O
- O
art O
performance O
with O
the O
same O
basic O
architecture O
across O
a O
range O
of O
tasks O
. O

2017 O
) O
who O
use O
language O
modeling O
, O
paraphrasing O
, O
entailment O
, O
and O
machine O
translation O
( O
mt O
) O
respectively O
for O
pretraining O
. O

in O
analogy O
, O
a O
hypercolumn O
for O
a O
word O
or O
sentence O
in O
nlp O
is O
the O
concatenation O
of O
embeddings O
at O
different O
layers O
in O
a O
pretrained O
model.et O
al O
. O
( O

2018 O
) O
, O
wieting O
and O
gimpel O
( O
2017 O
) O
, O
conneau O
2a O
hypercolumn O
at O
a O
pixel O
in O
cv O
is O
the O
vector O
of O
activations O
of O
all O
cnn B-MethodName
units O
above O
that O
pixel O
. O

2 O
) O
we O
propose O
discriminative B-MethodName
ﬁne I-MethodName
- I-MethodName
tuning I-MethodName
, O
slanted B-MethodName
triangular I-MethodName
learning I-MethodName
rates I-MethodName
, O
and O
gradual B-MethodName
unfreezing I-MethodName
, O
novel O
techniques O
to O
retain O
previous O
knowledge O
and O
avoid O
catastrophic O
forgetting O
during O
ﬁne O
- O
tuning O
. O

this O
method O
is O
known O
as O
hypercolumns O
( O
hariharan O
et O
al O
. O
, O

embeddings O
at O
different O
levels O
are O
then O
used O
as O
features O
, O
concatenated O
either O
with O
the O
word O
embeddings O
or O
with O
the O
inputs O
at O
intermediate O
layers O
. O

the O
prevailing O
approach O
is O
to O
pretrain O
embeddings O
that O
capture O
additional O
context O
via O
other O
tasks O
. O

hypercolumns O
in O
nlp O
, O
only O
recently O
have O
methods O
been O
proposed O
that O
go O
beyond O
transferring O
word O
embeddings O
. O

2014 O
) O
or O
several O
of O
the O
last O
layers O
of O
a O
pretrained O
model O
and O
leaving O
the O
remaining O
layers O
frozen O
( O
long O
et O
al O
. O
, O

in O
recent O
years O
, O
this O
approach O
has O
been O
superseded O
by O
ﬁne O
- O
tuning O
either O
the O
last O
( O
donahue O
et O
al O
. O
, O

2014 O
) O
achieve O
state O
- O
of O
- O
theart O
results O
using O
features O
of O
an O
imagenet O
model O
as O
input O
to O
a O
simple O
classiﬁer O
. O

for O
this O
reason O
, O
most O
work O
in O
cv O
focuses O
on O
transferring O
the O
last O
layers O
of O
the O
model O
( O
long O
et O
al O
. O
, O

2 O
related O
work O
transfer O
learning O
in O
cv O
features O
in O
deep O
neural O
networks O
in O
cv O
have O
been O
observed O
to O
transition O
from O
task- O
speciﬁc O
togeneral O
from O
the O
ﬁrst O
to O
the O
last O
layer O
( O
yosinski O
et O
al O
. O
, O

5 O
) O
we O
make O
the O
pretrained O
models O
and O
our O
code O
available O
to O
enable O
wider O
adoption O
. O

4 O
) O
we O
show O
that O
our O
method O
enables O
extremely O
sample O
- O
efﬁcient O
transfer O
learning O
and O
perform O
an O
extensive O
ablation O
analysis O
. O

3 O
) O
we O
signiﬁcantly O
outperform O
the O
state O
- O
of O
- O
the O
- O
art O
on O
six O
representative O
text O
classiﬁcation O
datasets O
, O
with O
an B-MetricName
error I-MetricName
reduction O
of O
18 B-MetricValue
- I-MetricValue
24 I-MetricValue
% I-MetricValue
on O
the O
majority O
of O
datasets O
. O

contributions O
our O
contributions O
are O
the O
following O
: O
1 O
) O
we O
propose O
universal B-MethodName
language I-MethodName
model I-MethodName
fine I-MethodName
- I-MethodName
tuning I-MethodName
( O
ulmfit B-MethodName
) O
, O
a O
method O
that O
can O
be O
used O
to O
achieve O
cv O
- O
like O
transfer O
learning O
for O
any O
task O
for O
nlp O
. O

on O
imdb B-DatasetName
, O
with O
100labeled O
examples O
, O
ulmfit B-MethodName
matches O
the O
performance O
of O
training O
from O
scratch O
with O
10and O
— O
given O
50k O
unlabeled O
examples O
— O
with O
100more O
data O
. O

we O
propose O
a O
new O
method O
, O
universal B-MethodName
language I-MethodName
model I-MethodName
fine I-MethodName
- I-MethodName
tuning I-MethodName
( O
ulmfit B-MethodName
) O
that O
addresses O
these O
issues O
and O
enables O
robust O
inductive O
transfer O
learning O
for O
any O
nlp O
task O
, O
akin O
to O
ﬁne O
- O
tuning O
imagenet O
models O
: O
the O
same O
3 O
- O
layer O
lstm B-MethodName
architecture O
— O
with O
the O
same O
hyperparameters O
and O
no O
additions O
other O
than O
tuned O
dropout O
hyperparameters O
— O
outperforms O
highly O
engineered O
models O
and O
trans-329fer O
learning O
approaches O
on O
six O
widely O
studied O
text O
classiﬁcation O
tasks O
. O

compared O
to O
cv O
, O
nlp O
models O
are O
typically O
more O
shallow O
and O
thus O
require O
different O
ﬁne O
- O
tuning O
methods O
. O

lms O
overﬁt O
to O
small O
datasets O
and O
suffered O
catastrophic O
forgetting O
when O
ﬁne O
- O
tuned O
with O
a O
classiﬁer O
. O

we O
show O
that O
not O
the O
idea O
of O
lm O
ﬁne O
- O
tuning O
but O
our O
lack O
of O
knowledge O
of O
how O
to O
train O
them O
effectively O
has O
been O
hindering O
wider O
adoption O
. O

dai O
and O
le O
( O
2015 O
) O
ﬁrst O
proposed O
ﬁnetuning O
a O
language O
model O
( O
lm O
) O
but O
require O
millions O
of O
in O
- O
domain O
documents O
to O
achieve O
good O
performance O
, O
which O
severely O
limits O
its O
applicability O
. O

however O
, O
inductive O
transfer O
via O
ﬁnetuning O
has O
been O
unsuccessful O
for O
nlp O
( O
mou O
et O
al O
. O
, O

2010 O
) O
, O
we O
should O
be O
able O
to O
do O
better O
than O
randomly O
initializing O
the O
remaining O
parameters O
of O
our O
models O
. O

in O
light O
of O
the O
beneﬁts O
of O
pretraining O
( O
erhan O
et O
al O
. O
, O

2018 O
) O
still O
train O
the O
main O
task O
model O
from O
scratch O
and O
treat O
pretrained O
embeddings O
as O
ﬁxed O
parameters O
, O
limiting O
their O
usefulness O
. O

recent O
approaches O
that O
concatenate O
embeddings O
derived O
from O
other O
tasks O
with O
the O
input O
at O
different O
layers O
( O
peters O
et O
al O
. O
, O

2013 O
) O
, O
a O
simple O
transfer O
technique O
that O
only O
targets O
a O
model O
’s O
ﬁrst O
layer O
, O
has O
had O
a O
large O
impact O
in O
practice O
and O
is O
used O
in O
most O
state O
- O
of O
- O
the O
- O
art O
models O
. O

for O
inductive O
transfer O
, O
ﬁne O
- O
tuning O
pretrained O
word O
embeddings O
( O
mikolov O
et O
al O
. O
, O

research O
in O
nlp O
focused O
mostly O
on O
transductive O
transfer O
( O
blitzer O
et O
al O
. O
, O

while O
deep O
learning O
models O
have O
achieved O
state O
- O
of O
- O
the O
- O
art O
on O
many O
nlp O
tasks O
, O
these O
models O
are O
trained O
from O
scratch O
, O
requiring O
large O
datasets O
, O
and O
days O
to O
converge O
. O

jeremy O
focused O
on O
the O
algorithm O
development O
and O
implementation O
, O
sebastian O
focused O
on O
the O
experiments O
and O
writing O
. O

2011 O
) O
, O
and O
commercial O
document O
classiﬁcation O
, O
such O
as O
for O
legal O
discovery O
( O
roitblat O
et O
al O
. O
, O

2012 O
) O
, O
emergency O
response O
( O
caragea O
et O
al O
. O
, O

text B-TaskName
classiﬁcation I-TaskName
is O
a O
category O
of O
natural O
language O
processing O
( O
nlp O
) O
tasks O
with O
real O
- O
world O
applications O
such O
as O
spam O
, O
fraud O
, O
and O
bot O
detection O
( O
jindal O
and O
liu O
, O
2007 O
; O
ngai O
et O
al O
. O
, O

applied O
cv O
models O
( O
including O
object O
detection O
, O
classiﬁcation O
, O
and O
segmentation O
) O
are O
rarely O
trained O
from O
scratch O
, O
but O
instead O
are O
ﬁne O
- O
tuned O
from O
models O
that O
have O
been O
pretrained O
on O
imagenet O
, O
ms O
- O
coco O
, O
and O
other O
datasets O
( O
sharif O
razavian O
et O
al O
. O
, O

1 O
introduction O
inductive O
transfer O
learning O
has O
had O
a O
large O
impact O
on O
computer O
vision O
( O
cv O
) O
. O

furthermore O
, O
with O
only O
100labeled O
examples O
, O
it O
matches O
the O
performance O
of O
training O
from O
scratch O
on O
100more O
data O
. O

our O
method O
signiﬁcantly O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
on O
six O
text O
classiﬁcation O
tasks O
, O
reducing O
the O
error B-MetricName
by O
1824 B-MetricValue
% I-MetricValue
on O
the O
majority O
of O
datasets O
. O

we O
propose O
universal B-MethodName
language I-MethodName
model I-MethodName
fine I-MethodName
- I-MethodName
tuning I-MethodName
( O
ulmfit B-MethodName
) O
, O
an O
effective O
transfer O
learning O
method O
that O
can O
be O
applied O
to O
any O
task O
in O
nlp O
, O
and O
introduce O
techniques O
that O
are O
key O
for O
ﬁne O
- O
tuning O
a O
language O
model O
. O

in O
contrast O
, O
a O
clear O
dichotomy O
between O
the O
recommender O
system O
perceived O
as O
useful O
or O
useless O
existed O
; O
the O
system O
had O
features O
to O
prompt O
new O
common O
words O
or O
old O
poorly O
- O
scored O
words O
. O

the O
feedback O
system O
with O
features O
for O
pronunciation O
scoring O
, O
speech O
replay O
, O
and O
giving O
a O
pronunciation O
example O
was O
deemed O
essential O
by O
most O
of O
the O
respondents O
. O

13 O
of O
them O
said O
they O
need O
to O
improve O
their O
pronunciation O
skills O
in O
english O
because O
of O
their O
foreign O
accent O
. O

16 O
international O
students O
who O
spoke O
non O
- O
native O
english O
and O
lived O
in O
australia O
participated O
. O

an O
artiﬁcial O
intelligence O
system O
can O
take O
a O
role O
in O
these O
guided O
learning O
approaches O
as O
an O
enabler O
of O
an O
application O
for O
pronunciation O
learning O
with O
a O
recommender O
system O
to O
guide O
language O
learners O
through O
exercises O
and O
feedback O
system O
to O
correct O
their O
pronunciation O
. O

this O
paper O
proposes O
a O
contextualized B-MethodName
character I-MethodName
representation I-MethodName
for I-MethodName
cged I-MethodName
and O
related O
solutions O
for O
the O
error O
sparse O
problem O
, O
which O
are O
improved O
compared O
to O
the O
baseline O
approach O
. O

8 O
conclusion O
as O
more O
and O
more O
people O
learn O
chinese O
, O
the O
automatic B-TaskName
diagnosis I-TaskName
of I-TaskName
chinese I-TaskName
grammatical I-TaskName
error I-TaskName
becomes O
more O
and O
more O
important O
. O

2017 O
) O
added O
more O
linguistic O
information O
on B-MethodName
lstm+crf I-MethodName
model O
, O
such O
as O
pos B-MethodName
, O
n O
- O
gram O
, O
pmi O
score O
and O
dependency O
features O
. O

2017 O
) O
used O
the O
lstm+crf B-MethodName
model O
to O
detect O
dependencies O
between O
outputs O
to O
better O
detect O
error O
messages O
. O
( O

2017 O
) O
combined O
machine O
learning O
with O
traditional O
n O
- O
gram O
methods O
, O
using O
bi B-MethodName
- I-MethodName
lstm I-MethodName
to O
detect O
the O
location O
of O
errors O
and O
adding O
additional O
linguistic O
information O
, O
pos B-MethodName
, O
ngram O
. O
( O

huang O
and O
wang O
, O
2016 O
) O
used O
bi B-MethodName
- I-MethodName
lstm I-MethodName
to O
annotate O
the O
errors O
in O
the O
sentence O
. O
( O

grammatical B-TaskName
error I-TaskName
detection I-TaskName
is O
usually O
considered O
as O
the O
sequence O
labeling O
task O
( O
zheng O
et O
al O
. O
, O

in O
the O
past O
few O
years O
, O
the O
diagnosis B-TaskName
of I-TaskName
chinese I-TaskName
grammatical I-TaskName
errors I-TaskName
has O
also O
been O
developing O
in O
machine O
learning O
. O

due O
to O
the O
continuous O
rise O
of O
machine O
learning O
in O
recent O
years O
, O
the O
ﬁeld O
of O
natural O
language O
processing O
is O
increasingly O
turning O
to O
machine O
learning O
. O

2017 O
) O
based O
on O
n O
- O
gram O
used O
the O
kmp O
algorithm O
to O
speed O
up O
the O
search O
for O
correct O
candidates O
. O

lin O
and O
chu O
, O
2015 O
) O
used O
n O
- O
gram O
to O
establish O
a O
scoring O
system O
to O
better O
give O
correction O
options O
. O
( O

2013 O
) O
still O
used O
n O
- O
gram O
as O
the O
main O
method O
, O
and O
added O
web O
resources O
to O
improve O
detection O
results O
. O
( O

7 O
related O
work B-TaskName
chinese I-TaskName
grammatical I-TaskName
error I-TaskName
diagnosis I-TaskName
task O
has O
been O
developed O
for O
a O
long O
time O
. O

since O
our O
model O
only O
proposes O
one O
candidate O
, O
the O
results O
on O
correction O
and O
top3 O
correction O
are O
the O
same O
. O

table O
14 O
show O
the O
results O
in O
cged2018 B-DatasetName
in O
correction O
part O
. O

table O
10 O
, O
11 O
, O
12 O
, O
13 O
shows O
the O
experiment O
results O
we O
submitted O
in O
cged2018 B-DatasetName
in O
detection O
part O
. O

for O
errors O
, O
pos B-MethodName
may O
provide O
some O
information O
to O
help O
the O
model O
detect O
better O
. O

it O
can O
be O
seen O
that O
pos B-MethodName
is O
useful O
in O
chinese B-TaskName
error I-TaskName
detection I-TaskName
. O

since O
we O
are O
dealing O
with O
characters O
, O
so O
we O
use O
pos B-MethodName
for O
the O
character O
’s O
corresponding O
word O
as O
the O
character O
’s O
pos B-MethodName
. O

we O
also O
tried O
to O
add O
artiﬁcial O
information O
to O
the O
model O
to O
improve O
the O
experimental O
results O
, O
so O
we O
added O
pos B-MethodName
( O
part B-MethodName
of I-MethodName
speech I-MethodName
) O
information O
. O

the O
model O
can O
also O
detect O
error O
information O
very O
well O
without O
artiﬁcial O
features O
. O

the O
results O
from O
f1 B-MetricName
show O
that O
the O
proposed O
model O
is O
improved O
compared O
to O
the O
baseline O
model O
. O

perimental O
results O
of O
our O
proposed O
model O
with O
new O
save O
function O
and O
reconstructive O
loss O
. O

at O
0.05 B-HyperparameterValue
, O
the O
f1 B-MetricName
values O
of O
all O
levels O
are O
too O
low O
, O
so O
we O
use O
0.1 B-HyperparameterValue
as O
the O
weight B-HyperparameterName
in O
the O
following O
experiments O
. O

when O
the O
weight B-HyperparameterName
is O
too O
large O
, O
false B-MetricName
positive I-MetricName
rate I-MetricName
is O
too O
large O
indicates O
that O
the O
error O
is O
not O
detected O
, O
which O
is O
not O
consistent O
with O
the O
objectives O
of O
this O
task O
. O

when O
is O
0.2 B-HyperparameterValue
or O
0.1 B-HyperparameterValue
is O
more O
suitable O
for O
our O
task O
. O

it O
can O
be O
seen O
that O
when O
the O
weight B-HyperparameterName
decreases O
, O
the O
false B-MetricName
positive I-MetricName
rate I-MetricName
decreases O
signiﬁcantly O
, O
which O
indicates O
that O
the O
model O
captures O
more O
correct O
information O
. O

so O
after O
that O
we O
modify O
the O
value O
of O
the O
weight B-HyperparameterName
in O
eq O
. O

the O
second O
part O
of O
table O
6 O
, O
7 O
, O
8 O
, O
9 O
shows O
the O
different O
models O
with O
new O
function O
to O
save O
model O
and O
reconstructive O
loss O
for O
modifying O
the O
value O
of O
in O
eq O
. O

there O
is O
an O
improvement O
in O
error O
detection O
, O
but O
too O
many O
errors O
are O
detected O
and O
the O
correct O
information O
is O
ignored O
. O

the O
results O
of O
mixing O
the O
above O
methods O
are O
also O
given O
. O

it O
can O
be O
seen O
from O
the O
experiment O
that O
modifying O
the O
save O
function O
and O
rebuilding O
the O
loss O
function O
all O
have O
a O
good O
improvement O
on O
the O
error O
detection O
of O
the O
model O
. O

the O
of O
the O
model O
with O
reconstructive O
loss O
is O
set O
to O
0.5 B-HyperparameterValue
. O

the O
ﬁrst O
part O
of O
table O
6 O
, O
7 O
, O
8 O
, O
9 O
shows O
the O
results O
of O
the O
comparison O
between O
the O
model O
using O
new O
function O
to O
save O
model O
, O
with O
the O
reconstruction O
loss O
function O
and O
the O
original O
model O
. O

since O
the O
experiment O
results O
are O
similar O
on O
cged2017 B-DatasetName
dataset O
, O
they O
are O
not O
given O
. O

6 O
result O
in O
this O
part O
, O
we O
show O
our O
experiment O
results O
in O
the O
cged2016 B-DatasetName
test O
set O
. O

the O
following O
metrics O
are O
measured O
at O
detection O
, O
identiﬁcation O
, O
position O
- O
level O
. O

the O
model O
can O
recommend O
at O
most O
3 O
correction O
at O
each O
error O
. O

correction O
level O
: O
characters O
marked O
as O
s O
and O
m O
need O
to O
give O
correct O
candidates O
. O

position O
level O
: O
the O
system O
results O
should O
be O
perfectly O
identical O
with O
the O
quadruples O
of O
the O
gold O
standard O
. O

identiﬁcation O
level O
: O
the O
correct O
situation O
should O
be O
exactly O
the O
same O
as O
the O
gold O
standard O
for O
a O
given O
type O
of O
error O
. O

in O
other O
words O
, O
the O
sentences O
are O
classiﬁed O
into O
two O
categories O
. O

if O
there O
is O
an O
error O
, O
the O
sentence O
is O
incorrect O
. O

detection O
level O
: O
determines O
whether O
a O
sentence O
is O
correct O
or O
not O
. O

2016 O
) O
, O
the O
evaluation O
method O
includes O
three O
levels O
, O
detection O
level O
, O
identiﬁcation O
level O
, O
position O
level O
. O

5.3 O
evaluation O
method O
according O
to O
( O
lee O
et O
al O
. O
, O

vide O
part O
of O
data O
from O
nlptea2016 B-DatasetName
to O
the O
development O
set O
. O

we O
use O
adam O
optimizer O
to O
train O
our O
model O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
0.001 B-HyperparameterValue
. O

the O
character O
embedding B-HyperparameterName
size I-HyperparameterName
is O
400 B-HyperparameterValue
, O
the O
hidden B-HyperparameterName
units I-HyperparameterName
of I-HyperparameterName
bilstm I-HyperparameterName
is O
256 B-HyperparameterValue
. O

we O
also O
use O
wiki O
corpus O
to O
build O
our O
n O
- O
gram O
dictionaries O
. O

5.2 O
hyper O
- O
parameter O
and O
data O
we O
use O
word2vec3to O
pretrain O
our O
character O
embeddings O
by O
wiki O
corpus4 O
. O

unlike O
traditional O
sequence O
labeling O
, O
chinese B-TaskName
grammatical I-TaskName
error I-TaskName
diagnosis I-TaskName
may O
result O
in O
inaccurate O
word O
segmentation O
due O
to O
existing O
errors O
, O
so O
we O
use O
character O
embeddings O
to O
replace O
word O
embeddings O
. O

5 O
evaluation O
5.1 O
baseline O
in O
this O
experiment O
, O
we O
build O
the O
bi B-MethodName
- I-MethodName
lstm I-MethodName
model O
for O
sequence O
labelling O
as O
our O
baseline O
model O
. O

we O
regard O
that O
the O
candidate O
with O
the O
highest O
score O
is O
the O
correct O
candidate O
. O

we O
ﬁrst O
search O
the O
words O
in O
the O
word O
dictionary O
which O
have O
the O
same O
character O
as O
the O
character O
labeled O
m. O
then O
, O
calculate O
the O
candidates O
’ O
score O
. O

for O
the O
error O
typed O
with O
m O
, O
we O
also O
use O
slto O
calculate O
the O
score O
using O
2 O
- O
gram O
and O
3 O
- O
gram O
. O

the O
character O
with O
the O
highest O
score O
is O
considered O
to O
be O
correct O
. O

the O
candidate O
dictionary O
is O
directly O
used O
to O
replace O
the O
character O
and O
the O
score O
is O
calculated O
. O

for O
the O
error O
which O
is O
typed O
with O
s O
is O
a O
character O
, O
we O
calculate O
the O
slscore O
for O
the O
character O
. O

we O
select O
the O
candidate O
word O
with O
the O
highest O
score O
as O
the O
correct O
one O
. O

we O
replace O
each O
characters O
in O
the O
words O
and O
calculate O
the O
score O
separately O
. O

when O
we O
choose O
the O
word O
to O
replace O
, O
we O
prefer O
to O
select O
the O
word O
that O
have O
only O
one O
character O
different O
from O
the O
original O
word O
. O

we O
merged O
the O
two O
dictionaries O
to O
one O
dictionary O
of O
candidates O
for O
characters O
. O

we O
use O
the O
dictionaries O
of O
characters O
with O
similar O
pronunciation O
and O
similar O
shape O
in O
( O
wu O
et O
al O
. O
, O

we O
use O
this O
score O
for O
errors O
typed O
s. O
in O
order O
to O
reduce O
the O
amount O
of O
calculation O
, O
we O
only O
keep O
the O
calculation O
of O
2 O
- O
gram O
and O
3 O
- O
gram O
, O
the O
example O
of O
n O
- O
gram O
of O
words O
is O
shown O
in O
table O
4.175for O
the O
error O
which O
is O
typed O
with O
s O
is O
a O
word O
, O
we O
will O
calculate O
the O
slscore O
of O
the O
word O
. O

2016 O
) O
adds O
weights O
to O
the O
different O
n O
- O
gram O
by O
their O
length O
to O
favor O
higher O
gram O
. O

to O
increase O
the O
accuracy B-MetricName
of O
correction O
, O
( O
chen O
et O
al O
. O
, O

the O
candidate O
word O
with O
highest O
scoring O
is O
regarded O
as O
the O
correction O
. O

if O
the O
candidate O
word O
has O
a O
higher O
score O
than O
the O
original O
word O
, O
the O
original O
word O
is O
considered O
to O
be O
wrong O
. O

if O
the O
original O
word O
has O
the O
highest O
score O
, O
the O
original O
word O
is O
considered O
to O
be O
correct O
. O

2016 O
) O
uses O
the O
method O
of O
calculating O
the O
n O
- O
gram O
score O
of O
each O
word O
to O
judge O
whether O
the O
word O
is O
correct O
or O
not O
and O
put O
forward O
correct O
candidates O
. O

2016 O
) O
and O
only O
put O
forward O
one O
candidate O
correction O
. O
( O

since O
we O
mainly O
deal O
with O
the O
detection O
problem O
, O
we O
have O
simpliﬁed O
the O
method O
in O
( O
chen O
et O
al O
. O
, O

4 O
correction O
system O
correct O
system O
we O
use O
in O
our O
model O
is O
the O
method O
proposed O
in O
( O
chen O
et O
al O
. O
, O

to O
address O
this O
issue O
, O
we O
add O
a O
loss O
function O
eq O
. O

therefore O
, O
the O
training O
of O
the O
model O
may O
have O
some O
problem O
. O

loss O
1= 1 O
nx O
x[ylna+ O
( O
1 y O
) O
ln(1 a)](7 O
) O
however O
, O
the O
problem O
that O
the O
number O
of O
correct O
characters O
in O
the O
dataset O
is O
much O
larger O
than O
the O
number O
of O
incorrect O
characters O
still O
exists O
. O

in O
the O
traditional O
lstm B-MethodName
model O
of O
sequence O
labelling O
, O
the O
cross O
- O
entropy O
loss O
function O
, O
eq O
. O

from O
the O
table O
6 O
, O
7 O
, O
8 O
, O
9 O
, O
it O
can O
also O
be O
seen O
that O
although O
the O
results O
have O
improved O
but O
the O
increase O
is O
limited O
. O

3.3 O
loss O
function O
although O
the O
model O
can O
detect O
more O
error O
information O
but O
it O
is O
not O
enough O
, O
when O
we O
use O
eq O
. O

the O
model O
can O
capture O
more O
error O
information O
when O
there O
are O
fewer O
errors O
in O
the O
sentence O
. O

4 O
is O
that O
when O
we O
save O
the O
model O
, O
we O
expect O
the O
model O
to O
detect O
more O
wrong O
information O
and O
ignore O
some O
correct O
information O
. O

therefore O
, O
we O
propose O
to O
save O
the O
model O
no O
longer O
when O
the O
development O
set O
achieves O
the O
max O
accuracy B-MetricName
, O
but O
when O
eq O
. O

the O
model O
discriminates O
most O
of O
test O
sentences O
to O
be O
correct O
. O

analyzing O
the O
result O
, O
we O
see O
that O
the O
model O
learns O
the O
correct O
part O
more O
, O
and O
learns O
the O
error O
information O
less O
. O

however O
, O
when O
the O
development O
set O
has O
reached O
the O
greatest O
accuracy O
, O
the O
output O
of O
the O
model O
in O
test O
set O
is O
not O
good O
. O

3.2 O
function O
of O
save O
model O
we O
use O
the O
traditional O
training O
method O
, O
accuracy O
, O
to O
train O
our O
model O
. O

after O
dividing O
the O
errors O
into O
four O
categories O
, O
it O
can O
be O
seen O
that O
due O
to O
the O
small O
number O
of O
errors O
, O
it O
may O
not O
be O
conducive O
to O
the O
training O
of O
the O
model O
. O

in O
table O
2 O
and O
3 O
, O
we O
give O
the O
number O
of O
errors O
in O
cged2018 B-DatasetName
. O

the O
task O
of O
cged20181is B-DatasetName
to O
automatically O
diagnose O
grammatical O
errors O
in O
chinese O
sentences O
written O
by O
second O
language O
learners O
. O

although O
one O
sentence O
may O
contain O
multiple O
errors O
but O
the O
number O
of O
errors O
is O
insufﬁcient O
. O

we O
use O
the O
mask B-MethodName
to O
get O
the O
contextualized O
character O
representation O
which O
can O
better O
represent O
the O
meaning O
of O
characters O
and O
better O
obtain O
the O
information O
we O
need O
in O
the O
text O
. O

mask B-MethodName
= O
(wmt+bm O
) O
( O
2 O
) O
xt O
xt O
mask B-MethodName
( O
3 O
) O
whereis O
the O
sigmoid O
activation O
function O
to O
control O
the O
output O
between O
0 O
to O
1 O
. O

then O
we O
use O
tto O
calculate O
the O
contextualized O
character O
vectors O
as O
input O
of O
traditional O
sequence O
labeling O
model O
of O
lstm B-MethodName
instead O
of O
the O
traditional O
character O
vectors O
. O

nn O
: O
rce!rteis O
a O
feedforward O
neural O
network O
parametrized O
by O
.ceis B-HyperparameterName
the O
character B-HyperparameterName
embedding I-HyperparameterName
size I-HyperparameterName
and B-HyperparameterName
teis I-HyperparameterName
the O
text B-HyperparameterName
representation I-HyperparameterName
size I-HyperparameterName
. O

t=1 O
mmx O
t=1nn(xt O
) O
( O
1 O
) O
where O
xtin O
our O
work O
represents O
the O
character O
representation O
in O
each O
time O
step O
. O

we O
take O
advantage O
of O
this O
method O
proposed O
in O
( O
choi O
et O
al O
. O
, O

that O
is O
to O
say O
, O
under O
the O
different O
context O
conditions O
, O
we O
need O
to O
mask O
out O
some O
dimensions O
of O
the O
word O
embedding O
vectors O
. O

but O
in O
different O
texts O
, O
the O
semantic O
information O
we O
need O
to O
use O
is O
different O
, O
so O
we O
need O
to O
ignore O
the O
unneeded O
semantic O
information O
. O

2016 O
) O
puts O
forward O
that O
each O
dimension O
of O
a O
word O
vector O
may O
represent O
some O
semantic O
information O
of O
the O
word O
. O

2.2 O
building O
contextualized O
character O
representation O
( O
choi O
et O
al O
. O
, O

to O
address O
this O
issue O
, O
we O
propose O
to O
use O
the O
contextualized B-MethodName
character I-MethodName
representation I-MethodName
for I-MethodName
cged I-MethodName
to O
solve O
the O
ambiguity O
problem O
. O

therefore O
, O
we O
use O
the O
same O
character O
vector O
to O
represent O
the O
same O
character O
in O
different O
contexts O
is O
inaccurate O
, O
and O
sometimes O
there O
may O
be O
a O
big O
semantic O
deviation O
. O

for O
example O
, O
the O
character O
” O
s O
” O
in O
word O
” O
 s O
” O
( O
a O
dozen O
) O
means O
dozen O
, O
in O
word O
” O
s O
” O
( O
play O
the173drum O
) O
means O
play O
. O

the O
experiment O
results O
show O
that O
our O
model O
have O
better O
result O
compared O
to O
the O
baseline O
without O
artiﬁcial O
features O
. O

this O
model O
have O
better O
considered O
the O
different O
semantics O
of O
words O
in O
chinese O
texts O
. O

for O
error O
typed O
s O
and O
m O
, O
the O
model O
can O
give O
at O
most O
three O
correct O
candidates O
. O

the O
cged B-TaskName
system O
needs O
to O
detect O
the O
location O
of O
errors O
and O
gives O
the O
type O
of O
each O
error O
. O

the O
errors O
include O
four O
types O
, O
redundant O
words O
( O
denoted O
as O
a O
capital O
” O
r O
” O
) O
, O
missing O
words O
( O
” O
m O
” O
) O
, O
word O
selection O
errors O
( O
” O
s O
” O
) O
and O
word O
ordering O
errors O
( O
” O
w O
” O
) O
. O

traditional O
chinese O
learning O
methods O
cost O
a O
lot O
of O
labor O
and O
time O
, O
so O
it O
is O
very O
important O
to O
establish O
an O
automatic B-TaskName
diagnosis I-TaskName
system I-TaskName
for I-TaskName
chinese I-TaskName
grammatical I-TaskName
error I-TaskName
. O

however O
, O
there O
are O
some O
differences O
between O
chinese O
and O
english O
, O
such O
as O
no O
changes O
in O
tense O
in O
chinese O
, O
which O
makes O
it O
difﬁcult O
for O
many O
chinese O
learners O
to O
ﬁnd O
their O
own O
mistakes O
in O
writing O
. O

1 O
introduction O
with O
the O
rapid O
development O
of O
china O
, O
more O
and O
more O
non O
- O
native O
chinese O
speakers O
begin O
to O
learn O
chinese O
. O

compared O
to O
the O
traditional O
model O
using O
lstm B-MethodName
( O
long B-MethodName
- I-MethodName
short I-MethodName
term I-MethodName
memory I-MethodName
) O
, O
our O
model O
have O
better O
performance O
and O
there O
is O
no O
need O
to O
add O
too O
many O
artiﬁcial O
features O
. O

in O
this O
paper O
, O
we O
propose O
a O
chinese B-TaskName
grammatical I-TaskName
error I-TaskName
diagnosis I-TaskName
( O
cged B-TaskName
) O
model B-MethodName
with I-MethodName
contextualized I-MethodName
character I-MethodName
representation I-MethodName
. O

establishing O
an O
automatic B-TaskName
diagnosis I-TaskName
system I-TaskName
for I-TaskName
chinese I-TaskName
grammatical I-TaskName
error I-TaskName
has O
become O
an O
important O
challenge O
. O

recent O
machine O
learning O
techniques O
, O
such O
as O
convolutional O
neural O
networks O
( O
cnn O
) O
( O
collobertet O
al O
. O
, O

readability B-TaskName
assessment I-TaskName
has O
a O
long O
research O
history O
, O
and O
many O
methods O
have O
been O
developed O
in O
the O
last O
couple O
of O
decades O
( O
dale O
and O
chall O
, O
1948 O
; O
mc O
laughlin O
, O
1969 O
; O
kincaid O
et O
al O
. O
, O

the O
experimental O
results O
show O
that O
( O
1 O
) O
using O
frequency B-MethodName
class I-MethodName
metric O
can O
represent O
frequency O
information O
better O
than O
using O
other O
common O
metrics O
such O
as O
raw O
counts O
or O
ranking O
; O
( O
2 O
) O
the O
model O
that O
integrates O
the O
frequency B-MethodName
embeddings I-MethodName
directly O
to O
the O
fullyconnected O
layer O
performs O
better O
than O
applying O
ﬁlters O
on O
the O
concatenated O
word B-MethodName
frequency I-MethodName
embeddings I-MethodName
and O
( O
3 O
) O
both O
proposed O
models O
outperform O
the O
baseline O
( O
the O
traditional O
ndc B-MethodName
method O
) O
and O
the O
cnn B-MethodName
models O
without O
using O
frequency O
information O
in O
both O
english O
and O
chinese O
datasets.107references O
jeanne O
s O
chall O
and O
edgar O
dale O
. O

5 O
conclusion O
in O
this O
paper O
, O
we O
have O
proposed O
two O
models O
that O
employ O
both O
word B-MethodName
and I-MethodName
frequency I-MethodName
embeddings I-MethodName
for O
the O
readability B-TaskName
assessment I-TaskName
task I-TaskName
. O

this O
method O
is O
extensible O
and O
can O
easily O
be O
applied O
to O
different O
languages O
without O
prior O
knowledge O
about O
these O
languages O
. O

it O
proves O
our O
hypothesis O
that O
frequency O
information O
is O
useful O
in O
judging B-TaskName
the I-TaskName
difﬁculty I-TaskName
level I-TaskName
of O
a O
document O
. O

finally O
, O
it O
shows O
that O
the O
frequency B-MethodName
embeddings I-MethodName
help O
improving O
the O
results O
in O
both O
english O
( O
to O
93 B-MetricValue
% I-MetricValue
) O
and O
chinese O
( O
to O
49 B-MetricValue
% I-MetricValue
) O
when O
we O
concatenate O
the O
frequency B-MethodName
embeddings I-MethodName
and O
word B-MethodName
embeddings I-MethodName
, O
using O
the O
frequency B-MethodName
class I-MethodName
information O
. O

it O
means O
that O
it O
is O
not O
necessary O
to O
apply O
ﬁlters O
and O
max O
poolings O
on O
the O
frequency B-MethodName
embeddings I-MethodName
and O
the B-MethodName
frequency I-MethodName
and I-MethodName
word I-MethodName
embeddings I-MethodName
can O
be O
learned O
separated O
and O
ﬁnally O
concatenate O
before O
going O
to O
the O
fully O
connected O
layer O
. O

the O
result O
suggests O
that O
model O
wfe B-MethodName
- I-MethodName
sep I-MethodName
works O
better O
than O
wfe B-MethodName
- I-MethodName
com I-MethodName
. O

since O
frequency B-MethodName
class I-MethodName
information O
is O
more O
representative O
than O
word O
counts O
and O
word O
ranks O
, O
it O
perhaps O
helps O
the O
model O
learn O
to O
classify B-TaskName
the I-TaskName
difﬁculty I-TaskName
levels I-TaskName
better O
in O
more O
general O
cases O
. O

this O
model O
is O
however O
worse O
than O
when O
using O
only O
frequency B-MethodName
class I-MethodName
information O
. O

when O
using O
all O
frequency O
levels O
, O
word O
ranks O
and O
number O
of O
occurrences O
together O
for O
frequency B-MethodName
embedding I-MethodName
, O
the O
results O
are O
better O
than O
other O
models O
. O

however O
, O
in O
our O
case O
, O
it O
does O
not O
work O
as O
well O
as O
when O
keeping O
the O
embedding O
vectors O
static B-MethodName
for O
both O
english O
and O
chinese O
. O

non B-MethodName
- I-MethodName
static I-MethodName
model O
is O
supposed O
to O
ﬁne O
- O
tune O
to O
the O
speciﬁc O
given O
task O
. O

among O
three O
we B-MethodName
methods O
( O
using O
pre O
- O
trained O
word B-MethodName
embeddings I-MethodName
) O
, O
the O
static B-MethodName
model O
achieves O
the O
best O
results O
. O

it O
shows O
that O
pre O
- O
trained O
embeddings O
play O
an O
important O
role O
in O
determining B-TaskName
the I-TaskName
difﬁculty I-TaskName
levels I-TaskName
. O

the O
random B-MethodName
- I-MethodName
we I-MethodName
method O
works O
better O
for O
english O
and O
much O
better O
for O
chinese O
in O
comparedto O
the O
ndc B-MethodName
, O
but O
lower O
than O
when O
using O
pretrained O
frequency B-MethodName
and I-MethodName
word I-MethodName
embeddings I-MethodName
. O

their O
results O
are O
still O
much O
lower O
than O
the O
cnn B-MethodName
methods O
using O
pretrained O
frequency B-MethodName
and I-MethodName
word I-MethodName
embeddings I-MethodName
. O

4.3 O
result O
and O
discussion O
the O
result O
shows O
that O
the O
traditional O
method O
ndc B-MethodName
works O
much O
better O
for O
english O
dataset O
( O
50 B-MetricValue
% I-MetricValue
) O
than O
for O
chinese O
( O
17 B-MetricValue
% I-MetricValue
) O
, O
which O
is O
probably O
explained O
by O
the O
fact O
that O
the O
formulae O
was O
originally O
designed O
for O
english O
language O
. O

in O
both O
settings O
, O
the O
frequency B-MethodName
embeddings I-MethodName
are O
kept B-MethodName
static I-MethodName
during O
training O
. O

in O
the O
wfe B-MethodName
setting O
, O
we O
use O
the O
three O
frequency O
metrics O
: O
raw O
counts O
, O
ranking O
and O
frequency O
class O
, O
while O
in O
the O
wfe B-MethodName
- I-MethodName
class I-MethodName
setting O
, O
we O
use O
only O
the O
frequency O
class O
metric O
. O

for O
chinese O
, O
we O
collected O
a O
dataset O
consisting O
of O
news O
( O
320 O
k O
documents O
) O
and O
wikipedia O
, O
tokenised O
and O
trained O
the O
word B-MethodName
embeddings I-MethodName
on O
it O
. O

we O
concatenate O
the O
pre O
- O
trained O
word B-MethodName
embeddings I-MethodName
and O
the O
frequency B-MethodName
embeddings I-MethodName
as O
explained O
in O
section O
3 O
. O

only O
frequency B-MethodName
embeddings I-MethodName
are O
used O
in O
this O
setting O
( O
without O
word B-MethodName
embeddings I-MethodName
) O
. O

each O
static B-MethodName
and O
non B-MethodName
- I-MethodName
static I-MethodName
we O
is O
treated O
as O
one O
channel O
while O
gradients O
are O
backpropagated O
only O
through O
one O
of O
the O
channels O
. O

these O
two O
settings O
followed O
the O
method O
in O
( O
kim O
, O
2014 O
) O
, O
where O
all O
words O
are O
kept O
either O
static B-MethodName
( O
in O
static B-MethodName
setting O
) O
or O
updated O
( O
in O
non B-MethodName
- I-MethodName
static I-MethodName
setting O
) O
including O
the O
unknown O
ones O
while O
others O
parameters O
are O
learned O
. O

we O
used O
rectiﬁed O
linear O
units O
as O
activation O
functions O
for O
the O
convolutional O
layers O
, O
dropout B-HyperparameterName
rate I-HyperparameterName
of O
0.5 B-HyperparameterValue
and O
mini O
- O
batch B-HyperparameterName
size I-HyperparameterName
of O
50 B-HyperparameterValue
. O

the O
ﬁlter B-HyperparameterName
windows I-HyperparameterName
’ I-HyperparameterName
sizes I-HyperparameterName
are O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
with O
100 O
feature O
maps O
each O
. O

for O
english O
, O
we O
used O
the O
pre O
- O
trained O
word2vec B-MethodName
by O
( O
mikolov O
et O
al O
. O
, O

finally O
, O
if O
pdwis O
above O
5 O
% O
, O
then O
add O
3.6365 O
to O
the O
raw O
score O
to O
get O
the O
adjusted O
score O
. O

raw O
score O
is O
calculated O
as: O
= O
0 O
: O
1579pdw+ O
0:0496nw O
nswhere O
nwis O
the O
number O
of O
words O
and O
nsis O
the O
number O
of O
sentences O
in O
the O
whole O
corpus O
, O
hencenw O
nsrepresents O
the O
average O
sentence O
length O
in O
the O
corpus O
. O

pdwis O
the O
percentage O
of O
difﬁcult O
words O
in O
a O
document O
, O
calculated O
as O
the O
number O
of O
difﬁcult O
words O
divided O
by O
the O
total O
number O
of O
words O
in O
the O
document O
. O

the O
new B-MethodName
dale I-MethodName
- I-MethodName
chall I-MethodName
readability O
level O
( O
chall O
and O
dale O
, O
1995 O
) O
is O
a O
traditional O
readability O
test O
. O

we O
split O
randomly O
the O
dataset O
70 B-HyperparameterValue
% I-HyperparameterValue
for O
training O
, O
27 B-HyperparameterValue
% I-HyperparameterValue
for O
testing O
and O
3 B-HyperparameterValue
% I-HyperparameterValue
for O
a O
development O
set.4.2 O
experiment O
setup O
ndc B-MethodName
- I-MethodName
level I-MethodName
. O

in O
both O
datasets O
, O
the O
difﬁculty O
levels O
were O
assigned O
by O
human O
experts O
. O

in O
total O
, O
there O
are O
279 O
documents O
with O
4671 O
sentences O
in O
enct B-DatasetName
and O
637 O
documents O
with O
16145 O
sentences O
in O
cpt B-DatasetName
. O

the O
second O
dataset O
, O
cpt B-DatasetName
, O
was O
collected O
from O
chinese O
primary O
textbook O
and O
contains O
six O
difﬁculty O
levels O
. O

the O
ﬁrst O
dataset O
, O
enct B-DatasetName
, O
was O
built O
with O
four O
reading O
levels O
from O
english O
new O
concept O
textbook O
. O

4 O
evaluation O
4.1 O
dataset O
we O
evaluate O
our O
methods O
for O
english O
and O
chinese O
readability B-TaskName
assessment I-TaskName
on O
two O
datasets O
collected O
by O
( O
jiang O
et O
al O
. O
, O

the O
feature O
map O
extracted O
from O
applying O
the O
ﬁlters O
on O
word O
embeddings O
is O
then O
computed O
as O
: O
ci O
= O
f(wxw O
i O
: O
i+h 1+b O
) O
( O
4 O
) O
finally O
this O
feature O
map O
is O
concatenated O
with O
the O
frequency O
embeddings O
, O
and O
then O
use O
dropout O
for O
regularisation O
similar O
to O
the O
architecture O
described O
in O
( O
kim O
, O
2014 O
) O
( O
see O
section O
4.2 O
) O
. O

convolutional O
layers O
and O
max O
poolings O
are O
applied O
to O
the O
word O
embeddings O
as O
these O
layers O
help O
ﬁnding O
and O
representing O
features O
of O
interests O
, O
while O
these O
layers O
are O
omitted O
for O
frequency O
embeddings O
. O

4 O
is O
applied O
on O
the O
window B-HyperparameterName
size I-HyperparameterName
hfrom O
xitoxi+h 1 O
, O
and O
the O
weights O
w2rhkewhere O
ke O
= O
kw+kfandbis O
the O
bias.105 O
figure O
1 O
: B-MethodName
convolutional B-MethodName
neural I-MethodName
network I-MethodName
architecture O
with B-MethodName
word I-MethodName
frequency I-MethodName
embedding I-MethodName
we O
then O
apply O
max O
- O
over O
- O
time O
pooling O
operations O
in O
the O
feature O
map O
. O

in O
this O
model O
, O
word O
embeddings O
and O
frequency O
embeddings O
are O
learned O
separately O
before O
being O
fetched O
into O
a O
fully O
connected O
layer O
. O

2 O
, O
where O
a O
feature O
ciis O
obtained O
using O
a O
non O
- O
linear O
activation O
function O
f O
: O
ci O
= O
f(wxe O
i O
: O
i+h 1+b O
) O
( O
3 O
) O
where O
xi O
: O
i+h 1represents O
the O
matrix O
which O
composes O
of O
vectors O
from O
xitoxi+h 1 O
. O

a O
feature O
map O
is O
generated O
using O
ﬁlters O
of O
window B-HyperparameterName
size I-HyperparameterName
hto O
the O
sentence O
matrix O
in O
eq O
. O

the O
sentence O
with O
length O
nis O
then O
represented O
by O
a O
matrix O
: O
[ O
xw O
1xf O
1 O
; O
: O
: O
: O
; O
xw O
ixf O
i O
; O
: O
: O
: O
; O
xw O
nxf O
n O
] O
( O
2 O
) O
andxe O
i O
= O
xw O
ixf O
irepresents O
the O
ﬁnal O
embedding O
of O
word O
xi O
, O
which O
is O
a O
concatenation O
of O
word O
and O
frequency O
embeddings O
. O

note O
that O
in O
the O
frequency O
embeddings O
, O
instead O
of O
randomly O
assigning O
values O
to O
unknown O
words O
as O
in O
word O
embeddings O
, O
we O
set O
them O
to O
the O
highest O
frequency O
class O
adopted O
from O
the O
training O
corpus O
. O

xw O
irepresents O
the O
word O
embeddings O
of O
word O
wiwhile O
xf O
irepresents O
its O
frequency O
embeddings O
. O

letxw O
i2rkwandxf O
i2rkf O
, O
where O
xiis O
a O
word O
in O
a O
sentence O
of O
length O
n O
, O
kwis O
the O
word B-HyperparameterName
embedding I-HyperparameterName
dimension I-HyperparameterName
and O
kfis O
the O
frequency B-HyperparameterName
embedding I-HyperparameterName
dimension I-HyperparameterName
. O

the O
network O
learns O
these O
ﬁlters O
’ O
weights O
that O
activate O
features O
extracted O
from O
the O
these O
embeddings O
. O

in O
this O
model O
, O
the O
ﬁlters O
are O
applied O
to O
the O
concatenated O
embeddings O
of O
word O
and O
frequency O
. O

in O
particular O
, O
we O
propose O
two O
models O
( O
figure O
1 O
) O
wfe B-MethodName
- I-MethodName
com I-MethodName
( O
left O
) O
and O
wfe B-MethodName
- I-MethodName
sep I-MethodName
( O
right O
) O
. O

in O
particular O
, O
the O
frequency O
class O
fc O
( O
w)of O
a O
word O
wdescribes O
the O
frequency O
freq O
( O
w)of O
the O
word O
in O
relation O
to O
the O
frequency O
freq O
max O
of O
the O
most O
frequent O
word O
, O
i.e. O
, O
the O
word O
with O
ranking O
0 O
( O
sabine O
fiedler O
and O
quasthoff O
, O
2012 O
): O
fc(w O
) O
= O
log2freqmax O
freqw(1 O
) O
our O
architecture O
is O
slightly O
different O
from O
the O
cnn O
architecture O
presented O
in O
( O
kim O
, O
2014 O
) O
. O

among O
these O
metrics O
, O
the O
word O
frequency O
class O
information O
is O
the O
most O
generalised O
one O
. O

we O
take O
these O
metrics O
directly O
as O
an O
embedding O
vector O
represents O
words O
in O
the O
corpus O
. O

1we O
have O
not O
taken O
into O
account O
rare O
words O
that O
are O
easy O
to O
understand O
, O
for O
examples O
names O
, O
locationsthe O
three O
common O
metrics O
representing O
word O
frequency O
information O
are O
raw O
counts O
( O
number O
of O
times O
a O
word O
appears O
in O
the O
whole O
corpus O
) O
, O
ranking O
( O
i.e. O
, O
rank O
0 O
for O
the O
most O
common O
word O
) O
and O
frequency O
classes O
. O

in O
addition O
, O
frequency O
information O
plays O
the O
role O
of O
pointing O
out O
which O
words O
are O
more O
difﬁcult O
to O
understand1 O
. O

word O
embeddings O
help O
associating O
the O
topics O
of O
documents O
, O
which O
are O
important O
to O
assess B-TaskName
the I-TaskName
readability I-TaskName
levels I-TaskName
( O
e.g. O
, O
there O
are O
topics O
that O
are O
more O
difﬁcult O
to O
understand O
than O
others O
from O
their O
natures O
) O
. O

our O
hypothesis O
is O
that O
the O
model O
can O
learn O
better O
from O
knowing O
words O
’ O
difﬁculty O
levels O
besides O
their O
meanings O
. O

from O
this O
observation O
, O
we O
propose O
a O
model O
that O
takes O
into O
account O
also O
word O
frequency O
information O
besides O
word O
embeddings O
. O

in O
the O
readability B-TaskName
assessment I-TaskName
scenario O
, O
frequency O
information O
is O
important O
in O
deciding O
whether O
a O
document O
is O
hard O
to O
read O
or O
not O
( O
jiang O
et O
al O
. O
, O

although O
they O
can O
reﬂect O
word O
meaning O
and O
topics O
, O
they O
do O
not O
take O
directly O
frequency O
information O
of O
a O
word O
into O
account O
. O

they O
take O
into O
account O
the O
context O
in O
which O
a O
word O
appears O
to O
learn O
the O
representation O
of O
words O
. O

word O
embeddings O
are O
used O
transferrably O
in O
many O
general O
nlp O
tasks O
. O

motivated O
by O
the O
recent O
success O
of O
convolutional B-MethodName
neural I-MethodName
network I-MethodName
( O
cnn B-MethodName
) O
models O
in O
many O
text O
classiﬁcation O
tasks O
, O
we O
employ O
the O
models O
for O
learning O
and O
classifying O
a O
given O
text O
to O
its O
difﬁculty O
level O
. O

these O
methods O
are O
not O
easily O
transferred O
to O
other O
languages O
especially O
asian O
. O

3 O
our O
method O
while O
traditional O
methods O
are O
simple O
to O
implement O
, O
they O
focus O
mostly O
on O
latin O
languages O
such O
as O
english O
. O

most O
of O
these O
studies O
however O
require O
hand O
- O
crafted O
, O
language O
- O
dependent O
features O
, O
and O
not O
readily O
applicable O
to O
multilingual O
setting O
. O

2007 O
) O
, O
grammatical O
templates O
( O
wang O
and O
andersen O
, O
2016 O
) O
, O
word O
frequency O
smoothed O
by O
correlation O
information O
( O
jiang O
et O
al O
. O
, O

in O
these O
studies O
, O
documents O
are O
represented O
by O
different O
types O
of O
features O
such O
as O
bag O
of O
words O
, O
lexical O
and O
grammatical O
features O
extracted O
from O
parse O
trees O
( O
heilman O
et O
al O
. O
, O

the O
data O
driven O
approach O
treats O
readability B-TaskName
assessment I-TaskName
as O
a O
machine O
learning O
problem O
, O
that O
is O
to O
automatically O
learn O
the O
mapping O
from O
documents O
to O
difﬁculty O
levels O
based O
on O
training O
examples O
( O
si O
and O
callan O
, O
2001 O
; O
heilman O
et O
al O
. O
, O

though O
considered O
quick O
and O
easy O
to O
compute O
, O
these O
tra-104ditional O
metrics O
/ O
formulae O
are O
designed O
with O
some O
speciﬁc O
language O
in O
mind O
, O
and O
thus O
they O
may O
not O
work O
well O
when O
applying O
to O
other O
languages O
. O

these O
early O
studies O
evaluated O
text O
difﬁculty O
based O
on O
shallow O
features O
such O
as O
word O
difﬁculty O
levels O
, O
the O
average O
sentence O
length O
, O
the O
average O
number O
of O
syllables O
. O

1975 O
) O
, O
( O
chall O
and O
dale O
, O
1995 O
) O
. O

the O
traditional O
approach O
include O
( O
dale O
and O
chall O
, O
1948 O
) O
, O
fog O
index O
( O
gunning O
, O
1952 O
) O
, O
smog O
( O
mc O
laughlin O
, O
1969 O
) O
and O
flesch O
- O
kincaid O
index O
( O
kincaid O
et O
al O
. O
, O

2 O
related O
work O
readability B-TaskName
assessment I-TaskName
methods O
can O
be O
classiﬁed O
into O
two O
categories O
, O
the O
traditional O
approach O
and O
data O
driven O
approach O
. O

since O
this O
model O
does O
not O
depend O
on O
hand O
- O
crafted O
features O
, O
it O
can O
be O
easily O
adapted O
to O
multiple O
languages O
. O

these O
two O
embedding O
layers O
are O
employed O
in O
a O
cnn B-MethodName
architecture O
to O
determine O
the O
readability O
level O
of O
a O
given O
document O
. O

we O
therefore O
propose O
two O
models O
that O
jointly O
represent O
words O
based O
on O
their O
meanings O
with O
traditional O
word O
embeddings O
and O
their O
frequency O
levels O
with O
the O
so O
- O
called O
frequency B-MethodName
embeddings I-MethodName
. O

